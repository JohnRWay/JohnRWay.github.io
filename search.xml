<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[�������Լ���ʹ��Hexo]]></title>
    <url>%2F2019%2F06%2F03%2Fhexo-update%2F</url>
    <content type="text"><![CDATA[����ԭ�����ļ����뿽���ļ���������_config.yml������ theme������ scaffolds #����ģ�������� package.json #˵��ʹ����Щ�������� .gitignore #�޶����ύ��ʱ����Щ�ļ����Ժ��������� source��1����������Щ�ļ��Ǳ��뿽���ģ�������֮ǰ�Լ��޸ĵ��ļ�����վ������_config.yml��theme�ļ�����������⣬�Լ�source�����Լ�д�Ĳ����ļ�����Щ�϶�Ҫ�����ġ� ����֮�⣬���������ļ���Ҫ�У�����scaffolds�ļ��У����µ�ģ�壩��package.json��˵��ʹ����Щ������.gitignore���޶����ύ��ʱ����Щ�ļ����Ժ��ԣ��� ��ʵ���������ļ����������޸ĵģ����Լ�ʹ��ʧ�ˣ�Ҳû�й�ϵ�����ǿ��Խ���һ���µ��ļ��У�Ȼ��������ִ��hexo init���ͻ������������ļ�������ֻ��Ҫ�����ǿ�������ʹ�ü��ɡ� �ܽ᣺_config.yml��theme/��source/��scaffolds/��package.json��.gitignore������Ҫ�����ġ���2������������Щ�ļ��ǲ��ؿ����ģ�����˵����ɾ���ģ�������.git�ļ�����������վ���Ŀ¼�£���������Ŀ¼�µ�.git�ļ���������ɾ���� Ȼ�����ļ���node_modules������npm install���������ɣ���public���������hexo gʱ���������ɣ���.deploy_git�ļ��У���ʹ��hexo dʱҲ���������ɣ���db.json�ļ��� ��ʵ������Щ�ļ�Ҳ������.gitignore�ļ�������صĿ��Ժ��Ե����ݡ� �ܽ᣺.git/��node_modules/��public/��.deploy_git/��db.json�ļ���Ҫɾ���� ����������װ Git�ӹ���Git����git�����µ����ϰ�װ����Ϊhttps�ٶ���������ÿ�ζ�Ҫ���������õ���ʹ��ssh��ʹ�����淽����������git bash�������û����ƺ��ʼ���ַ12$ git config --global user.name "username"$ git config --global user.email "username@example.com" ���û���Ŀ¼�����У�ssh-keygen -t rsa -C ��youremail@example.com�� �����е��ʼ���ַ�����Լ����ʼ���ַ��Ȼ��һ·�س������ɺ󣬻����û���Ŀ¼������.sshĿ¼��������id_rsa��id_rsa.pub�����ļ�������������SSH key��Կ�ԣ�id_rsa��˽Կ��ǧ����й¶��ȥ��id_rsa.pub�ǹ�Կ�����Է��ĵظ����κ��ˡ���½GitHub���򿪡�Settings��-&gt;��SSH and GPG keys����Ȼ������new SSH key������������Title����Key�ı�����ճ����Կid_rsa.pub�ļ������ݣ�ǧ��Ҫճ����˽Կ�ˣ������������Add SSH Key�������Ӧ�ÿ����Ѿ����ӵ�Key�� ��װNode.JS��װ Hexo��git bash�ͻ��ˣ����� npm install hexo-cli -g����ʼ��װhexo����֮ǰ������ npm install hexo -g ��װ�ģ��˴���ʹ����������Ϊ���� npm install hexo-cli -g���װ������޷��ύ����������ܰ汾���⡣ ��װģ����git bash���л�Ŀ¼���¿������ļ����ʹ�� npm install �������ģ�鰲װ��ע�⣺��Ҫ��hexo init��ʼ���������ļ��Ѿ��������ɣ��������ʹ�ã���վ�������ļ�_config.yml�ᱻ��ʼ��ΪĬ��ֵ ͬ����Github12hexo ghexo d �������123$ npm install npm install hexo-deployer-git --save #ͬ��������github����ǰ��ͬ�����ɹ��Ͱ�װ���$ npm install hexo-generator-feed --save #RSS����$ npm install hexo-generator-sitemap --save #վ���ͼ ���ͣ�Coding For Dream����I never feared death or dying, I only fear never trying. �CFast &amp; Furious]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>����</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[����windows��telnet]]></title>
    <url>%2F2019%2F06%2F03%2Fwindows-telnet%2F</url>
    <content type="text"><![CDATA[����ĵ��ԣ���װ��ϵͳʹ��cmd��telnet��ʱ����ʾû�д���������µ�������һҹ�ص����ǰ����ɶҲû�У����ǿ��顣 ��������˵���������windows��telnet���ܼ򵥡�������ͼ����һ����������ѡ����telnet����˿ͻ���ѡ� Ϊ����֤Telnet���������Ƿ����ɹ������ǿ�����cmd�������²���һ�£����ʱ��Ͳ�������ʾtelnet�����޷��ҵ��ˡ� ���ú��ˣ��� ԭ�����ף�ת����ע�����������ͣ�Coding For Dream����I never feared death or dying, I only fear never trying. �CFast &amp; Furious]]></content>
      <categories>
        <category>telnet</category>
        <category>ԭ��</category>
      </categories>
      <tags>
        <tag>����</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[navicat����������µ�mysql��������]]></title>
    <url>%2F2019%2F06%2F01%2Fmysql-accesee-right%2F</url>
    <content type="text"><![CDATA[���»���̨����������ȫ�����°�װ��һ�飬�ڰ�װ��MysqlServer������Ӧ�û�Ȩ�޺�ȻҲ������һЩ���⡣ ������ΰ�װMysqlServer�������������Բο���ǰд����ƪ���͡� ��VM�ϰ�װ���MySQL��Server֮��ʹ��Navicat�޷��������ӵ�����ʹ��cmd��telnet�����¼����һ�·����Ƿ���û�п����˿ڷ��ʡ� �������ʹ�����ַ���������������⡣1.�رշ���ǽ����򵥵ķ����������κ����ơ����Ƿ���ǽû�����ܾ��������㱼����Ȼ�е����ܾ����ٵ�ɶ��12systemctl stop firewalldsystemctl status firewalld ��ʱ�ٴ�����һ�����ݿ⣬���������ϵġ� ��������˻��ǽ�����ý�Ϊ��ȫ�ķ�ʽ�� 2.ʹ��iptables���п��Ŷ˿ڡ���ʵ����iptables����ǽ���������ع����鿴�˿��Ƿ�ɷ��ʣ�telnet ip �˿ں� ���籾����3306��telnet localhost 3306�� ���ŵĶ˿�λ��/etc/sysconfig/iptables���鿴ʱͨ�� more /etc/sysconfig/iptables ����鿴����뿪�Ŷ˿ڣ��磺3306����1��ͨ��vi /etc/sysconfig/iptables ����༭����һ��-A INPUT -p tcp -m tcp –dport 3306 -j ACCEPT ������2��ִ�� /etc/init.d/iptables restart ���iptables����������3������ /etc/rc.d/init.d/iptables save ���⻰��1.����ĵ��Ծ�Ȼ��ȥcmd������telnet��û�У����Բο���һƪ�ġ�����windows��telnet����2.�����õ�CentOS7.3.1��ʱ��ʹ��iptables������ʾcannot found command��������ΪĬ��ʹ�õ���firewall��Ϊ����ǽ��û�а�װiptables������ʹ����������12345systemctl stop firewalld systemctl mask firewalldyum install -y iptables yum install iptables-services Ȼ�����iptables�ļ����Ϳ�����������������1234systemctl start iptables.servicesystemctl restart iptables.service // ��������ǽʹ������Ч systemctl enable iptables.service // ���÷���ǽ�������� �����������Ƿ�װ��iptables123456789101112131415service iptables status #��װiptables yum install -y iptables #����iptables yum update iptables #��װiptables-services yum install iptables-servicessystemctl disable iptables #��ֹiptables���� systemctl stop iptables #��ͣ���� systemctl enable iptables #�����ֹiptables systemctl start iptables #�������� ��ʵfirewall��iptables���Ƿ���ǽ��ʵ�ֵ�ԭ����һ�����ˣ������ﲻ����˵�� ԭ�����ף�ת����ע�����������ͣ�Coding For Dream����I never feared death or dying, I only fear never trying. �CFast &amp; Furious]]></content>
      <categories>
        <category>Mysql</category>
        <category>Navicat</category>
        <category>ԭ��</category>
      </categories>
      <tags>
        <tag>����</tag>
        <tag>Mysql</tag>
        <tag>������</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这是我的第一个blog!]]></title>
    <url>%2F2019%2F06%2F01%2FMy%20First%20Blog%2F</url>
    <content type="text"><![CDATA[终于把我的blog差不多建立起来了，以后就开始用这个写blog了。 blog的内容主要是技术方面的，另外包含一些业务上分享，生活随想及读书笔记等等。技术方面有源码的阅读，算法题，读书笔记，及一些技术点的深究。业务方面就是金融行业的一些积累知识。 介绍下自己，我是金融行业的小小码农，计算机小硕。我会好几门语言，常用的有Java，英语，汉语，大概还会一点点的日语吧。平时除了codeing还要兼带leader职责，所以时间上就显得不充裕。但我仍旧会抽时间看些开源产品和管理方面的书籍，还会上课学习一些计算机基础课程巩固自己的基础。因为我是技术出身，喜欢技术，所以坚决不放弃技术。通过自己的不断努力及参与最近几年金融行业的转型优化，对微服务和分布式等有一些实践和自己的看法。希望通过blog记录下我的点滴积累，并和大家进行分享交流。 上大学的时候说实话就是混社团了，没有好好学习，等到大四的时候，好多同学都保送和考上研究生了，而且大部分都是985的好学校，那个时候才发现自己把最好的时光浪费掉了。因为担心自己和同学的差距越来越大，所以想通过学习改变自己的状态，最后坚持学习几乎成了习惯，因为哪天没学习，就会有点小小罪恶感。学习固然是好的，但自己学习没有交流渠道那就很苦闷了，所以程序员也要找到相应的渠道，去分享，去交流，这样才能碰撞出想法和激情（当然包括赚钱的方式和更有价值的工作等等），让你更有动力学习下去。 所以最近参加了耗子叔（陈皓）组建的100天打卡学习，没有强制要求，只有自觉和群友的督促。这不是一项任务，而是为人生培养更好的习惯。 加油！Coding For Dream！！ I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>随想</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[blog更新计划]]></title>
    <url>%2F2019%2F06%2F01%2Fblog-schedule%2F</url>
    <content type="text"><![CDATA[以前不敢写东西，因为较十几年工作的大佬觉得自己经验太少，才疏学浅不敢动笔。CSDN上的blog也主要是转别人的文章，怕自己执笔出错。但是看了耗子叔的一篇文章后，觉得他说的太对了：不要害怕以后的自己对现在的自己打脸，打脸带来的是对技术思想上的批判和成长。 对比以上对学习的分析的金字塔，下决心把自己使用过的产品，熟悉的东西进行整理放到blog上。一是记录自己的点滴成长，让我的知识更加巩固；二是与网友们交流，碰撞，相互促进。 接下来写点自己的更新计划： 技术上： 1.源码阅读：JDK源码，Spring源码，RocketMQ源码。 2.JVM分析及优化。 3.服务器：总结tomcat，jetty，jboss，weblogic等。 4.数据库：a.关系型数据库Relation Databases：Mysql，Oracle； ​ b.时间序列数据库TSDB：InfluxDB； ​ c.数据库优化方面； 5.SpringCloud产品体系及详细使用。 6.阿里金融云产品：EDAS，DRDS，REDIS，HSF，CSB，ARMS等等。 7.阿里云产品与SpringCloud体系的对比。 8.微服务。从概念，划分，通信，网关，编排到治理，安全等等。 9.消息队列：RocketMQ，AliMQ，Kafka，IBM MQ。 10.算法题。 11.读书笔记：包含技术方面和管理方面的。（技术领导力） 12.容器：VMware，Docker，Kubernetes等。 13.Liunx：分为shell脚本和常用命令。 14.面试题：整理Java方面的面试题及收集的大厂面试题。 15.代码：提交到GitHub上一些代码。自己写的一些demo，封装等。 16.工具的使用：软件工具的使用教程。 业务上： 1.金融行业的业务分享: 业务设计，中台设计等。 大致汇总整理了以上几点，我觉得有个纲领贴出来好一些，因为我这样有驱动力。能够一直做下去。 加油！Coding For Dream！！ I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>计划</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[万亿级交易量下的苏宁支付平台设计]]></title>
    <url>%2F2019%2F05%2F29%2Fpayplatform-design%2F</url>
    <content type="text"><![CDATA[123作者简介： 肖军：现担任苏宁易购集团总经理助理；曾先后就职于蚂蚁金服和苏宁金服，专注于金融相关的产品研发工作；擅长互联网产品设计，高可用架构设计，体系化的研发团队管理。设计过多款行业领先的互联网金融产品，并荣获数项金融相关专利；主导过多次双11,618,818等大促稳定性设计和保障工作；丰富的从0到1大规模研发团队搭建与管理经验。 本文主要是根据作者在2018QCon演讲内容整理而成： 苏宁金融交易量3年内从1000亿增长到万亿+，服务用户3亿+，服务场景从服务于苏宁易购内部生态，扩展到服务全渠道，全场景，多业态的线上线下智慧零售的开放生态圈，一方面要满足公司业务发展要求，快速研发新产品，另一方面要满足818大促，双11等大促设计要求； 本次主要介绍苏宁支付系统如何实现500天性能提升2000倍，从100笔/秒提升到20万笔/秒，给飞行中的飞机换引擎，将包括三大章节六个部分：苏宁支付平台发展历程，以及现在运行的总体架构设计，以及配套的可视化作战指挥系统，以及在业务急速变化，万亿级交易量的状态下，如何对全局架构进行优雅地重构，以及重构过程中的实战案例，最后介绍一下我们目前规划的、对未来的展望；具体技术包括高可用设计技巧，高伸缩性设计思路，弹性的流量和资源控制，异地多活，全链路压测，消除数据瓶颈与单点，热点追踪与防护，故障自愈，账务系统之大账户瓶颈解决方案，以及未来怎么实现机器人自动巡检和自动修复等实战经验分享。 第一部分，我们介绍苏宁支付平台的演进路线， 架构演进的驱动与目标； 苏宁支付平台演进经历了四个阶段：从传统的架构，到SOA架构，到云计算架构以及目前的智能支付引擎；服务场景也从单一的服务苏宁易购，到服务苏宁内外部生态圈，再到提供行业解决方案。TPS从100到20w+的支付处理能力；交付周期也从最初的按月交付到现在的准实时交付。 那是什么驱动我们进行一次次的架构演进呢？驱动力和目标是什么呢？支付平台是整个金融的基础设施，也是公共设施，服务于几十个事业部的几百条产品线，如果每一条产品线提一个需求，那就要同时响应几百个需求，同时还要面对业务的大促，因为苏宁是O2O的模式，业务场景会更加复杂，线上线下都有：线下的五一、国庆；线上的418、618、818、双11、双12，基本上每两个月就有一个S级大促；一方面要保证业务需求的快速响应，另一方面也需要保证大促的安全稳定，对来说业务需要快，对系统来讲需要稳，那就需要我们的系统，是一个高可用、可伸缩、低成本、快速交付的系统。 第二部分介绍现在正在运行的总体架构设计，包括总体业务架构设计，总体系统架构设计，总体技术架构设计，关键子域的架构设计； 首先是总体业务架构设计，主要包括4个部分：第1部分是我们服务的渠道和场景，包括SDK,WAP,PC各端，线上线下门店；以及电商体系的应用，金融APP的应用等；第2是我们的合作方银行：包括中农工建交等以前的直连银行，以后后来的网联，银联等；第3是贯穿整个全流程的风险控制体系与运营支撑体系，包括欺诈风险，信用风险，以及配置产品，运营的各种支撑系统；第4是云支付平台本身。在云支付平台中包含三大子架构域：一是开放平台，把我们内部的服务统一开放出去给各个渠道、各个服务去使用；二是对这个平台进行层层抽象，将c端业务平台当中线上线下的公用逻辑抽取到c端业务平台，b端业务平台当中各个行业支付的公用逻辑，我们会抽取到b端公用平台。第3作为支付核心，会统一整合内外部支付工具以及账户核心操作指令统一提供给上层使用。 业务架构决定了系统架构，从纵向来看，我们分为应用层、数据层、技术服务层、基础设施层，以及贯穿整个全流程的决策支持与运营支撑层。从横向来看，分成面向用户和商户的服务交付层（通过开放平台交付给我们的合作伙伴，通过这个平台前置服务于苏宁易购生态圈的各个应用场景）；应用服务层（包括业务处理类、管理类、数据类）；通用服务层（即平时常见的支付收银台、风控、合同计费等）；核心服务层（包括会员、账务核心、清结算等）；网关服务层（因为我们需要集成外部的一些服务，包括金融服务，通过金融交换服务去做；沟通网关，面向运营商的；业务网关，面向和我们合作的商户的）；整体架构的设计，我们采用了插件式的架构设计思想，比如服务交付层，我们基于标准的平台业务进行服务交付，这样可以使各应用域独立并行的研发；对网关服务层，我们基于标准的外部服务引入，使平台具有快速可扩展性。 业务架构和系统架构决定了我们的技术架构，技术架构包括三大部分：持续交付层，以及支撑我们持续交付的中间件层以及基础设施部分：持续交付重点有两个，第一是快：开发快，所以我们有开发的插件、模板生成工具；测试快，从自动化测试到持续集成，到一键建站的统一拉起；发布快，有现成的发布流程支持；业务验收快，这个是我们支付平台独有的一项，上线之后要做业务匹配分析和还原，这个有两点好处：（1）对业务来说，可以快速地知道需求有没有按照业务预期的开发；（2）对研发人员来说，可以快速地知道研发的需求是否获得了业务收益。这样，业务和研发人员有了统一的视图。第二是稳：开发的过程会做这一些非功能性的设计，如：伸缩型设计，监控设计，资损防控设计；资损防控设计有三层：第1层是开发与测试，第2次是监控与核对，第3层是止损与追款。平稳，发布时支持灰度发布和预案回滚。比如做升级，收单要从单库切到多库，不能一刀切，需要按业务场景、用户、访问链路灰度，可以保证整个系统的平稳；如果一个系统上线后，如果出问题能马上实现回滚，对用户没有影响； 接下来讲关键子域架构设计，从收银台到交易，从支付服务到支付引擎，我们如何实现标准化和插件化。首先收银台分为三层：第1层是业务产品层；第2层是业务接入层，会做一些异常的适配，如不同的errorCode可以展示不同的异常界面，对用户体验比较好。第3是核心层，用户偏好与习惯、额度控制等。收银台是从一个简单的收银门面，千人一面，逐渐发展到千人千面，再到一人千面；就是不同的用户在不同场景下看到的收银台是不一样的，到同一个用户在不同场景下也可以进行个性化的适配； 这个是收单系统和交易系统的介绍：分成两部分，一个是公用的部分，就是我通用的上下层的依赖，比如支付引擎、会员、收银台、收费计费等，中间的收单服务层可以通过设计模式去封装，根据不同的业务请求，然后统一做收单路由到不同的插件去处理。这样即使有几十条产品线同时请求，我们也能同时响应，因为只需要改动一个插件即可； 基于这种插件式架构的设计思想，接下来的支付服务也是这样设计的。支付引擎会整合银行相关的外部支付工具，以及零钱宝、任性付、信用支付、现金贷等内部支付工具，同时进行账户操作指令的封装（主账务核心和各个业务的微账务核心）。对我们来说，账务核心、支付工具群和支付外转接中心都是一个个插件，开发速度快；在具体的一个支付工具内部，也是插件式的；这样就完全可以实现大规模的并行研发； 最后是网关层，面向接入银行渠道和接入合作大商户：第1层是报文组装解析层，第2层是适配器层，第3层是路由引擎；由图可见，每家银行的公用逻辑相同，可以通过设计模式封装。不同的是输入参数的获取策略以及输出参数的不同阐释策略。具体实践时代码结构上可以用设计模式来封装；实现代码上每个银行的输入报文的不同，可以用velocity模板来做。在返回报文时，每个银行的错误码和异常处理机制也不一样，可以通过groovy脚本来解析，这样对于接入新银行和商户不用做系统发布，直接配置即可，实现插件化可配置；2015年前公司一个月接一家银行，2016年后可以实现一个月接几十家。 基于前面的整体架构，我们思考，如果每个系统中的模块、调用链都能可视化，不管新的优化、重构、还是做业务需求都能快速实现。 基于这个理念，我们做了一个可视化作战指挥系统。包括三大部分：研发的可视化： 聚焦统一目标下的交付全链路、全资源可视化；统一目标是指公司的战略目标，从上图可见，战略目标KPI一定极简指标，要定北极星指标，一般我们会定三项，战略目标分解到事业部，事业部分解到研发中心对应具体需求，而需求的整个研发周期已经可视化了，可以清楚知道每一个需求、每天做的事情是不是帮助整个集团在完成战略目标。运行的可视化：系统上线之后可以看到从机房，到整个调用链，到每个架构域，再到每个具体的系统；以至系统里面的每个模块，都能清楚他们的状态。管控的可视化： 组件自治，资源弹性调度；每逢大促尤其是洪峰时候，需要执行应急预案，我们就需要知道，执行应急预案之后影响的用户场景，以及各个硬件执行过程当中的操作的步骤。 可视化作战系统架构设计，这里面除了平时的一些常用的技术设计之外，还有三点核心的设计思想：第1点，对研发来说，体现在可视化整个研发生命周期，但是起点与平常不同，平常的起点可能就是一个需求，但这个的起点是战略。第2点，运行时关注不稳定性的因素，去主动分析、依赖分析和变更感知。分析变更感知的前提，是需要对每个系统做SLA；第3点，为了进行平滑的管控，需要做几个工作：制定应急预案后进行线下，线上的演练，除了线下测试环境的演练外，生产环境也需要实际的演练，比如划拨一定量的生产用户，调度一定量的业务场景，也会自动注入一些故障，尽量让演练流量的结构构成接近真实流量，以保证演练的真实性。如果故障没有经过演练，真实发生是不可控的；故障能否快速恢复，能否自愈，对用户来说是不是感到平滑。 苏宁金融集团年交易量已过万亿，日均资金流水几十亿，需要保证每一笔交易资金的安全；对这样业务需求极速变化的高并发金融资金系统进行重构，就犹如对发射出去的导弹进行二次加速，任何一个小失误，都可能导致上亿的资损，影响上亿的用户体验；那么在重构过程中，如何保证优雅就非常重要？首先需要确定我们的目标是什么？基于这个目标我们的困难是什么？解决方案是什么？怎么去实施？怎么去演进？怎么去验证？ 基于这个愿景，就要满足两点：快和稳； 第一个是快：我们需要对业务敏捷、快速响应业务的变化；这个也是研发中心的核心使命，能对集团业务能够很好的支撑，甚至是驱动业务的发展；第2个是稳，性能要高（20万TPS、高可用），平时会考核MTTR，出现故障后多久能恢复，10秒、20秒还是一分钟？通过这个指标去牵动其它所有的工作的优化，避免指标太多，工作没有重点，不能聚焦；比如定10个指标，每个指标权重10%，看似面面俱到，其实没有重点；但是系统指标有很多，有成功率，耗时，异常率，各个硬件的使用率等等，作为负责人要找到北极星指标；我们的北极星指标就是MTTR；以及这么多系统能否实现弹性治理。 基于目标，然后识别关键问题，那我们的关键问题有四类：1、交付速度：基于标准的复用，并行、分布研发；2、高可用：需要分析故障点，建设DB单点/热点防护、自动化运维、服务自愈、应用级灾备能力3、可伸缩：从应用到IDC、服务器、网络，做到全网伸缩；4、低成本：不仅要节流，还要开源，重点是公司盈利和控制资损，通过技术对业务驱动力产生的正向价值，产品运营效果评估，这个也是对团队负责人的一个要求：要善于做技术产品化和技术品牌的运营； 高可用的重点是故障识别与应对，即对故障源的实时感知和可视化的治理。故障源来源：按照我们的服务模型分三方面：提供的服务、服务本身、依赖服务。提供的服务：故障来源在于请求，比较经常出故障的是：重复请求、并发请求、超量请求。针对每一个请求我们用不同的策略进行处理。外部服务：首先检测通信是否正常，通信正常后服务是否可用，服务可用后响应是否超时，这些都没问题后功能契约SLA是否满足，这样就形成了一个体系化的处理方法，就不会遗漏，也便于团队的知识传承（不论是代码结构设计，还是团队设计思想的统一，都是比较好的） 从PAAS平台进入到IAAS实现全网可伸缩。 对外提供的服务是吞吐量、单资源存储量的上限、响应时间；内部服务：关注DB、数据库总连接数、单数据库每秒事务数、慢SQL；依赖服务：银行实时清算能力、关键服务访问量等；这个是个可伸缩的框架，接下来我们详讲一些关键系统的可伸缩设计，具体是怎么做到的，交易系统、支付引擎系统和账务核心，如何实现可伸缩的。 首先是交易系统的设计，大维度上，将B、C端拆封开，然后进行读写分离，再对写进行分库分表。这里要特别讲一下分库分表中间件有两个特别的功能：灰度支持和影子库表支持。灰度支持即用来对不同用户，对不同场景，不同功能进行灰度，影子库表主要便于生产压测使用，后面在生产压测部分会详讲； 支付引擎即支付服务的分库分表策略，按照用户维度进行拆分。这里面有个核心设计，我们设计了一个逻辑数据源，而不是物理数据源，便于迁库，减少DBA的运营成本。 账务系统可伸缩，难点在于热点账户；热点账户即资金处理频繁、时间点密集、基于等待的数据库排它锁的大账户；正常情况下，我们开发人员首先是切入系统进行优化，但实际以业务为切入点会更好。首先对业务场景进行优化，实现进出资金隔日，业务错峰，拆分收支账户，收款方到账准时实化，收款方到账准时实化；接下来才是系统上的优化，系统上做到分布式锁、资金资源池机制、缓冲记账、并发控制、异步化；最后是账户层面的优化，不同账户制定不同的策略。比如中间账户：只登记账户明细流水，不更新余额，日终进行汇总轧差，一次性更新；待清算账户：采用单边记账方式，待清算账户不做余额更新并且不登记账户明细流水，待日终进行单边汇总；特定业务收费账户：异步分段补账等，这样的优化才是最有效的，而且扩展性好，后期的维护成本也低； 在解决了核心瓶颈点之后，设计架构的演进路线也很重要；因为我们做系统重构是不能停业务的，就好比飞机在飞行的过程中，进行换引擎的操作；基于以上目标，我们进行架构重构，就需要注意这三点：1、要与业务发展路线合拍，顺势而为：切忌沉浸在自己的技术世界里面，因为公司首先是盈利组织，研发首先要服务好业务，才能驱动业务，才能长远发展，阻碍业务发展的研发团队和研发技术方案是不长久的，特别是对于一个竞争对手激烈的高度发展的公司；2、专注主线、边界优先，步步为营：就像装修房子一样，可以先把墙装好，但内部的每一个房间不一定马上装修，因为我们内部是可控的，内部系统重构可以放后，但是要先做提供给边界系统的接口，这样既可以很好的控制风险，也便于多团队之间的协同作战；3、定期可视化投入产出比：因为金融系统不可能把业务停下来，他一定是在业务发展的过程当中，需要实时评估，重构和业务发展是否合拍，这样便于获得集团的大力支持，减少各方的不理解，减少很多不必要的阻碍，消除部门壁垒，消除决策层的顾虑； 实施的过程当中，如何管控项目？项目前：需要消除风险，获得支持，确定项目价值与范围，明确业务影响 ，获得相关干系人支持，用架构概念验证原型；项目中：做到短、平、快，严格控制项目范围扩张，Rebase不可避免的业务需求；项目发布时：做到稳定，用户体验连续，基于场景的立体化监控与报警，比如有时监控时我们常发现单个系统的耗时、成功率等指标都正常，但实际影响了整个调用链，影响了某个场景，所以一定需要基于场景做立体化监控。其次，每个核心链路上的系统一定要经过应急预案的演练。然后，要保持悲观主义的心态和终结者的思维，潜意识里面要假设重构时一定会有问题， 所以一定要守好上线的最后底线，要做到快速回滚和降级。最后是阶段性复盘，聚焦目标，防止做的需求偏离业务目标，这一点对于研发管理者特别重要，通过这些回顾逐渐拿到和业务方的平等话语权；慢慢就会建立起和业务的一个很好的沟通渠道； 架构重构后需要验证合理性，怎么知道我们设计的架构就是满足我们的预期呢？主要包括3个方面：1、新老场景的推演：新老场景是否都能支持；2、核心服务的推演：上下游系统需要演练是否稳定 ，3、非功能性的推演：系统是否可隔离、可配置、可监控、可回滚、无单点、无状态等；主要推演架构的高可用、可伸缩、研发成本，运维成本和迁移成本5个指标，通过这5个指标在上述3个方面的推演，基本上就可以验证架构合理性了，其实做架构决策也可以参考这个依据，比如多个研发中心，让你做架构决策，这个系统谁做合适？特别是架构师怎么让自己的架构决策做到大家都认可，这种思维方式都可以参考； 接下来介绍重构过程中的一些经典案例，便于大家可以在自己的公司里面快速的复制和集成； 第一个：两周建成立体化监控体系：为什么需要监控？因为重构过程中，随时有可能出问题，所以需要一个监控系统能随时看到重构的链路情况。为什么是两周？因为重构对时间其实是有要求的，需要快速完成。如何在两周之内建成一个立体化监控体系呢？有几点：1、指标要极简2、可视化3、管控全网化（规则报警、一键定位、洪峰控制、业务降级），4、统一日志模型，针对重构系统的变化，要保证监控是准确的，所以需要对服务模型抽象出三层（服务的使用者、服务的提供者、服务的集成者）打日志，每个服务的接口都会打digest摘要日志。实现过程中，会请架构师或资深的技术经理搭好插件化框架，完成日志模型搭建、异常体系的建设，领域模型，对输出结果的阐释策略。后面程序开发时，开发人员只需要开发对应的插件即可，这样就将程序的设计和重构变成了工厂的批量化生产，所以这个维度上其实就能减少很多风险。做到以上几点，才能保证重构过程比较平稳和风险可视化。 第2个案例，全链路压测：我们的压测系统经历这么几个阶段：首先是线下压测阶段，这会面临3个问题：测试环境和生产环境配置不一致（比如生产环境配了10个数据库，而测试环境只有5个）；测试环境和生产环境数据结构不一致，（生产环境有些用户可能有50个订单行，而测试环境基本上为了测试方便，可能只构建了1个订单行）；用户访问路径不一致（比如生产环境用户从4级页到收银台有4步，而测试环境直接从金融App进来1步完成。漏斗不同决定了大促的流量比例不同，比如前面有1万TPS，经过3层漏斗只剩1000TPS如果只有2层漏斗，可能剩下5000TPS，对资源的消耗是不一样的）第二个阶段，我们开始做生产憋单压测（比如代扣的订单憋在一起，从收单到支付服务到银行，都可以进行压测，但是对用户进入收银台前面的路径获取不到，基于这个缺点），这样可以实现部分链路的生产环境的真实流量压测；第三节阶段就是，我们目前正在使用的全链路生产压测，就是把全链路串起来；当时我们做的时候，需要解决3个问题：（1）服务的用户商户怎么办？（2）银行不配合压测（3）如何保证支付链路系统的配置准确。解决方案是：在易购建立测试商户和账户，配置虚拟银行，配置影子库影子表，改造中间件，增加影子库单号判断，做到与生产用户数据隔离。 在多活这个方面，我们也经历了几个阶段的发展，初期因为业务量小，我们做了一个稻草系统，它是一个最小可用支付系统，只关注80%主要的银行和支付工具，即便出问题时，能保证核心链路仍可用。这个系统在交易量大肯定是有问题的，下一阶段就开始做支付核心链路failover，但是仍不能解决机房出问题（如停电问题，网络设备问题等）。所以后来做了多活，要做多活就要解决几个核心问题：跨机房耗时问题、依赖服务部署与治理、研发体系配套改造、故障切换的工具支持。我们的解决方案主要是：1、支付链路单元化;2、消息同步服务化;3、依赖服务做多活部署;4、研发体系的配套支持，支持发布到金丝雀环境等；5、机房选址，跨机房调用其实存在很大延迟；6、容灾能力，支持机房级容灾，按系统、按链路容灾； 从单机房到多机房，在架构演进过程中，也要支持容灾，因为演进过程中要做系统发布，逐步切流量。比如整体流量先切换白名单用户，再切换1/256,1/8,1/4，到1/2等，也可以针对单个系统的切换，单个模块的切换等（包括数据源的动态切换，以及切换过程中，安全等问题），便于在建设过程中控制风险； 热点防护包括3部分：1、发现热点，感知热点源，通过埋点，关注请求，关注整个链路依赖的资源；2、热点诊断，主要通过实时分析，离线分析，上下游分析;3、热点治理，最粗暴的直接限流，这个是有损服务，是一刀切。可以稍微再优雅一点，进行故障隔离，比如由于场景1导致的问题，可以直接把场景1切调，对其它场景没有影响，这样可以做到稍微精细化一点；业务场景优化，比如账务核心收支账户分离；热点结构优化，比如说我们在收银台上有一个活动，只取前1000名满100减50，其他人没有资格，其实是通过优化缓存结构实现的；热点拆分，对数据库进行分库，对数据表进行分表， 对记录进行缓存机制处理。基础服务包括统一日志，服务的SLA，决策支持；应用工具包括系统级的紫金大盘、产品级的地动仪，这两个其实是可视化工具，用来观测治理之后的效果。 接下来讲从100tps到20万tps的实践过程。 横向看，从总体架构优化到应用程序优化到数据程序优化到技术组件的优化；纵向看，深入到每一个架构，从收银台到收单到支付服务到渠道到账务核心到清结算进行优化。应用层看链路能否缩短，再看内部服务能否治理，再到线程池调优，去事务。数据层优化，需要考虑收益，比如SQL优化排第一位，因为比分库分表的收益来的快。原则是能用单库尽量用单库，不能用单库时，才考虑分库分表。DB配置参数优化，可以优化引擎参数。因为优化过程会产生资源消耗，所以仍然要考虑业务目标。基础技术平台优化，要遵循体系，服务的本身，依赖方，服务方。中间是RSF分布式服务框架（内部通过这种服务来进行路由和调度。数据方面，分库分表组件和缓存；通信方面，调度组件）的优化。接下来是存储，如SSD，以前是普通硬盘，那么IOPS会比较低，如果本次优化目标是提升2倍，那么只要更换SSD，速度快，成本低，对用户也没有损伤。闭环验证中心，这个是我们做性能优化的一个亮点：特别是重构、性能优化的时候，需要快速知道结果是不是我们想要的，下面的服务日志、调用日志都是比较通用的。监控决策中心：提供灰度方案，移动端应急管控。虽然日常有规则管控，做SLA、流控，但是关键的核心系统出问题（如支付服务），仍需人工介入确认是否执行降级和流控，因为一旦流控，会对用户产生影响。我们最近也在建设大促机器人，实现自动巡检，和智能的治理，这个后面会讲到；然后需要验证大屏，可以直接看到对门店或交易是否有影响。 其实在做系统性能优化的时候，也会伴随研发组织与研发过程的“性能优化”；这个我有一篇文章《业务需求极速变化的高并发金融系统性能优化实践》，里面有详细的分享，主要介绍苏宁金融对业务需求极速变化的高并发金融系统进行性能优化的实践经验，主要包括：智能监控系统，瓶颈点驱动的性能优化，全局规划驱动的性能优化以及研发组织与研发过程的“性能优化”；核心技术点包括瓶颈点的可视化诊断，瓶颈点治理，热插拔架构设计，链路failover设计，应用N+X设计，异步化，数据库单点与热点账户防护；也包括从网络，中间件，应用层，数据层，DB的横向优化方案；以及从架构，代码，会话，缓存，线程与队列，事务，堆内存与GC的纵向优化方案，横纵向结合的体系化解决方案实践； 以上讲述了指导思想和方法论，具体需要怎么操作呢？可视化瓶颈点很重要，这里我举一个示例，比如银行网关系统，调用22次，透明化每一次调用，调用依赖系统多少次，每个系统的SQL有多少，这样可以清晰的可视化链路，保证快速知道哪里出了问题。然后就可以进行庖丁解牛式的优化了； 为什么要做故障自愈？因为出问题时，老板一定会问：影响多少用户？影响多少场景？什么时候恢复？那个时候会非常紧张，也忙于处理生产问题，无法快速回复老板问题，也无法快速想出优化方案，所以需要有个地方可以看到问题影响面、执行什么操作可以恢复。实现这样的系统需要三部分：故障源感知、智能诊断引擎、故障治理。故障源感知：指标分为3类，业务指标、系统指标、基础指标。观测指标发现，几类容易出问题的地方：（1）系统变更，出故障时首先问，昨天有没有发布？系统变更占权重很大；（2）突发业务量，可能某个商品突然很火爆，大促前估不准业务量；（3）操作失误，拓扑获取和链路追踪，知道调用链出了什么问题；（4）单点追踪；（5）安全攻击；通过诊断业务系统暴露的问题，可以将其指标化，才能便于工作的落地与执行：比如高可用时，不能定9999多少个9的目标，可以定MTTR=1分钟的目标。以前会定A完成日志模型，B完成SQL优化，C完成异常治理等，但一段时间后发现并没有解决问题，后来我们定了“北极星”指标MTTR=1分钟，这样技术经理自然的知道需要完成日志模型优化等工作。通过这一个指标去牵动其他指标的达成。故障治理有3方面：场景修复、链路修复、服务修复；服务修复需要提前定义执行引擎，WAF防火墙到负载均衡到RSF到SCM到TCC到DB，形成一个体系。出不同问题，会执行不同的应急预案。最后针对不同的业务流和资金流，做异常数据比对。 最后介绍我们正在做的事情和对未来的展望。机器人巡检，因为大促对我们来说是很重要的节点，每次也耗费很多人力物力，可以实现宏观上系统的整体化构造，微观上看到每个系统的状态。全网可视化作战沙盘，上面的几个指标被称为“北极星指标”，不是开发部门自定的，是根据集团战略目标分解的。战略目标分解到研发中心，研发中心分解到每一个项目。右上角的+号有3个功能：1、全景的产品视图，2、系统的治理，3、考核，每做一个优化需要考核，这样每个团队能形成同一个目标去做事。这样就将人，事全部链接成了一个整体，所有的工作都将形成闭环，同时所有决策层都能可视化的看到；便于所有工作的快速推进和落地；我写的一篇《从百亿到万亿：如何打造一支承担企业战略使命的研发团队》文章中，有更加详细的分享，欢迎一起交流。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>支付设计</category>
      </categories>
      <tags>
        <tag>金融支付</tag>
        <tag>系统设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis的分布式锁是否安全的讨论（下）]]></title>
    <url>%2F2019%2F05%2F28%2Fredis-txn-3%2F</url>
    <content type="text"><![CDATA[自从我写完这个话题的上半部分之后，就感觉头脑中出现了许多细小的声音，久久挥之不去。它们就像是在为了一些鸡毛蒜皮的小事而相互争吵个不停。的确，有关分布式的话题就是这样，琐碎异常，而且每个人说的话听起来似乎都有道理。今天，我们就继续探讨这个话题的后半部分。本文中，我们将从antirez反驳Martin Kleppmann的观点开始讲起，然后会涉及到Hacker News上出现的一些讨论内容，接下来我们还会讨论到基于Zookeeper和Chubby的分布式锁是怎样的，并和Redlock进行一些对比。最后，我们会提到Martin对于这一事件的总结。 还没有看过上半部分的同学，请先阅读：基于Redis的分布式锁是否安全的讨论（上） antirez的反驳Martin在发表了那篇分析分布式锁的blog (How to do distributed locking)之后，该文章在Twitter和Hacker News上引发了广泛的讨论。但人们更想听到的是Redlock的作者antirez对此会发表什么样的看法。Martin的那篇文章是在2016-02-08这一天发表的，但据Martin说，他在公开发表文章的一星期之前就把草稿发给了antirez进行review，而且他们之间通过email进行了讨论。不知道Martin有没有意料到，antirez对于此事的反应很快，就在Martin的文章发表出来的第二天，antirez就在他的博客上贴出了他对于此事的反驳文章，名字叫”Is Redlock safe?”，地址如下:http://antirez.com/news/101这是高手之间的过招。antirez这篇文章也条例非常清晰，并且中间涉及到大量的细节。antirez认为，Martin的文章对于Redlock的批评可以概括为两个方面（与Martin文章的前后两部分对应）：121.带有自动过期功能的分布式锁，必须提供某种fencing机制来保证对共享资源的真正的互斥保护。Redlock提供不了这样一种机制。2.Redlock构建在一个不够安全的系统模型之上。它对于系统的记时假设(timing assumption)有比较强的要求，而这些要求在现实的系统中是无法保证的。 antirez对这两方面分别进行了反驳。首先，关于fencing机制。antirez对于Martin的这种论证方式提出了质疑：既然在锁失效的情况下已经存在一种fencing机制能继续保持资源的互斥访问了，那为什么还要使用一个分布式锁并且还要求它提供那么强的安全性保证呢？即使退一步讲，Redlock虽然提供不了Martin所讲的递增的fencing token，但利用Redlock产生的随机字符串(my_random_value)可以达到同样的效果。这个随机字符串虽然不是递增的，但却是唯一的，可以称之为unique token。antirez举了个例子，比如，你可以用它来实现“Check and Set”操作，原话是：12When starting to work with a shared resource, we set its state to “&lt;token&gt;”, then we operate the read-modify-write only if the token is still the same when we write.（译文：当开始和共享资源交互的时候，我们将它的状态设置成“&lt;token&gt;”，然后仅在token没改变的情况下我们才执行“读取-修改-写回”操作。） 第一遍看到这个描述的时候，我个人是感觉没太看懂的。“Check and Set”应该就是我们平常听到过的CAS操作了，但它如何在这个场景下工作，antirez并没有展开说（在后面讲到Hacker News上的讨论的时候，我们还会提到）。然后，antirez的反驳就集中在第二个方面上：关于算法在记时(timing)方面的模型假设。在我们前面分析Martin的文章时也提到过，Martin认为Redlock会失效的情况主要有三种：1231.时钟发生跳跃。2.长时间的GC pause。3.长时间的网络延迟。 antirez肯定意识到了这三种情况对Redlock最致命的其实是第一点：时钟发生跳跃。这种情况一旦发生，Redlock是没法正常工作的。而对于后两种情况来说，Redlock在当初设计的时候已经考虑到了，对它们引起的后果有一定的免疫力。所以，antirez接下来集中精力来说明通过恰当的运维，完全可以避免时钟发生大的跳动，而Redlock对于时钟的要求在现实系统中是完全可以满足的。Martin在提到时钟跳跃的时候，举了两个可能造成时钟跳跃的具体例子：121.系统管理员手动修改了时钟。2.从NTP服务收到了一个大的时钟更新事件。 antirez反驳说：121.手动修改时钟这种人为原因，不要那么做就是了。否则的话，如果有人手动修改Raft协议的持久化日志，那么就算是Raft协议它也没法正常工作了。2.使用一个不会进行“跳跃”式调整系统时钟的ntpd程序（可能是通过恰当的配置），对于时钟的修改通过多次微小的调整来完成。 而Redlock对时钟的要求，并不需要完全精确，它只需要时钟差不多精确就可以了。比如，要记时5秒，但可能实际记了4.5秒，然后又记了5.5秒，有一定的误差。不过只要误差不超过一定范围，这对Redlock不会产生影响。antirez认为呢，像这样对时钟精度并不是很高的要求，在实际环境中是完全合理的。好了，到此为止，如果你相信antirez这里关于时钟的论断，那么接下来antirez的分析就基本上顺理成章了。关于Martin提到的能使Redlock失效的后两种情况，Martin在分析的时候恰好犯了一个错误（在本文上半部分已经提到过）。在Martin给出的那个由客户端GC pause引发Redlock失效的例子中，这个GC pause引发的后果相当于在锁服务器和客户端之间发生了长时间的消息延迟。Redlock对于这个情况是能处理的。回想一下Redlock算法的具体过程，它使用起来的过程大体可以分成5步：123451.获取当前时间。2.完成获取锁的整个过程（与N个Redis节点交互）。3.再次获取当前时间。4.把两个时间相减，计算获取锁的过程是否消耗了太长时间，导致锁已经过期了。如果没过期，5.客户端持有锁去访问共享资源。 在Martin举的例子中，GC pause或网络延迟，实际发生在上述第1步和第3步之间。而不管在第1步和第3步之间由于什么原因（进程停顿或网络延迟等）导致了大的延迟出现，在第4步都能被检查出来，不会让客户端拿到一个它认为有效而实际却已经过期的锁。当然，这个检查依赖系统时钟没有大的跳跃。这也就是为什么antirez在前面要对时钟条件进行辩护的原因。有人会说，在第3步之后，仍然可能会发生延迟啊。没错，antirez承认这一点，他对此有一段很有意思的论证，原话如下：12The delay can only happen after steps 3, resulting into the lock to be considered ok while actually expired, that is, we are back at the first problem Martin identified of distributed locks where the client fails to stop working to the shared resource before the lock validity expires. Let me tell again how this problem is common with all the distributed locks implementations, and how the token as a solution is both unrealistic and can be used with Redlock as well.（译文：延迟只能发生在第3步之后，这导致锁被认为是有效的而实际上已经过期了，也就是说，我们回到了Martin指出的第一个问题上，客户端没能够在锁的有效性过期之前完成与共享资源的交互。让我再次申明一下，这个问题对于所有的分布式锁的实现是普遍存在的，而且基于token的这种解决方案是不切实际的，但也能和Redlock一起用。） 这里antirez所说的“Martin指出的第一个问题”具体是什么呢？在本文上半部分我们提到过，Martin的文章分为两大部分，其中前半部分与Redlock没有直接关系，而是指出了任何一种带自动过期功能的分布式锁在没有提供fencing机制的前提下都有可能失效。这里antirez所说的就是指的Martin的文章的前半部分。换句话说，对于大延迟给Redlock带来的影响，恰好与Martin在文章的前半部分针对所有的分布式锁所做的分析是一致的，而这种影响不单单针对Redlock。Redlock的实现已经保证了它是和其它任何分布式锁的安全性是一样的。当然，与其它“更完美”的分布式锁相比，Redlock似乎提供不了Martin提出的那种递增的token，但antirez在前面已经分析过了，关于token的这种论证方式本身就是“不切实际”的，或者退一步讲，Redlock能提供的unique token也能够提供完全一样的效果。另外，关于大延迟对Redlock的影响，antirez和Martin在Twitter上有下面的对话：123456789antirez:@martinkl so I wonder if after my reply, we can at least agree about unbound messages delay to don’t cause any harm.Martin:@antirez Agree about message delay between app and lock server. Delay between app and resource being accessed is still problematic.（译文：antirez问：我想知道，在我发文回复之后，我们能否在一点上达成一致，就是大的消息延迟不会给Redlock的运行造成损害。Martin答：对于客户端和锁服务器之间的消息延迟，我同意你的观点。但客户端和被访问资源之间的延迟还是有问题的。） 通过这段对话可以看出，对于Redlock在第4步所做的锁有效性的检查，Martin是予以肯定的。但他认为客户端和资源服务器之间的延迟还是会带来问题的。Martin在这里说的有点模糊。就像antirez前面分析的，客户端和资源服务器之间的延迟，对所有的分布式锁的实现都会带来影响，这不单单是Redlock的问题了。 以上就是antirez在blog中所说的主要内容。有一些点值得我们注意一下：121.antirez是同意大的系统时钟跳跃会造成Redlock失效的。在这一点上，他与Martin的观点的不同在于，他认为在实际系统中是可以避免大的时钟跳跃的。当然，这取决于基础设施和运维方式。2.antirez在设计Redlock的时候，是充分考虑了网络延迟和程序停顿所带来的影响的。但是，对于客户端和资源服务器之间的延迟（即发生在算法第3步之后的延迟），antirez是承认所有的分布式锁的实现，包括Redlock，是没有什么好办法来应对的。 讨论进行到这，Martin和antirez之间谁对谁错其实并不是那么重要了。只要我们能够对Redlock（或者其它分布式锁）所能提供的安全性的程度有充分的了解，那么我们就能做出自己的选择了。 Hacker News上的一些讨论针对Martin和antirez的两篇blog，很多技术人员在Hacker News上展开了激烈的讨论。这些讨论所在地址如下： 针对Martin的blog的讨论：https://news.ycombinator.com/item?id=11059738针对antirez的blog的讨论：https://news.ycombinator.com/item?id=11065933 在Hacker News上，antirez积极参与了讨论，而Martin则始终置身事外。下面我把这些讨论中一些有意思的点拿出来与大家一起分享一下（集中在对于fencing token机制的讨论上）。关于antirez提出的“Check and Set”操作，他在blog里并没有详加说明。果然，在Hacker News上就有人出来问了。antirez给出的答复如下：1You want to modify locked resource X. You set X.currlock = token. Then you read, do whatever you want, and when you write, you “write-if-currlock == token”. If another client did X.currlock = somethingelse, the transaction fails. 翻译一下可以这样理解：假设你要修改资源X，那么遵循下面的伪码所定义的步骤。1231.先设置X.currlock = token。2.读出资源X（包括它的值和附带的X.currlock）。3.按照”write-if-currlock == token”的逻辑，修改资源X的值。意思是说，如果对X进行修改的时候，X.currlock仍然和当初设置进去的token相等，那么才进行修改；如果这时X.currlock已经是其它值了，那么说明有另外一方也在试图进行修改操作，那么放弃当前的修改，从而避免冲突。 随后Hacker News上一位叫viraptor的用户提出了异议，它给出了这样一个执行序列：12345678A: X.currlock = Token_ID_AA: resource readA: is X.currlock still Token_ID_A? yesB: X.currlock = Token_ID_BB: resource readB: is X.currlock still Token_ID_B? yesB: resource writeA: resource write 到了最后两步，两个客户端A和B同时进行写操作，冲突了。不过，这位用户应该是理解错了antirez给出的修改过程了。按照antirez的意思，判断X.currlock是否修改过和对资源的写操作，应该是一个原子操作。只有这样理解才能合乎逻辑，否则的话，这个过程就有严重的破绽。这也是为什么antirez之前会对fencing机制产生质疑：既然资源服务器本身都能提供互斥的原子操作了，为什么还需要一个分布式锁呢？因此，antirez认为这种fencing机制是很累赘的，他之所以还是提出了这种“Check and Set”操作，只是为了证明在提供fencing token这一点上，Redlock也能做到。但是，这里仍然有一些不明确的地方，如果将”write-if-currlock == token”看做是原子操作的话，这个逻辑势必要在资源服务器上执行，那么第二步为什么还要“读出资源X”呢？除非这个“读出资源X”的操作也是在资源服务器上执行，它包含在“判断-写回”这个原子操作里面。而假如不这样理解的话，“读取-判断-写回”这三个操作都放在客户端执行，那么看不出它们如何才能实现原子性操作。在下面的讨论中，我们暂时忽略“读出资源X”这一步。 这个基于random token的“Check and Set”操作，如果与Martin提出的递增的fencing token对比一下的话，至少有两点不同：121.“Check and Set”对于写操作要分成两步来完成（设置token、判断-写回），而递增的fencing token机制只需要一步（带着token向资源服务器发起写请求）。2.递增的fencing token机制能保证最终操作共享资源的顺序，那些延迟时间太长的操作就无法操作共享资源了。但是基于random token的“Check and Set”操作不会保证这个顺序，那些延迟时间太长的操作如果后到达了，它仍然有可能操作共享资源（当然是以互斥的方式）。 对于前一点不同，我们在后面的分析中会看到，如果资源服务器也是分布式的，那么使用递增的fencing token也要变成两步。而对于后一点操作顺序上的不同，antirez认为这个顺序没有意义，关键是能互斥访问就行了。他写下了下面的话：123456So the goal is, when race conditions happen, to avoid them in some way. ……Note also that when it happens that, because of delays, the clients are accessing concurrently, the lock ID has little to do with the order in which the operations were indented to happen.（译文： 我们的目标是，当竞争条件出现的时候，能够以某种方式避免。……还需要注意的是，当那种竞争条件出现的时候，比如由于延迟，客户端是同时来访问的，锁的ID的大小顺序跟那些操作真正想执行的顺序，是没有什么关系的。） 这里的lock ID，跟Martin说的递增的token是一回事。 随后，antirez举了一个“将名字加入列表”的操作的例子：1234567T0: Client A receives new name to add from web.T0: Client B is idleT1: Client A is experiencing pauses.T1: Client B receives new name to add from web.T2: Client A is experiencing pauses.T2: Client B receives a lock with ID 1T3: Client A receives a lock with ID 2 你看，两个客户端（其实是Web服务器）执行“添加名字”的操作，A本来是排在B前面的，但获得锁的顺序却是B排在A前面。因此，antirez说，锁的ID的大小顺序跟那些操作真正想执行的顺序，是没有什么关系的。关键是能排出一个顺序来，能互斥访问就行了。那么，至于锁的ID是递增的，还是一个random token，自然就不那么重要了。 Martin提出的fencing token机制，给人留下了无尽的疑惑。这主要是因为他对于这一机制的描述缺少太多的技术细节。从上面的讨论可以看出，antirez对于这一机制的看法是，它跟一个random token没有什么区别，而且，它需要资源服务器本身提供某种互斥机制，这几乎让分布式锁本身的存在失去了意义。围绕fencing token的问题，还有两点是比较引人注目的，Hacker News上也有人提出了相关的疑问：12（1）关于资源服务器本身的架构细节。（2）资源服务器对于fencing token进行检查的实现细节，比如是否需要提供一种原子操作。 关于上述问题（1），Hacker News上有一位叫dwenzek的用户发表了下面的评论：123…… the issue around the usage of fencing tokens to reject any late usage of a lock is unclear just because the protected resource and its access are themselves unspecified. Is the resource distributed or not? If distributed, does the resource has a mean to ensure that tokens are increasing over all the nodes? Does the resource have a mean to rollback any effects done by a client which session is interrupted by a timeout?（译文：…… 关于使用fencing token拒绝掉延迟请求的相关议题，是不够清晰的，因为受保护的资源以及对它的访问方式本身是没有被明确定义过的。资源服务是不是分布式的呢？如果是，资源服务有没有一种方式能确保token在所有节点上递增呢？对于客户端的Session由于过期而被中断的情况，资源服务有办法将它的影响回滚吗？） 这些疑问在Hacker News上并没有人给出解答。而关于分布式的资源服务器架构如何处理fencing token，另外一名分布式系统的专家Flavio Junqueira在他的一篇blog中有所提及（我们后面会再提到）。关于上述问题（2），Hacker News上有一位叫reza_n的用户发表了下面的疑问：123I understand how a fencing token can prevent out of order writes when 2 clients get the same lock. But what happens when those writes happen to arrive in order and you are doing a value modification? Don’t you still need to rely on some kind of value versioning or optimistic locking? Wouldn’t this make the use of a distributed lock unnecessary?（译文： 我理解当两个客户端同时获得锁的时候fencing token是如何防止乱序的。但是如果两个写操作恰好按序到达了，而且它们在对同一个值进行修改，那会发生什么呢？难道不会仍然是依赖某种数据版本号或者乐观锁的机制？这不会让分布式锁变得没有必要了吗？） 一位叫Terr_的Hacker News用户答：123I believe the “first” write fails, because the token being passed in is no longer “the lastest”, which indicates their lock was already released or expired.（译文： 我认为“第一个”写请求会失败，因为它传入的token不再是“最新的”了，这意味着锁已经释放或者过期了。） Terr_的回答到底对不对呢？这不好说，取决于资源服务器对于fencing token进行检查的实现细节。让我们来简单分析一下。为了简单起见，我们假设有一台（先不考虑分布式的情况）通过RPC进行远程访问文件服务器，它无法提供对于文件的互斥访问（否则我们就不需要分布式锁了）。现在我们按照Martin给出的说法，加入fencing token的检查逻辑。由于Martin没有描述具体细节，我们猜测至少有两种可能。第一种可能，我们修改了文件服务器的代码，让它能多接受一个fencing token的参数，并在进行所有处理之前加入了一个简单的判断逻辑，保证只有当前接收到的fencing token大于之前的值才允许进行后边的访问。而一旦通过了这个判断，后面的处理不变。现在想象reza_n描述的场景，客户端1和客户端2都发生了GC pause，两个fencing token都延迟了，它们几乎同时到达了文件服务器，而且保持了顺序。那么，我们新加入的判断逻辑，应该对两个请求都会放过，而放过之后它们几乎同时在操作文件，还是冲突了。既然Martin宣称fencing token能保证分布式锁的正确性，那么上面这种可能的猜测也许是我们理解错了。 当然，还有第二种可能，就是我们对文件服务器确实做了比较大的改动，让这里判断token的逻辑和随后对文件的处理放在一个原子操作里了。这可能更接近antirez的理解。这样的话，前面reza_n描述的场景中，两个写操作都应该成功。 基于ZooKeeper的分布式锁更安全吗？很多人（也包括Martin在内）都认为，如果你想构建一个更安全的分布式锁，那么应该使用ZooKeeper，而不是Redis。那么，为了对比的目的，让我们先暂时脱离开本文的题目，讨论一下基于ZooKeeper的分布式锁能提供绝对的安全吗？它需要fencing token机制的保护吗？我们不得不提一下分布式专家Flavio Junqueira所写的一篇blog，题目叫“Note on fencing and distributed locks”，地址如下：https://fpj.me/2016/02/10/note-on-fencing-and-distributed-locks/Flavio Junqueira是ZooKeeper的作者之一，他的这篇blog就写在Martin和antirez发生争论的那几天。他在文中给出了一个基于ZooKeeper构建分布式锁的描述（当然这不是唯一的方式）：1231.客户端尝试创建一个znode节点，比如/lock。那么第一个客户端就创建成功了，相当于拿到了锁；而其它的客户端会创建失败（znode已存在），获取锁失败。2.持有锁的客户端访问共享资源完成后，将znode删掉，这样其它客户端接下来就能来获取锁了。3.znode应该被创建成ephemeral的。这是znode的一个特性，它保证如果创建znode的那个客户端崩溃了，那么相应的znode会被自动删除。这保证了锁一定会被释放。 看起来这个锁相当完美，没有Redlock过期时间的问题，而且能在需要的时候让锁自动释放。但仔细考察的话，并不尽然。 ZooKeeper是怎么检测出某个客户端已经崩溃了呢？实际上，每个客户端都与ZooKeeper的某台服务器维护着一个Session，这个Session依赖定期的心跳(heartbeat)来维持。如果ZooKeeper长时间收不到客户端的心跳（这个时间称为Sesion的过期时间），那么它就认为Session过期了，通过这个Session所创建的所有的ephemeral类型的znode节点都会被自动删除。设想如下的执行序列：123451.客户端1创建了znode节点/lock，获得了锁。2.客户端1进入了长时间的GC pause。3.客户端1连接到ZooKeeper的Session过期了。znode节点/lock被自动删除。4.客户端2创建了znode节点/lock，从而获得了锁。5.客户端1从GC pause中恢复过来，它仍然认为自己持有锁。 最后，客户端1和客户端2都认为自己持有了锁，冲突了。这与之前Martin在文章中描述的由于GC pause导致的分布式锁失效的情况类似。看起来，用ZooKeeper实现的分布式锁也不一定就是安全的。该有的问题它还是有。但是，ZooKeeper作为一个专门为分布式应用提供方案的框架，它提供了一些非常好的特性，是Redis之类的方案所没有的。像前面提到的ephemeral类型的znode自动删除的功能就是一个例子。还有一个很有用的特性是ZooKeeper的watch机制。这个机制可以这样来使用，比如当客户端试图创建/lock的时候，发现它已经存在了，这时候创建失败，但客户端不一定就此对外宣告获取锁失败。客户端可以进入一种等待状态，等待当/lock节点被删除的时候，ZooKeeper通过watch机制通知它，这样它就可以继续完成创建操作（获取锁）。这可以让分布式锁在客户端用起来就像一个本地的锁一样：加锁失败就阻塞住，直到获取到锁为止。这样的特性Redlock就无法实现。 小结一下，基于ZooKeeper的锁和基于Redis的锁相比在实现特性上有两个不同：12在正常情况下，客户端可以持有锁任意长的时间，这可以确保它做完所有需要的资源访问操作之后再释放锁。这避免了基于Redis的锁对于有效时间(lock validity time)到底设置多长的两难问题。实际上，基于ZooKeeper的锁是依靠Session（心跳）来维持锁的持有状态的，而Redis不支持Sesion。基于ZooKeeper的锁支持在获取锁失败之后等待锁重新释放的事件。这让客户端对锁的使用更加灵活。 顺便提一下，如上所述的基于ZooKeeper的分布式锁的实现，并不是最优的。它会引发“herd effect”（羊群效应），降低获取锁的性能。一个更好的实现参见下面链接：http://zookeeper.apache.org/doc/r3.4.9/recipes.html#sc_recipes_Locks 我们重新回到Flavio Junqueira对于fencing token的分析。Flavio Junqueira指出，fencing token机制本质上是要求客户端在每次访问一个共享资源的时候，在执行任何操作之前，先对资源进行某种形式的“标记”(mark)操作，这个“标记”能保证持有旧的锁的客户端请求（如果延迟到达了）无法操作资源。这种标记操作可以是很多形式，fencing token是其中比较典型的一个。随后Flavio Junqueira提到用递增的epoch number（相当于Martin的fencing token）来保护共享资源。而对于分布式的资源，为了方便讨论，假设分布式资源是一个小型的多备份的数据存储(a small replicated data store)，执行写操作的时候需要向所有节点上写数据。最简单的做标记的方式，就是在对资源进行任何操作之前，先把epoch number标记到各个资源节点上去。这样，各个节点就保证了旧的（也就是小的）epoch number无法操作数据。当然，这里再展开讨论下去可能就涉及到了这个数据存储服务的实现细节了。比如在实际系统中，可能为了容错，只要上面讲的标记和写入操作在多数节点上完成就算成功完成了（Flavio Junqueira并没有展开去讲）。在这里我们能看到的，最重要的，是这种标记操作如何起作用的方式。这有点类似于Paxos协议（Paxos协议要求每个proposal对应一个递增的数字，执行accept请求之前先执行prepare请求）。antirez提出的random token的方式显然不符合Flavio Junqueira对于“标记”操作的定义，因为它无法区分新的token和旧的token。只有递增的数字才能确保最终收敛到最新的操作结果上。在这个分布式数据存储服务（共享资源）的例子中，客户端在标记完成之后执行写入操作的时候，存储服务的节点需要判断epoch number是不是最新，然后确定能不能执行写入操作。如果按照上一节我们的分析思路，这里的epoch判断和接下来的写入操作，是不是在一个原子操作里呢？根据Flavio Junqueira的相关描述，我们相信，应该是原子的。那么既然资源本身可以提供原子互斥操作了，那么分布式锁还有存在的意义吗？应该说有。客户端可以利用分布式锁有效地避免冲突，等待写入机会，这对于包含多个节点的分布式资源尤其有用（当然，是出于效率的原因）。 Chubby的分布式锁是怎样做fencing的？提到分布式锁，就不能不提Google的Chubby。Chubby是Google内部使用的分布式锁服务，有点类似于ZooKeeper，但也存在很多差异。Chubby对外公开的资料，主要是一篇论文，叫做“The Chubby lock service for loosely-coupled distributed systems”，下载地址如下：https://research.google.com/archive/chubby.html 另外，YouTube上有一个的讲Chubby的talk，也很不错，播放地址：https://www.youtube.com/watch?v=PqItueBaiRg&amp;feature=youtu.be&amp;t=487Chubby自然也考虑到了延迟造成的锁失效的问题。论文里有一段描述如下：123a process holding a lock L may issue a request R, but then fail. Another process may ac- quire L and perform some action before R arrives at its destination. If R later arrives, it may be acted on without the protection of L, and potentially on inconsistent data.（译文： 一个进程持有锁L，发起了请求R，但是请求失败了。另一个进程获得了锁L并在请求R到达目的方之前执行了一些动作。如果后来请求R到达了，它就有可能在没有锁L保护的情况下进行操作，带来数据不一致的潜在风险。） 这跟Martin的分析大同小异。Chubby给出的用于解决（缓解）这一问题的机制称为sequencer，类似于fencing token机制。锁的持有者可以随时请求一个sequencer，这是一个字节串，它由三部分组成：1231.锁的名字。2.锁的获取模式（排他锁还是共享锁）。3.lock generation number（一个64bit的单调递增数字）。作用相当于fencing token或epoch number。 客户端拿到sequencer之后，在操作资源的时候把它传给资源服务器。然后，资源服务器负责对sequencer的有效性进行检查。检查可以有两种方式：121.调用Chubby提供的API，CheckSequencer()，将整个sequencer传进去进行检查。这个检查是为了保证客户端持有的锁在进行资源访问的时候仍然有效。2.将客户端传来的sequencer与资源服务器当前观察到的最新的sequencer进行对比检查。可以理解为与Martin描述的对于fencing token的检查类似。 当然，如果由于兼容的原因，资源服务本身不容易修改，那么Chubby还提供了一种机制：1lock-delay。Chubby允许客户端为持有的锁指定一个lock-delay的时间值（默认是1分钟）。当Chubby发现客户端被动失去联系的时候，并不会立即释放锁，而是会在lock-delay指定的时间内阻止其它客户端获得这个锁。这是为了在把锁分配给新的客户端之前，让之前持有锁的客户端有充分的时间把请求队列排空(draining the queue)，尽量防止出现延迟到达的未处理请求。 可见，为了应对锁失效问题，Chubby提供的三种处理方式：CheckSequencer()检查、与上次最新的sequencer对比、lock-delay，它们对于安全性的保证是从强到弱的。而且，这些处理方式本身都没有保证提供绝对的正确性(correctness)。但是，Chubby确实提供了单调递增的lock generation number，这就允许资源服务器在需要的时候，利用它提供更强的安全性保障。 关于时钟在Martin与antirez的这场争论中，冲突最为严重的就是对于系统时钟的假设是不是合理的问题。Martin认为系统时钟难免会发生跳跃（这与分布式算法的异步模型相符），而antirez认为在实际中系统时钟可以保证不发生大的跳跃。Martin对于这一分歧发表了如下看法（原话）：123So, fundamentally, this discussion boils down to whether it is reasonable to make timing assumptions for ensuring safety properties. I say no, Salvatore says yes — but that’s ok. Engineering discussions rarely have one right answer.（译文： 从根本上来说，这场讨论最后归结到了一个问题上：为了确保安全性而做出的记时假设到底是否合理。我认为不合理，而antirez认为合理 —— 但是这也没关系。工程问题的讨论很少只有一个正确答案。） 那么，在实际系统中，时钟到底是否可信呢？对此，Julia Evans专门写了一篇文章，“TIL: clock skew exists”，总结了很多跟时钟偏移有关的实际资料，并进行了分析。这篇文章地址：http://jvns.ca/blog/2016/02/09/til-clock-skew-exists/Julia Evans在文章最后得出的结论是：clock skew is real （时钟偏移在现实中是存在的） Martin的事后总结我们前面提到过，当各方的争论在激烈进行的时候，Martin几乎始终置身事外。但是Martin在这件事过去之后，把这个事件的前后经过总结成了一个很长的故事线。如果你想最全面地了解这个事件发生的前后经过，那么建议去读读Martin的这个总结：https://storify.com/martinkl/redlock-discussion在这个故事总结的最后，Martin写下了很多感性的评论：12345678For me, this is the most important point: I don’t care who is right or wrong in this debate — I care about learning from others’ work, so that we can avoid repeating old mistakes, and make things better in future. So much great work has already been done for us: by standing on the shoulders of giants, we can build better software.……By all means, test ideas by arguing them and checking whether they stand up to scrutiny by others. That’s part of the learning process. But the goal should be to learn, not to convince others that you are right. Sometimes that just means to stop and think for a while.（译文： 对我来说最重要的一点在于：我并不在乎在这场辩论中谁对谁错 —— 我只关心从其他人的工作中学到的东西，以便我们能够避免重蹈覆辙，并让未来更加美好。前人已经为我们创造出了许多伟大的成果：站在巨人的肩膀上，我们得以构建更棒的软件。……对于任何想法，务必要详加检验，通过论证以及检查它们是否经得住别人的详细审查。那是学习过程的一部分。但目标应该是为了获得知识，而不应该是为了说服别人相信你自己是对的。有时候，那只不过意味着停下来，好好地想一想。） 关于分布式锁的这场争论，我们已经完整地做了回顾和分析。 按照锁的两种用途，如果仅是为了效率(efficiency)，那么你可以自己选择你喜欢的一种分布式锁的实现。当然，你需要清楚地知道它在安全性上有哪些不足，以及它会带来什么后果。而如果你是为了正确性(correctness)，那么请慎之又慎。在本文的讨论中，我们在分布式锁的正确性上走得最远的地方，要数对于ZooKeeper分布式锁、单调递增的epoch number以及对分布式资源进行标记的分析了。请仔细审查相关的论证。 Martin为我们留下了不少疑问，尤其是他提出的fencing token机制。他在blog中提到，会在他的新书《Designing Data-Intensive Applications》的第8章和第9章再详加论述。目前，这本书尚在预售当中。我感觉，这会是一本值得一读的书，它不同于为了出名或赚钱而出版的那种短平快的书籍。可以看出作者在这本书上投入了巨大的精力。最后，我相信，这个讨论还远没有结束。分布式锁(Distributed Locks)和相应的fencing方案，可以作为一个长期的课题，随着我们对分布式系统的认识逐渐增加，可以再来慢慢地思考它。思考它更深层的本质，以及它在理论上的证明。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Redis的分布式锁是否安全的讨论（上）]]></title>
    <url>%2F2019%2F05%2F28%2Fredis-txn-2%2F</url>
    <content type="text"><![CDATA[网上有关Redis分布式锁的文章可谓多如牛毛了，不信的话你可以拿关键词“Redis 分布式锁”随便到哪个搜索引擎上去搜索一下就知道了。这些文章的思路大体相近，给出的实现算法也看似合乎逻辑，但当我们着手去实现它们的时候，却发现如果你越是仔细推敲，疑虑也就越来越多。 实际上，大概在一年以前，关于Redis分布式锁的安全性问题，在分布式系统专家Martin Kleppmann和Redis的作者antirez之间就发生过一场争论。由于对这个问题一直以来比较关注，所以我前些日子仔细阅读了与这场争论相关的资料。这场争论的大概过程是这样的：为了规范各家对基于Redis的分布式锁的实现，Redis的作者提出了一个更安全的实现，叫做Redlock。有一天，Martin Kleppmann写了一篇blog，分析了Redlock在安全性上存在的一些问题。然后Redis的作者立即写了一篇blog来反驳Martin的分析。但Martin表示仍然坚持原来的观点。随后，这个问题在Twitter和Hacker News上引发了激烈的讨论，很多分布式系统的专家都参与其中。 对于那些对分布式系统感兴趣的人来说，这个事件非常值得关注。不管你是刚接触分布式系统的新手，还是有着多年分布式开发经验的老手，读完这些分析和评论之后，大概都会有所收获。要知道，亲手实现过Redis Cluster这样一个复杂系统的antirez，足以算得上分布式领域的一名专家了。但对于由分布式锁引发的一系列问题的分析中，不同的专家却能得出迥异的结论，从中我们可以窥见分布式系统相关的问题具有何等的复杂性。实际上，在分布式系统的设计中经常发生的事情是：许多想法初看起来毫无破绽，而一旦详加考量，却发现不是那么天衣无缝。 下面，我们就从头至尾把这场争论过程中各方的观点进行一下回顾和分析。在这个过程中，我们把影响分布式锁的安全性的那些技术细节展开进行讨论，这将是一件很有意思的事情。这也是一个比较长的故事。当然，其中也免不了包含一些小“八卦”。 Redlock算法就像本文开头所讲的，借助Redis来实现一个分布式锁(Distributed Lock)的做法，已经有很多人尝试过。人们构建这样的分布式锁的目的，是为了对一些共享资源进行互斥访问。但是，这些实现虽然思路大体相近，但实现细节上各不相同，它们能提供的安全性和可用性也不尽相同。所以，Redis的作者antirez给出了一个更好的实现，称为Redlock，算是Redis官方对于实现分布式锁的指导规范。Redlock的算法描述就放在Redis的官网上：https://redis.io/topics/distlock在Redlock之前，很多人对于分布式锁的实现都是基于单个Redis节点的。而Redlock是基于多个Redis节点（都是Master）的一种实现。为了能理解Redlock，我们首先需要把简单的基于单Redis节点的算法描述清楚，因为它是Redlock的基础。 基于单Redis节点的分布式锁首先，Redis客户端为了获取锁，向Redis节点发送如下命令：1SET resource_name my_random_value NX PX 30000 上面的命令如果执行成功，则客户端成功获取到了锁，接下来就可以访问共享资源了；而如果上面的命令执行失败，则说明获取锁失败。注意，在上面的SET命令中：my_random_value是由客户端生成的一个随机字符串，它要保证在足够长的一段时间内在所有客户端的所有获取锁的请求中都是唯一的。NX表示只有当resource_name对应的key值不存在的时候才能SET成功。这保证了只有第一个请求的客户端才能获得锁，而其它客户端在锁被释放之前都无法获得锁。PX 30000表示这个锁有一个30秒的自动过期时间。当然，这里30秒只是一个例子，客户端可以选择合适的过期时间。最后，当客户端完成了对共享资源的操作之后，执行下面的Redis Lua脚本来释放锁：12345if redis.call("get",KEYS[1]) == ARGV[1] then return redis.call("del",KEYS[1])else return 0end 这段Lua脚本在执行的时候要把前面的my_random_value作为ARGV[1]的值传进去，把resource_name作为KEYS[1]的值传进去。至此，基于单Redis节点的分布式锁的算法就描述完了。这里面有好几个问题需要重点分析一下。 首先第一个问题，这个锁必须要设置一个过期时间。否则的话，当一个客户端获取锁成功之后，假如它崩溃了，或者由于发生了网络分割（network partition）导致它再也无法和Redis节点通信了，那么它就会一直持有这个锁，而其它客户端永远无法获得锁了。antirez在后面的分析中也特别强调了这一点，而且把这个过期时间称为锁的有效时间(lock validity time)。获得锁的客户端必须在这个时间之内完成对共享资源的访问。第二个问题，第一步获取锁的操作，网上不少文章把它实现成了两个Redis命令：12SETNX resource_name my_random_valueEXPIRE resource_name 30 虽然这两个命令和前面算法描述中的一个SET命令执行效果相同，但却不是原子的。如果客户端在执行完SETNX后崩溃了，那么就没有机会执行EXPIRE了，导致它一直持有这个锁。第三个问题，也是antirez指出的，设置一个随机字符串my_random_value是很有必要的，它保证了一个客户端释放的锁必须是自己持有的那个锁。假如获取锁时SET的不是一个随机字符串，而是一个固定值，那么可能会发生下面的执行序列：123451.客户端1获取锁成功。2.客户端1在某个操作上阻塞了很长时间。3.过期时间到了，锁自动释放了。4.客户端2获取到了对应同一个资源的锁。5.客户端1从阻塞中恢复过来，释放掉了客户端2持有的锁。之后，客户端2在访问共享资源的时候，就没有锁为它提供保护了。 第四个问题，释放锁的操作必须使用Lua脚本来实现。释放锁其实包含三步操作：’GET’、判断和’DEL’，用Lua脚本来实现能保证这三步的原子性。否则，如果把这三步操作放到客户端逻辑中去执行的话，就有可能发生与前面第三个问题类似的执行序列：123456781.客户端1获取锁成功。2.客户端1访问共享资源。3.客户端1为了释放锁，先执行’GET’操作获取随机字符串的值。4.客户端1判断随机字符串的值，与预期的值相等。5.客户端1由于某个原因阻塞住了很长时间。6.过期时间到了，锁自动释放了。7.客户端2获取到了对应同一个资源的锁。8.客户端1从阻塞中恢复过来，执行DEL操纵，释放掉了客户端2持有的锁。 实际上，在上述第三个问题和第四个问题的分析中，如果不是客户端阻塞住了，而是出现了大的网络延迟，也有可能导致类似的执行序列发生。前面的四个问题，只要实现分布式锁的时候加以注意，就都能够被正确处理。但除此之外，antirez还指出了一个问题，是由failover引起的，却是基于单Redis节点的分布式锁无法解决的。正是这个问题催生了Redlock的出现。这个问题是这样的。假如Redis节点宕机了，那么所有客户端就都无法获得锁了，服务变得不可用。为了提高可用性，我们可以给这个Redis节点挂一个Slave，当Master节点不可用的时候，系统自动切到Slave上（failover）。但由于Redis的主从复制（replication）是异步的，这可能导致在failover过程中丧失锁的安全性。考虑下面的执行序列：12341.客户端1从Master获取了锁。2.Master宕机了，存储锁的key还没有来得及同步到Slave上。3.Slave升级为Master。4.客户端2从新的Master获取到了对应同一个资源的锁。 于是，客户端1和客户端2同时持有了同一个资源的锁。锁的安全性被打破。针对这个问题，antirez设计了Redlock算法，我们接下来会讨论。 【其它疑问】前面这个算法中出现的锁的有效时间(lock validity time)，设置成多少合适呢？如果设置太短的话，锁就有可能在客户端完成对于共享资源的访问之前过期，从而失去保护；如果设置太长的话，一旦某个持有锁的客户端释放锁失败，那么就会导致所有其它客户端都无法获取锁，从而长时间内无法正常工作。看来真是个两难的问题。而且，在前面对于随机字符串my_random_value的分析中，antirez也在文章中承认的确应该考虑客户端长期阻塞导致锁过期的情况。如果真的发生了这种情况，那么共享资源是不是已经失去了保护呢？antirez重新设计的Redlock是否能解决这些问题呢？ 分布式锁Redlock由于前面介绍的基于单Redis节点的分布式锁在failover的时候会产生解决不了的安全性问题，因此antirez提出了新的分布式锁的算法Redlock，它基于N个完全独立的Redis节点（通常情况下N可以设置成5）。运行Redlock算法的客户端依次执行下面各个步骤，来完成获取锁的操作：123451.获取当前时间（毫秒数）。2.按顺序依次向N个Redis节点执行获取锁的操作。这个获取操作跟前面基于单Redis节点的获取锁的过程相同，包含随机字符串my_random_value，也包含过期时间(比如PX 30000，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。这里的失败，应该包含任何类型的失败，比如该Redis节点不可用，或者该Redis节点上的锁已经被其它客户端持有（注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）。3.计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（&gt;= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。4.如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。5.如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起释放锁的操作（即前面介绍的Redis Lua脚本）。 当然，上面描述的只是获取锁的过程，而释放锁的过程比较简单：客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁的时候成功与否。 由于N个Redis节点中的大多数能正常工作就能保证Redlock正常工作，因此理论上它的可用性更高。我们前面讨论的单Redis节点的分布式锁在failover的时候锁失效的问题，在Redlock中不存在了，但如果有节点发生崩溃重启，还是会对锁的安全性有影响的。具体的影响程度跟Redis对数据的持久化程度有关。 假设一共有5个Redis节点：A, B, C, D, E。设想发生了如下的事件序列：1231.客户端1成功锁住了A, B, C，获取锁成功（但D和E没有锁住）。2.节点C崩溃重启了，但客户端1在C上加的锁没有持久化下来，丢失了。3.节点C重启后，客户端2锁住了C, D, E，获取锁成功。 这样，客户端1和客户端2同时获得了锁（针对同一资源）。在默认情况下，Redis的AOF持久化方式是每秒写一次磁盘（即执行fsync），因此最坏情况下可能丢失1秒的数据。为了尽可能不丢数据，Redis允许设置成每次修改数据都进行fsync，但这会降低性能。当然，即使执行了fsync也仍然有可能丢失数据（这取决于系统而不是Redis的实现）。所以，上面分析的由于节点重启引发的锁失效问题，总是有可能出现的。为了应对这一问题，antirez又提出了延迟重启(delayed restarts)的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间(lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。 关于Redlock还有一点细节值得拿出来分析一下：在最后释放锁的时候，antirez在算法描述中特别强调，客户端应该向所有Redis节点发起释放锁的操作。也就是说，即使当时向某个节点获取锁没有成功，在释放锁的时候也不应该漏掉这个节点。这是为什么呢？设想这样一种情况，客户端发给某个Redis节点的获取锁的请求成功到达了该Redis节点，这个节点也成功执行了SET操作，但是它返回给客户端的响应包却丢失了。这在客户端看来，获取锁的请求由于超时而失败了，但在Redis这边看来，加锁已经成功了。因此，释放锁的时候，客户端也应该对当时获取锁失败的那些Redis节点同样发起请求。实际上，这种情况在异步通信模型中是有可能发生的：客户端向服务器通信是正常的，但反方向却是有问题的。 【其它疑问】前面在讨论单Redis节点的分布式锁的时候，最后我们提出了一个疑问，如果客户端长期阻塞导致锁过期，那么它接下来访问共享资源就不安全了（没有了锁的保护）。这个问题在Redlock中是否有所改善呢？显然，这样的问题在Redlock中是依然存在的。另外，在算法第4步成功获取了锁之后，如果由于获取锁的过程消耗了较长时间，重新计算出来的剩余的锁有效时间很短了，那么我们还来得及去完成共享资源访问吗？如果我们认为太短，是不是应该立即进行锁的释放操作？那到底多短才算呢？又是一个选择难题。 Martin的分析Martin Kleppmann在2016-02-08这一天发表了一篇blog，名字叫”How to do distributed locking “，地址如下：https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.htmlMartin在这篇文章中谈及了分布式系统的很多基础性的问题（特别是分布式计算的异步模型），对分布式系统的从业者来说非常值得一读。这篇文章大体可以分为两大部分： 前半部分，与Redlock无关。Martin指出，即使我们拥有一个完美实现的分布式锁（带自动过期功能），在没有共享资源参与进来提供某种fencing机制的前提下，我们仍然不可能获得足够的安全性。后半部分，是对Redlock本身的批评。Martin指出，由于Redlock本质上是建立在一个同步模型之上，对系统的记时假设(timing assumption)有很强的要求，因此本身的安全性是不够的。首先我们讨论一下前半部分的关键点。Martin给出了下面这样一份时序图：分布式锁失效的时序 在上面的时序图中，假设锁服务本身是没有问题的，它总是能保证任一时刻最多只有一个客户端获得锁。上图中出现的lease这个词可以暂且认为就等同于一个带有自动过期功能的锁。客户端1在获得锁之后发生了很长时间的GC pause，在此期间，它获得的锁过期了，而客户端2获得了锁。当客户端1从GC pause中恢复过来的时候，它不知道自己持有的锁已经过期了，它依然向共享资源（上图中是一个存储服务）发起了写数据请求，而这时锁实际上被客户端2持有，因此两个客户端的写请求就有可能冲突（锁的互斥作用失效了）。初看上去，有人可能会说，既然客户端1从GC pause中恢复过来以后不知道自己持有的锁已经过期了，那么它可以在访问共享资源之前先判断一下锁是否过期。但仔细想想，这丝毫也没有帮助。因为GC pause可能发生在任意时刻，也许恰好在判断完之后。 也有人会说，如果客户端使用没有GC的语言来实现，是不是就没有这个问题呢？Martin指出，系统环境太复杂，仍然有很多原因导致进程的pause，比如虚存造成的缺页故障(page fault)，再比如CPU资源的竞争。即使不考虑进程pause的情况，网络延迟也仍然会造成类似的结果。 总结起来就是说，即使锁服务本身是没有问题的，而仅仅是客户端有长时间的pause或网络延迟，仍然会造成两个客户端同时访问共享资源的冲突情况发生。而这种情况其实就是我们在前面已经提出来的“客户端长期阻塞导致锁过期”的那个疑问。 那怎么解决这个问题呢？Martin给出了一种方法，称为fencing token。fencing token是一个单调递增的数字，当客户端成功获取锁的时候它随同锁一起返回给客户端。而客户端访问共享资源的时候带着这个fencing token，这样提供共享资源的服务就能根据它进行检查，拒绝掉延迟到来的访问请求（避免了冲突）。如下图：带有fencing token的时序 在上图中，客户端1先获取到的锁，因此有一个较小的fencing token，等于33，而客户端2后获取到的锁，有一个较大的fencing token，等于34。客户端1从GC pause中恢复过来之后，依然是向存储服务发送访问请求，但是带了fencing token = 33。存储服务发现它之前已经处理过34的请求，所以会拒绝掉这次33的请求。这样就避免了冲突。 现在我们再讨论一下Martin的文章的后半部分。Martin在文中构造了一些事件序列，能够让Redlock失效（两个客户端同时持有锁）。为了说明Redlock对系统记时(timing)的过分依赖，他首先给出了下面的一个例子（还是假设有5个Redis节点A, B, C, D, E）：12341.客户端1从Redis节点A, B, C成功获取了锁（多数节点）。由于网络问题，与D和E通信失败。2.节点C上的时钟发生了向前跳跃，导致它上面维护的锁快速过期。3.客户端2从Redis节点C, D, E成功获取了同一个资源的锁（多数节点）。4.客户端1和客户端2现在都认为自己持有了锁。 上面这种情况之所以有可能发生，本质上是因为Redlock的安全性(safety property)对系统的时钟有比较强的依赖，一旦系统的时钟变得不准确，算法的安全性也就保证不了了。Martin在这里其实是要指出分布式算法研究中的一些基础性问题，或者说一些常识问题，即好的分布式算法应该基于异步模型(asynchronous model)，算法的安全性不应该依赖于任何记时假设(timing assumption)。在异步模型中：进程可能pause任意长的时间，消息可能在网络中延迟任意长的时间，甚至丢失，系统时钟也可能以任意方式出错。一个好的分布式算法，这些因素不应该影响它的安全性(safety property)，只可能影响到它的活性(liveness property)，也就是说，即使在非常极端的情况下（比如系统时钟严重错误），算法顶多是不能在有限的时间内给出结果而已，而不应该给出错误的结果。这样的算法在现实中是存在的，像比较著名的Paxos，或Raft。但显然按这个标准的话，Redlock的安全性级别是达不到的。 随后，Martin觉得前面这个时钟跳跃的例子还不够，又给出了一个由客户端GC pause引发Redlock失效的例子。如下：1234561.客户端1向Redis节点A, B, C, D, E发起锁请求。2.各个Redis节点已经把请求结果返回给了客户端1，但客户端1在收到请求结果之前进入了长时间的GC pause。3.在所有的Redis节点上，锁过期了。4.客户端2在A, B, C, D, E上获取到了锁。5.客户端1从GC pause从恢复，收到了前面第2步来自各个Redis节点的请求结果。客户端1认为自己成功获取到了锁。6.客户端1和客户端2现在都认为自己持有了锁。 Martin给出的这个例子其实有点小问题。在Redlock算法中，客户端在完成向各个Redis节点的获取锁的请求之后，会计算这个过程消耗的时间，然后检查是不是超过了锁的有效时间(lock validity time)。也就是上面的例子中第5步，客户端1从GC pause中恢复过来以后，它会通过这个检查发现锁已经过期了，不会再认为自己成功获取到锁了。随后antirez在他的反驳文章中就指出来了这个问题，但Martin认为这个细节对Redlock整体的安全性没有本质的影响。 抛开这个细节，我们可以分析一下Martin举这个例子的意图在哪。初看起来，这个例子跟文章前半部分分析通用的分布式锁时给出的GC pause的时序图是基本一样的，只不过那里的GC pause发生在客户端1获得了锁之后，而这里的GC pause发生在客户端1获得锁之前。但两个例子的侧重点不太一样。Martin构造这里的这个例子，是为了强调在一个分布式的异步环境下，长时间的GC pause或消息延迟（上面这个例子中，把GC pause换成Redis节点和客户端1之间的消息延迟，逻辑不变），会让客户端获得一个已经过期的锁。从客户端1的角度看，Redlock的安全性被打破了，因为客户端1收到锁的时候，这个锁已经失效了，而Redlock同时还把这个锁分配给了客户端2。换句话说，Redis服务器在把锁分发给客户端的途中，锁就过期了，但又没有有效的机制让客户端明确知道这个问题。而在之前的那个例子中，客户端1收到锁的时候锁还是有效的，锁服务本身的安全性可以认为没有被打破，后面虽然也出了问题，但问题是出在客户端1和共享资源服务器之间的交互上。 在Martin的这篇文章中，还有一个很有见地的观点，就是对锁的用途的区分。他把锁的用途分为两种： 为了效率(efficiency)。协调各个客户端避免做重复的工作。即使锁偶尔失效了，只是可能把某些操作多做一遍而已，不会产生其它的不良后果。比如重复发送了一封同样的email。为了正确性(correctness)。在任何情况下都不允许锁失效的情况发生，因为一旦发生，就可能意味着数据不一致(inconsistency)，数据丢失，文件损坏，或者其它严重的问题。最后，Martin得出了如下的结论：1.如果是为了效率(efficiency)而使用分布式锁，允许锁的偶尔失效，那么使用单Redis节点的锁方案就足够了，简单而且效率高。Redlock则是个过重的实现(heavyweight)。2.如果是为了正确性(correctness)在很严肃的场合使用分布式锁，那么不要使用Redlock。它不是建立在异步模型上的一个足够强的算法，它对于系统模型的假设中包含很多危险的成分(对于timing)。而且，它没有一个机制能够提供fencing token。那应该使用什么技术呢？Martin认为，应该考虑类似Zookeeper的方案，或者支持事务的数据库。Martin对Redlock算法的形容是：neither fish nor fowl （非驴非马） 【其它疑问】1.Martin提出的fencing token的方案，需要对提供共享资源的服务进行修改，这在现实中可行吗？2.根据Martin的说法，看起来，如果资源服务器实现了fencing token，它在分布式锁失效的情况下也仍然能保持资源的互斥访问。这是不是意味着分布式锁根本没有存在的意义了？3.资源服务器需要检查fencing token的大小，如果提供资源访问的服务也是包含多个节点的（分布式的），那么这里怎么检查才能保证fencing token在多个节点上是递增的呢？4.Martin对于fencing token的举例中，两个fencing token到达资源服务器的顺序颠倒了（小的fencing token后到了），这时资源服务器检查出了这一问题。如果客户端1和客户端2都发生了GC pause，两个fencing token都延迟了，它们几乎同时到达了资源服务器，但保持了顺序，那么资源服务器是不是就检查不出问题了？这时对于资源的访问是不是就发生冲突了？5.分布式锁+fencing的方案是绝对正确的吗？能证明吗？ 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis分布式锁的正确实现方式]]></title>
    <url>%2F2019%2F05%2F28%2Fredis-txn-1%2F</url>
    <content type="text"><![CDATA[前言分布式锁一般有三种实现方式： 数据库乐观锁； 基于Redis的分布式锁； 基于ZooKeeper的分布式锁。本篇博客将介绍第二种方式，基于Redis实现分布式锁。虽然网上已经有各种介绍Redis分布式锁实现的博客，然而他们的实现却有着各种各样的问题，为了避免误人子弟，本篇博客将详细介绍如何正确地实现Redis分布式锁。 可靠性首先，为了确保分布式锁可用，我们至少要确保锁的实现同时满足以下四个条件： 互斥性。在任意时刻，只有一个客户端能持有锁。不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。具有容错性。只要大部分的Redis节点正常运行，客户端就可以加锁和解锁。解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。 代码实现组件依赖首先我们要通过Maven引入Jedis开源组件，在pom.xml文件加入下面的代码：12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 加锁代码正确姿势Talk is cheap, show me the code。先展示代码，再带大家慢慢解释为什么这样实现：1234567891011121314151617181920212223242526public class RedisTool &#123; private static final String LOCK_SUCCESS = "OK"; private static final String SET_IF_NOT_EXIST = "NX"; private static final String SET_WITH_EXPIRE_TIME = "PX"; /** * 尝试获取分布式锁 * @param jedis Redis客户端 * @param lockKey 锁 * @param requestId 请求标识 * @param expireTime 超期时间 * @return 是否获取成功 */ public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) &#123; String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime); if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false; &#125;&#125; 可以看到，我们加锁就一行代码：jedis.set(String key, String value, String nxxx, String expx, int time)，这个set()方法一共有五个形参： 第一个为key，我们使用key来当锁，因为key是唯一的。第二个为value，我们传的是requestId，很多童鞋可能不明白，有key作为锁不就够了吗，为什么还要用到value？原因就是我们在上面讲到可靠性时，分布式锁要满足第四个条件解铃还须系铃人，通过给value赋值为requestId，我们就知道这把锁是哪个请求加的了，在解锁的时候就可以有依据。requestId可以使用UUID.randomUUID().toString()方法生成。第三个为nxxx，这个参数我们填的是NX，意思是SET IF NOT EXIST，即当key不存在时，我们进行set操作；若key已经存在，则不做任何操作；第四个为expx，这个参数我们传的是PX，意思是我们要给这个key加一个过期的设置，具体时间由第五个参数决定。第五个为time，与第四个参数相呼应，代表key的过期时间。 总的来说，执行上面的set()方法就只会导致两种结果： 当前没有锁（key不存在），那么就进行加锁操作，并对锁设置个有效期，同时value表示加锁的客户端。 已有锁存在，不做任何操作。心细的童鞋就会发现了，我们的加锁代码满足我们可靠性里描述的三个条件。首先，set()加入了NX参数，可以保证如果已有key存在，则函数不会调用成功，也就是只有一个客户端能持有锁，满足互斥性。其次，由于我们对锁设置了过期时间，即使锁的持有者后续发生崩溃而没有解锁，锁也会因为到了过期时间而自动解锁（即key被删除），不会发生死锁。最后，因为我们将value赋值为requestId，代表加锁的客户端请求标识，那么在客户端在解锁的时候就可以进行校验是否是同一个客户端。由于我们只考虑Redis单机部署的场景，所以容错性我们暂不考虑。 错误示例1比较常见的错误示例就是使用jedis.setnx()和jedis.expire()组合实现加锁，代码如下：123456789public static void wrongGetLock1(Jedis jedis, String lockKey, String requestId, int expireTime) &#123; Long result = jedis.setnx(lockKey, requestId); if (result == 1) &#123; // 若在这里程序突然崩溃，则无法设置过期时间，将发生死锁 jedis.expire(lockKey, expireTime); &#125;&#125; setnx()方法作用就是SET IF NOT EXIST，expire()方法就是给锁加一个过期时间。乍一看好像和前面的set()方法结果一样，然而由于这是两条Redis命令，不具有原子性，如果程序在执行完setnx()之后突然崩溃，导致锁没有设置过期时间。那么将会发生死锁。网上之所以有人这样实现，是因为低版本的jedis并不支持多参数的set()方法。 错误示例212345678910111213141516171819202122232425public static boolean wrongGetLock2(Jedis jedis, String lockKey, int expireTime) &#123; long expires = System.currentTimeMillis() + expireTime; String expiresStr = String.valueOf(expires); // 如果当前锁不存在，返回加锁成功 if (jedis.setnx(lockKey, expiresStr) == 1) &#123; return true; &#125; // 如果锁存在，获取锁的过期时间 String currentValueStr = jedis.get(lockKey); if (currentValueStr != null &amp;&amp; Long.parseLong(currentValueStr) &lt; System.currentTimeMillis()) &#123; // 锁已过期，获取上一个锁的过期时间，并设置现在锁的过期时间 String oldValueStr = jedis.getSet(lockKey, expiresStr); if (oldValueStr != null &amp;&amp; oldValueStr.equals(currentValueStr)) &#123; // 考虑多线程并发的情况，只有一个线程的设置值和当前值相同，它才有权利加锁 return true; &#125; &#125; // 其他情况，一律返回加锁失败 return false;&#125; 这一种错误示例就比较难以发现问题，而且实现也比较复杂。实现思路：使用jedis.setnx()命令实现加锁，其中key是锁，value是锁的过期时间。执行过程：1. 通过setnx()方法尝试加锁，如果当前锁不存在，返回加锁成功。2. 如果锁已经存在则获取锁的过期时间，和当前时间比较，如果锁已经过期，则设置新的过期时间，返回加锁成功。那么这段代码问题在哪里？ 由于是客户端自己生成过期时间，所以需要强制要求分布式下每个客户端的时间必须同步。 当锁过期的时候，如果多个客户端同时执行jedis.getSet()方法，那么虽然最终只有一个客户端可以加锁，但是这个客户端的锁的过期时间可能被其他客户端覆盖。 锁不具备拥有者标识，即任何客户端都可以解锁。 解锁代码正确姿势还是先展示代码，再带大家慢慢解释为什么这样实现：123456789101112131415161718192021222324public class RedisTool &#123; private static final Long RELEASE_SUCCESS = 1L; /** * 释放分布式锁 * @param jedis Redis客户端 * @param lockKey 锁 * @param requestId 请求标识 * @return 是否释放成功 */ public static boolean releaseDistributedLock(Jedis jedis, String lockKey, String requestId) &#123; String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end"; Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId)); if (RELEASE_SUCCESS.equals(result)) &#123; return true; &#125; return false; &#125;&#125; 可以看到，我们解锁只需要两行代码就搞定了！第一行代码，我们写了一个简单的Lua脚本代码，上一次见到这个编程语言还是在《黑客与画家》里，没想到这次居然用上了。第二行代码，我们将Lua代码传到jedis.eval()方法里，并使参数KEYS[1]赋值为lockKey，ARGV[1]赋值为requestId。eval()方法是将Lua代码交给Redis服务端执行。那么这段Lua代码的功能是什么呢？其实很简单，首先获取锁对应的value值，检查是否与requestId相等，如果相等则删除锁（解锁）。那么为什么要使用Lua语言来实现呢？因为要确保上述操作是原子性的。关于非原子性会带来什么问题，可以阅读【解锁代码-错误示例2】 。那么为什么执行eval()方法可以确保原子性，源于Redis的特性，下面是官网对eval命令的部分解释：简单来说，就是在eval命令执行Lua代码的时候，Lua代码将被当成一个命令去执行，并且直到eval命令执行完成，Redis才会执行其他命令。 错误示例1最常见的解锁代码就是直接使用jedis.del()方法删除锁，这种不先判断锁的拥有者而直接解锁的方式，会导致任何客户端都可以随时进行解锁，即使这把锁不是它的。123public static void wrongReleaseLock1(Jedis jedis, String lockKey) &#123; jedis.del(lockKey);&#125; 错误示例2这种解锁代码乍一看也是没问题，甚至我之前也差点这样实现，与正确姿势差不多，唯一区别的是分成两条命令去执行，代码如下：123456789public static void wrongReleaseLock2(Jedis jedis, String lockKey, String requestId) &#123; // 判断加锁与解锁是不是同一个客户端 if (requestId.equals(jedis.get(lockKey))) &#123; // 若在此时，这把锁突然不是这个客户端的，则会误解锁 jedis.del(lockKey); &#125;&#125; 如代码注释，问题在于如果调用jedis.del()方法的时候，这把锁已经不属于当前客户端的时候会解除他人加的锁。那么是否真的有这种场景？答案是肯定的，比如客户端A加锁，一段时间之后客户端A解锁，在执行jedis.del()之前，锁突然过期了，此时客户端B尝试加锁成功，然后客户端A再执行del()方法，则将客户端B的锁给解除了。 总结本文主要介绍了如何使用Java代码正确实现Redis分布式锁，对于加锁和解锁也分别给出了两个比较经典的错误示例。其实想要通过Redis实现分布式锁并不难，只要保证能满足可靠性里的四个条件。互联网虽然给我们带来了方便，只要有问题就可以google，然而网上的答案一定是对的吗？其实不然，所以我们更应该时刻保持着质疑精神，多想多验证。 如果你的项目中Redis是多机部署的，那么可以尝试使用Redis官方提供的Java组件Redisson实现分布式锁。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String被设计成不可变和不能被继承的原因]]></title>
    <url>%2F2019%2F05%2F07%2Fjdk-string%2F</url>
    <content type="text"><![CDATA[String是所有语言中最常用的一个类。我们知道在Java中，String是不可变的、final的。Java在运行时也保存了一个字符串池(String pool)，这使得String成为了一个特别的类。主要是为了“效率”和“安全性”的缘故。若String允许被继承，由于它的高度被使用率，可能会降低程序的性能，所以 String 被定义成 final。 一：String 和其他基本类型不同 , 他是个对象类型. 既然是对象类型 , 如果是在静态方法下是必须调用静态方法或值的 , 如果是非静态的方法 , 就必须要实例化.main函数是个static的. 所以String要能像其他的基本类型一样直接被调用. 这也是为什么在main函数下使用String类型不会报告错误的原因.. 二：当定义String类型的静态字段（也成类字段），可以用静态变量（非final）代替常量（final）加快程序速度。反之，对于原始数据类型，例如 int，也成立。例如，你可能创建一个如下的 String 对象：1Private static final String x="example"; 对于这个静态常量（由 final 关键字标识），你使用常量的每个时候都会创建一个临时的 String 对象。 在字节代码中，编译器去掉 ”x”，代替它的是字符串 “example”, 以致每次引用 ”x” 时 VM 都会进行一次哈希表查询。相比之下，对于静态变量 (非final关键字)，字符串只创建一次。仅当初始化 “x” 时，VM才进行哈希表查询。 还有另一个解释 :带有final修饰符的类是不可派生的。在java核心API中，有许多应用final的例子，例如java.lang.String。为String类指定final防止了人们覆盖length()方法。另外，如果指定一个类为final，则该类所有的方法都是final。java编译器会寻找机会内联（inline）所有的final方法（这和具体的编译器实现有关）。此举能够使性能平均提高50%。 另外补充一点：作用就是 final的类不能被继承，不能让别人继承有什么好处?意义就在于安全性，如此这般：java 自出生那天起就是“为人民服务”，这也就是为什么java做不了病毒，也不一定非得是病毒，反正总之就是为了安全，人家java的开发者目的就是不想让 java干这类危险的事儿，java并不是操作系统本地语言，换句话说java必须借助操作系统本身的力量才能做事，JDK中提供的好多核心类比如 String，这类的类的内部好多方法的实现都不是java编程语言本身编写的，好多方法都是调用的操作系统本地的API，这就是著名的“本地方法调用”，也只有这样才能做事，这种类是非常底层的，和操作系统交流频繁的，那么如果这种类可以被继承的话，如果我们再把它的方法重写了，往操作系统内部写入一段具有恶意攻击性质的代码什么的，这不就成了核心病毒了么？ String类不可变性的好处：1.只有当字符串是不可变的，字符串池才有可能实现。字符串池的实现可以在运行时节约很多heap空间，因为不同的字符串变量都指向池中的同一个字符串。但如果字符串是可变的，那么String interning将不能实现(译者注：String interning是指对不同的字符串仅仅只保存一个，即不会保存多个相同的字符串。)，因为这样的话，如果变量改变了它的值，那么其它指向这个值的变量的值也会一起改变。如果字符串是可变的，那么会引起很严重的安全问题。譬如，数据库的用户名、密码都是以字符串的形式传入来获得数据库的连接，或者在socket编程中，主机名和端口都是以字符串的形式传入。因为字符串是不可变的，所以它的值是不可改变的，否则黑客们可以钻到空子，改变字符串指向的对象的值，造成安全漏洞。因为字符串是不可变的，所以是多线程安全的，同一个字符串实例可以被多个线程共享。这样便不用因为线程安全问题而使用同步。字符串自己便是线程安全的。 2.类加载器要用到字符串，不可变性提供了安全性，以便正确的类被加载。譬如你想加载java.sql.Connection类，而这个值被改成了myhacked.Connection，那么会对你的数据库造成不可知的破坏。因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>string</tag>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[淘宝大秒系统设计详解]]></title>
    <url>%2F2019%2F05%2F06%2Ftb-design%2F</url>
    <content type="text"><![CDATA[摘要：最初的秒杀系统的原型是淘宝详情上的定时上架功能，由于有些卖家为了吸引眼球，把价格压得很低。但这给的详情系统带来了很大压力，为了将这种突发流量隔离，才设计了秒杀系统，文章主要介绍大秒系统以及这种典型读数据的热点问题的解决思路和实践经验。下面介绍一下许令波关于对淘宝秒杀系统设计的详解。许令波，滴滴研究员，2009年加入淘宝，目前负责商品详情业务和稳定性相关工作，长期关注性能优化领域，参与了淘宝高访问量Web系统主要的优化项目，著有《深入分析Java Web技术内幕》一书。个人网站 http://xulingbo.net 一些数据大家还记得2013年的小米秒杀吗？三款小米手机各11万台开卖，走的都是大秒系统，3分钟后成为双十一第一家也是最快破亿的旗舰店。经过日志统计，前端系统双11峰值有效请求约60w以上的QPS ，而后端cache的集群峰值近2000w/s、单机也近30w/s，但到真正的写时流量要小很多了，当时最高下单减库存tps是红米创造，达到1500/s。 热点隔离秒杀系统设计的第一个原则就是将这种热点数据隔离出来，不要让1%的请求影响到另外的99%，隔离出来后也更方便对这1%的请求做针对性优化。针对秒杀我们做了多个层次的隔离：业务隔离：把秒杀做成一种营销活动，卖家要参加秒杀这种营销活动需要单独报名，从技术上来说，卖家报名后对我们来说就是已知热点，当真正开始时我们可以提前做好预热。系统隔离：系统隔离更多是运行时的隔离，可以通过分组部署的方式和另外99%分开。秒杀还申请了单独的域名，目的也是让请求落到不同的集群中。数据隔离：秒杀所调用的数据大部分都是热数据，比如会启用单独cache集群或MySQL数据库来放热点数据，目前也是不想0.01%的数据影响另外99.99%。 当然实现隔离很有多办法，如可以按照用户来区分，给不同用户分配不同cookie，在接入层路由到不同服务接口中；还有在接入层可以对URL的不同Path来设置限流策略等。服务层通过调用不同的服务接口；数据层可以给数据打上特殊的标来区分。目的都是把已经识别出来的热点和普通请求区分开来。 动静分离前面介绍在系统层面上的原则是要做隔离，接下去就是要把热点数据进行动静分离，这也是解决大流量系统的一个重要原则。如何给系统做动静分离的静态化改造我以前写过一篇《高访问量系统的静态化架构设计》详细介绍了淘宝商品系统的静态化设计思路，感兴趣的可以在《程序员》杂志上找一下。我们的大秒系统是从商品详情系统发展而来，所以本身已经实现了动静分离，如图1。图1 大秒系统动静分离 除此之外还有如下特点：把整个页面Cache在用户浏览器如果强制刷新整个页面，也会请求到CDN实际有效请求只是“刷新抢宝”按钮这样把90%的静态数据缓存在用户端或者CDN上，当真正秒杀时用户只需要点击特殊的按钮“刷新抢宝”即可，而不需要刷新整个页面，这样只向服务端请求很少的有效数据，而不需要重复请求大量静态数据。秒杀的动态数据和普通的详情页面的动态数据相比更少，性能也比普通的详情提升3倍以上。所以“刷新抢宝”这种设计思路很好地解决了不刷新页面就能请求到服务端最新的动态数据。 基于时间分片削峰熟悉淘宝秒杀的都知道，第一版的秒杀系统本身并没有答题功能，后面才增加了秒杀答题，当然秒杀答题一个很重要的目的是为了防止秒杀器，2011年秒杀非常火的时候，秒杀器也比较猖獗，而没有达到全民参与和营销的目的，所以增加的答题来限制秒杀器。增加答题后，下单的时间基本控制在2s后，秒杀器的下单比例也下降到5%以下。新的答题页面如图2。图2 秒答题页面 其实增加答题还有一个重要的功能，就是把峰值的下单请求给拉长了，从以前的1s之内延长到2~10s左右，请求峰值基于时间分片了，这个时间的分片对服务端处理并发非常重要，会减轻很大压力，另外由于请求的先后，靠后的请求自然也没有库存了，也根本到不了最后的下单步骤，所以真正的并发写就非常有限了。其实这种设计思路目前也非常普遍，如支付宝的“咻一咻”已及微信的摇一摇。 除了在前端通过答题在用户端进行流量削峰外，在服务端一般通过锁或者队列来控制瞬间请求。 数据分层校验图3 分层校验 对大流量系统的数据做分层校验也是最重要的设计原则，所谓分层校验就是对大量的请求做成“漏斗”式设计，如图3所示：在不同层次尽可能把无效的请求过滤，“漏斗”的最末端才是有效的请求，要达到这个效果必须对数据做分层的校验，下面是一些原则：1.先做数据的动静分离2.将90%的数据缓存在客户端浏览器3.将动态请求的读数据Cache在Web端4.对读数据不做强一致性校验5.对写数据进行基于时间的合理分片6.对写请求做限流保护7.对写数据进行强一致性校验秒杀系统正是按照这个原则设计的系统架构，如图4所示。图4 秒杀系统分层架构 把大量静态不需要检验的数据放在离用户最近的地方；在前端读系统中检验一些基本信息，如用户是否具有秒杀资格、商品状态是否正常、用户答题是否正确、秒杀是否已经结束等；在写数据系统中再校验一些如是否是非法请求，营销等价物是否充足（淘金币等），写的数据一致性如检查库存是否还有等；最后在数据库层保证数据最终准确性，如库存不能减为负数。 实时热点发现其实秒杀系统本质是还是一个数据读的热点问题，而且是最简单一种，因为在文提到通过业务隔离，我们已能提前识别出这些热点数据，我们可以提前做一些保护，提前识别的热点数据处理起来还相对简单，比如分析历史成交记录发现哪些商品比较热门，分析用户的购物车记录也可以发现那些商品可能会比较好卖，这些都是可以提前分析出来的热点。比较困难的是那种我们提前发现不了突然成为热点的商品成为热点，这种就要通过实时热点数据分析了，目前我们设计可以在3s内发现交易链路上的实时热点数据，然后根据实时发现的热点数据每个系统做实时保护。具体实现如下：1.构建一个异步的可以收集交易链路上各个中间件产品如Tengine、Tair缓存、HSF等本身的统计的热点key（Tengine和Tair缓存等中间件产品本身已经有热点统计模块）。2.建立一个热点上报和可以按照需求订阅的热点服务的下发规范，主要目的是通过交易链路上各个系统（详情、购物车、交易、优惠、库存、物流）访问的时间差，把上游已经发现的热点能够透传给下游系统，提前做好保护。比如大促高峰期详情系统是最早知道的，在统计接入层上Tengine模块统计的热点URL。3.将上游的系统收集到热点数据发送到热点服务台上，然后下游系统如交易系统就会知道哪些商品被频繁调用，然后做热点保护。如图5所示。 图5 实时热点数据后台 重要的几个：其中关键部分包括：1.这个热点服务后台抓取热点数据日志最好是异步的，一方面便于做到通用性，另一方面不影响业务系统和中间件产品的主流程。2.热点服务后台、现有各个中间件和应用在做的没有取代关系，每个中间件和应用还需要保护自己，热点服务后台提供一个收集热点数据提供热点订阅服务的统一规范和工具，便于把各个系统热点数据透明出来。3.热点发现要做到实时（3s内）。 关键技术优化点前面介绍了一些如何设计大流量读系统中用到的原则，但是当这些手段都用了，还是有大流量涌入该如何处理呢？秒杀系统要解决几个关键问题。 Java处理大并发动态请求优化其实Java和通用的Web服务器相比（Nginx或Apache）在处理大并发HTTP请求时要弱一点，所以一般我们都会对大流量的Web系统做静态化改造，让大部分请求和数据直接在Nginx服务器或者Web代理服务器（Varnish、Squid等）上直接返回（可以减少数据的序列化与反序列化），不要将请求落到Java层上，让Java层只处理很少数据量的动态请求，当然针对这些请求也有一些优化手段可以使用：直接使用Servlet处理请求。避免使用传统的MVC框架也许能绕过一大堆复杂且用处不大的处理逻辑，节省个1ms时间，当然这个取决于你对MVC框架的依赖程度。直接输出流数据。使用resp.getOutputStream()而不是resp.getWriter()可以省掉一些不变字符数据编码，也能提升性能；还有数据输出时也推荐使用JSON而不是模板引擎（一般都是解释执行）输出页面。 同一商品大并发读问题你会说这个问题很容易解决，无非放到Tair缓存里面就行，集中式Tair缓存为了保证命中率，一般都会采用一致性Hash，所以同一个key会落到一台机器上，虽然我们的Tair缓存机器单台也能支撑30w/s的请求，但是像大秒这种级别的热点商品还远不够，那如何彻底解决这种单点瓶颈？答案是采用应用层的Localcache，即在秒杀系统的单机上缓存商品相关的数据，如何cache数据？也分动态和静态：1.像商品中的标题和描述这些本身不变的会在秒杀开始之前全量推送到秒杀机器上并一直缓存直到秒杀结束。2.像库存这种动态数据会采用被动失效的方式缓存一定时间（一般是数秒），失效后再去Tair缓存拉取最新的数据。 你可能会有疑问，像库存这种频繁更新数据一旦数据不一致会不会导致超卖？其实这就要用到我们前面介绍的读数据分层校验原则了，读的场景可以允许一定的脏数据，因为这里的误判只会导致少量一些原本已经没有库存的下单请求误认为还有库存而已，等到真正写数据时再保证最终的一致性。这样在数据的高可用性和一致性做平衡来解决这种高并发的数据读取问题。 同一数据大并发更新问题解决大并发读问题采用Localcache和数据的分层校验的方式，但是无论如何像减库存这种大并发写还是避免不了，这也是秒杀这个场景下最核心的技术难题。 同一数据在数据库里肯定是一行存储（MySQL），所以会有大量的线程来竞争InnoDB行锁，当并发度越高时等待的线程也会越多，TPS会下降RT会上升，数据库的吞吐量会严重受到影响。说到这里会出现一个问题，就是单个热点商品会影响整个数据库的性能，就会出现我们不愿意看到的0.01%商品影响99.99%的商品，所以一个思路也是要遵循前面介绍第一个原则进行隔离，把热点商品放到单独的热点库中。但是无疑也会带来维护的麻烦（要做热点数据的动态迁移以及单独的数据库等）。 分离热点商品到单独的数据库还是没有解决并发锁的问题，要解决并发锁有两层办法。应用层做排队。按照商品维度设置队列顺序执行，这样能减少同一台机器对数据库同一行记录操作的并发度，同时也能控制单个商品占用数据库连接的数量，防止热点商品占用太多数据库连接。数据库层做排队。应用层只能做到单机排队，但应用机器数本身很多，这种排队方式控制并发仍然有限，所以如果能在数据库层做全局排队是最理想的，淘宝的数据库团队开发了针对这种MySQL的InnoDB层上的patch，可以做到数据库层上对单行记录做到并发排队，如图6所示。 图6 数据库层对单行记录并发排队 你可能会问排队和锁竞争不要等待吗？有啥区别？如果熟悉MySQL会知道，InnoDB内部的死锁检测以及MySQL Server和InnoDB的切换会比较耗性能，淘宝的MySQL核心团队还做了很多其他方面的优化，如COMMIT_ON_SUCCESS和ROLLBACK_ON_FAIL的patch，配合在SQL里面加hint，在事务里不需要等待应用层提交COMMIT而在数据执行完最后一条SQL后直接根据TARGET_AFFECT_ROW结果提交或回滚，可以减少网络的等待时间（平均约0.7ms）。据我所知，目前阿里MySQL团队已将这些patch及提交给MySQL官方评审。 大促热点问题思考以秒杀这个典型系统为代表的热点问题根据多年经验我总结了些通用原则：隔离、动态分离、分层校验，必须从整个全链路来考虑和优化每个环节，除了优化系统提升性能，做好限流和保护也是必备的功课。 除去前面介绍的这些热点问题外，淘系还有多种其他数据热点问题：数据访问热点，比如Detail中对某些热点商品的访问度非常高，即使是Tair缓存这种Cache本身也有瓶颈问题，一旦请求量达到单机极限也会存在热点保护问题。有时看起来好像很容易解决，比如说做好限流就行，但你想想一旦某个热点触发了一台机器的限流阀值，那么这台机器Cache的数据都将无效，进而间接导致Cache被击穿，请求落地应用层数据库出现雪崩现象。这类问题需要与具体Cache产品结合才能有比较好的解决方案，这里提供一个通用的解决思路，就是在Cache的client端做本地Localcache，当发现热点数据时直接Cache在client里，而不要请求到Cache的Server。数据更新热点，更新问题除了前面介绍的热点隔离和排队处理之外，还有些场景，如对商品的lastmodifytime字段更新会非常频繁，在某些场景下这些多条SQL是可以合并的，一定时间内只执行最后一条SQL就行了，可以减少对数据库的update操作。另外热点商品的自动迁移，理论上也可以在数据路由层来完成，利用前面介绍的热点实时发现自动将热点从普通库里迁移出来放到单独的热点库中。 按照某种维度建的索引产生热点数据，比如实时搜索中按照商品维度关联评价数据，有些热点商品的评价非常多，导致搜索系统按照商品ID建评价数据的索引时内存已经放不下，交易维度关联订单信息也同样有这些问题。这类热点数据需要做数据散列，再增加一个维度，把数据重新组织。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>秒杀架构设计</category>
      </categories>
      <tags>
        <tag>高可用</tag>
        <tag>秒杀</tag>
        <tag>架构实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大神讲解微服务治理的技术演进和架构实践]]></title>
    <url>%2F2019%2F05%2F06%2Fmicroservice-governance%2F</url>
    <content type="text"><![CDATA[摘要：微服务平台上线后，由于采用微服务架构设计的，多个模块提供一个完整的业务流程，随着业务的发展,规模扩大，服务越来越多，需要协调线上运行的各个服务，保障服务的SLA;基于服务调用的性能KPI数据进行容量管理，合理分配各服务的资源占用;对故障业务做服务降级、流量控制、流量迁移等快速恢复业务。怎样的服务治理框架能满足需求？下面引用李林峰曾经分享的《微服务治理的技术演进和架构实践》，提供对微服务治理的详细阐述。 大佬介绍：李林锋，华为PaaS平台架构师，8年Java NIO通信框架、平台中间件架构设计和开发经验，开源框架Netty中国推广者。精通Netty、Mina、RPC框架、企业ESB总线、分布式服务框架、云计算等技术，《Netty权威指南》、《分布式服务框架原理与实践》作者，公司总裁技术创新奖获得者 微服务治理的技术演进和架构实践》分为三个部分。 为什么需要服务治理服务治理的技术演进历程云端微服务治理框架设计 为什么需要服务治理？第一、业务需求随着业务的发展，服务越来越多，如何协调线上运行的各个服务，保障服务的SLA，对服务架构和运维人员是一个很大的挑战。随着业务规模的不断扩大，小服务资源浪费等问题逐渐显现，需要能够基于服务调用的性能KPI数据进行容量管理，合理分配各个服务的资源占用，提高机器的利用率。线上业务发生故障时，需要对故障业务做服务降级、流量控制、流量迁移等，快速恢复业务。 着开发团队的不断扩大，服务的上线越来越随意，甚至发生功能相同、服务名不同的服务同时上线。上线容易下线难，为了规范服务的上线和下线，在服务发布前，需要走服务预发布流程，由架构师或者项目经理对需要上线的服务做发布审核，审核通过的才能够上线。为了满足服务线下管控、保障线上高效运行，需要有一个统一的服务治理框架对服务进行统一、有效管控，保障服务的高效、健康运行。 第二、技术需求大部分服务化框架的服务属性通过XML配置或者Java注解的方式进行定义，以服务端流量控制为例： 服务发布的XML文件通常会打包到服务提供者的jar包中，部署在Java Web或者Java Container容器中，存储在服务端的本地磁盘中。 无论采用注解还是XML配置的方式，如果需要在运行态动态修改服务提供者的流控阈值，都需要在本地修改配置或者修改源码，重新打包部署并升级应用。无法实现在线、配置化的修改和动态生效。由于诸如流控阈值、服务的超时时间等无法预测出最优值，需要修改之后上线验证，根据服务运行效果决定是否再做调整，因此经常需要反复调整，采用修改源码-重新打包部署-应用升级的方式进行服务治理，效率低下。因此，在技术上需要一个服务治理框架，提供Web Portal，微服务运维或者治理人员通过在线配置化的方式修改服务提供者或者消费者的属性，可以实时动态生效。 服务治理的技术演进历程第一代服务治理 SOA Governance: 以IBM为首的SOA解决方案提供商推出的针对企业IT系统的服务治理框架，它主要聚焦在对企业IT系统中异构服务的质量管理、服务发布审批流程管理和服务建模、开发、测试以及运行的全生命周期管理。第二代以分布式服务框架为中心的服务治理：随着电商和移动互联网的快速发展，基于电商平台的统一分布式服务框架的全新服务治理理念诞生，它聚焦于对内部同构服务的线上治理，保障线上服务的运行质量。相比于传统IT架构的服务治理，由于服务的开发模式、部署规模、组网类型、业务特点等差异巨大，因此服务治理的重点也从线下转移到了线上服务质量保障。第三代以云化为核心的云端微服务治理架构：2013年至今，随着云计算和微服务架构的发展，以AWS为首的基于微服务架构 云服务化的云端服务治理体系诞生，它的核心理念是服务微自治，利用云调度的弹性和敏捷，逐渐消除人工治理。微服务架构可以实现服务一定程度的自治，例如服务独立打包、独立部署、独立升级和独立扩容。通过云计算的弹性伸缩、单点故障迁移、服务健康度管理和自动容量规划等措施，结合微服务治理，逐步实现微服务的自治。 第一代 SOA Governance服务治理第一代SOA Service GovernanceSOA Governance的定位：面向企业IT系统异构服务的治理和服务生命周期管理，它治理的服务通常是SOA服务。传统的SOA Governance包含四部分内容：1.服务建模：验证功能需求与业务需求，发现和评估当前服务，服务建模和性能需求，开发治理规划。2.服务组装：创建服务更新计划，创建和修改服务以满足所有业务需求，根据治理策略评估服务，批准组装完成。3.服务部署：确保服务的质量，措施包括功能测试、性能测试和满足度测试，批准服务部署。4.服务管理：在整个生命周期内管理和监控服务，跟踪服务注册表中的服务，根据服务质量等级协议（SLA）上报服务的性能KPI数据进行服务质量管理。 SOA Governance 工作原理图如下所示： 传统SOA Governance的主要缺点如下：1.分布式服务框架的发展，内部服务框架需要统一，服务治理也需要适应新的架构，能够由表及里，对服务进行细粒度的管控。2.微服务架构的发展和业务规模的扩大，导致服务规模量变引起质变，服务治理的特点和难点也随之发生变化。3.缺少服务运行时动态治理能力，面对突发的流量高峰和业务冲击，传统的服务治理在响应速度、故障快速恢复等方面存在不足，无法更敏捷应对业务需求。 第二代分布式服务框架服务治理分布式服务框架的服务治理定位：面向互联网业务的服务治理，聚焦在对内部采用统一服务框架服务化的业务运行态、细粒度的敏捷治理体系。治理的对象：基于统一分布式服务框架开发的业务服务，与协议本身无关，治理的可以是SOA服务，也可以是基于内部服务框架私有协议开发的各种服务。治理策略：针对互联网业务的特点，例如突发的流量高峰、网络延时、机房故障等，重点针对大规模跨机房的海量服务进行运行态治理，保障线上服务的高SLA，满足用户的体验。常用的治理策略包括服务的限流降级、服务迁入迁出、服务动态路由和灰度发布等。 分布式服务框架典型的服务治理体系如下所示： 第三代云端微服务治理随着云计算的发展，Dev&amp;Ops逐渐流行起来，基础设施服务化（IaaS）为大规模、批量流水线式软件交付提供了便利，AWS做为全球最大的云计算解决方案提供商，在微服务云化开发和治理方面积累了非常多的经验，具体总结如下1.全公司统一服务化开发环境，统一简单化服务框架（Coral Service）,统一运行平台，快速高效服务开发；2.所有后端应用服务化，系统由多项服务化组件构成。3.服务共享、原子化、重用。4.服务由小研发团队（2 Pizza Team）负责服务开发、测试、部署和治理，运维整个生命周期支撑。5.高度自动化和Dev&amp;Ops支持，一键式服务部署和回退。6.超大规模支持：后台几十万个服务，成千上万开发者同时使用，平均每秒钟有1-2个服务部署。7.尝试基于Docker容器部署微服务。8.服务治理是核心：服务性能KPI统计、告警、服务健康度管理、灵活的弹性伸缩策略、故障自动迁移、服务限流和服务降级等多种治理手段，保障服务高质量运行。 云端微服务治理架构设计云端微服务治理架构设计的目标如下：防止业务服务架构腐化：通过服务注册中心对服务强弱依赖进行分析，结合运行时服务调用链关系分析，梳理不合理的依赖和调用路径，优化服务化架构，防止代码腐化。快速故障定界定位：通过Flume等分布式日志采集框架，实时收集服务调用链日志、服务性能KPI数据、服务接口日志、运行日志等，实时汇总和在线分析，集中存储和展示，实现故障的自动发现、自动分析和在线条件检索，方便运维人员、研发人员进行实时故障诊断。服务微管控：细粒度的运行期服务治理，包括限流降级、服务迁入迁出、服务超时控制、智能路由、统一配置、优先级调度和流量迁移等，提供方法级治理和动态生效功能，通过一系列细粒度的治理策略，在故障发生时可以多管齐下，在线调整，快速恢复业务。服务生命周期管理：包括服务的上线审批、下线通知，服务的在线升级，以及线上和线下服务文档库的建设。灵活的资源调度：基于Docker容器，可以实现微服务的独立部署和细粒度的资源隔离。基于云端的弹性伸缩，可以实现微服务级别的按需伸缩和故障隔离。 云端微服务治理架构设计云端微服务治理从架构上可以分为三层： 第一层：微服务治理展示层，它的实现为微服务治理Portal，主要面向系统运维人员或者治理人员，提供在线、配置化的治理界面。第二层：微服务治理SDK，向服务治理提供治理元数据、治理接口、以及客户端的治理类库。第三层：微服务治理服务实现层，微服务治理服务，通过服务注册中心，刷新服务治理属性，同时通知服务提供者和消费者集群各节点刷新内存，使服务治理Portal下发的服务治理策略动态生效。 1.微服务治理Portal微服务治理Portal是微服务治理的门户，它提供服务治理操作界面，供系统运维人员或者测试人员对线上运行的微服务进行动态治理，以保障服务的SLA。Portal框架可以基于AngularJS等Web框架进行开发，它的门户界面如下所示：可以支持同时配置多个服务注册中心集群，对不同的微服务集群进行治理。 选择某个微服务集群之后，就可以对该集群的微服务进行治理，界面示例如下： 点击查看，可以查看微服务的状态，以及各种性能指标。点击治理，弹出选择菜单，可以对选择的微服务进行相关的治理操作。 2.微服务治理SDK服务治理SDK层，它主要由如下几部分组成：服务治理元数据：服务治理元数据主要包括服务治理实体对象，包括服务模型、应用模型、治理组织模型、用户权限模型、数据展示模型等。元数据模型通过Data Mapper和模型扩展，向上层界面屏蔽底层服务框架的数据模型，实现展示层和服务框架的解耦，元数据也可以用于展示界面的定制扩展；服务治理接口：服务治理Portal调用服务治理接口，实现服务治理。例如服务降级接口、服务流控接口、服务路由权重调整接口、服务迁移接口等。服务接口与具体的协议无关，它通常基于分布式服务框架自身实现，可以是Restful接口，也可以是内部的私有协议；服务治理客户端类库：由于服务治理服务本身通常也是基于分布式服务框架开发，因此服务治理Portal需要集成分布式服务框架的客户端类库，实现服务的自动发现和调用；调用示例：客户端SDK需要提供服务治理接口的参数说明、注意事项以及给出常用的调用示例，方便前端开发人员使用；集成开发指南：服务治理SDK需要提供集成开发指南，指导使用者如何在开发环境中搭建、集成和使用服务治理SDK。 3.线上服务治理线上服务治理包含多种策略，例如：流量控制、服务降级、优先级调度等。微服务启动的时候，将XML或者注解的服务提供者或者消费者属性注册到服务注册中心，由运维人员通过服务治理Portal进行在线修改，注册中心通知服务提供者和消费者刷新内存，动态生效。下面就这几种典型的治理策略进行说明。 第一、流量控制当资源成为瓶颈时，服务框架需要对消费者做限流，启动流控保护机制。流量控制有多种策略，比较常用的有：针对访问速率的静态流控、针对资源占用的动态流控、针对消费者并发连接数的连接控制和针对并行访问数的并发控制。静态流控：主要针对客户端访问速率进行控制，它通常根据服务质量等级协定（SLA）中约定的QPS做全局流量控制，例如订单服务的静态流控阈值为100 QPS，则无论集群有多少个订单服务实例，它们总的处理速率之和不能超过100 QPS。动态流控：它的最终目标是为了保命，并不是对流量或者访问速度做精确控制。当系统负载压力非常大时，系统进入过负载状态，可能是CPU、内存资源已经过载，也可能是应用进程内部的资源几乎耗尽，如果继续全量处理业务，可能会导致长时间的Full GC、消息严重积压或者应用进程宕机，最终将压力转移到集群其它节点，引起级联故障。触发动态流控的因子是资源，资源又分为系统资源和应用资源两大类，根据不同的资源负载情况，动态流控又分为多个级别，每个级别流控系数都不同，也就是被拒绝掉的消息比例不同。每个级别都有相应的流控阈值，这个阈值通常支持在线动态调整。并发控制：针对线程的并发执行数进行控制，它的本质是限制对某个服务或者服务的方法过度消费，耗用过多的资源而影响其它服务的正常运行。并发控制有两种形式：针对服务提供者的全局控制和针对服务消费者的局部控制。连接控制：通常分布式服务框架服务提供者和消费者之间采用长连接私有协议，为了防止因为消费者连接数过多导致服务端负载压力过大，系统需要支持针对连接数进行流控。 4.服务降级大促或者业务高峰时，为了保证核心服务的SLA，往往需要停掉一些不太重要的业务，例如商品评论、论坛或者粉丝积分等。另外一种场景就是某些服务因为某种原因不可用，但是流程不能直接失败，需要本地Mock服务端实现，做流程放通。以图书阅读为例，如果用户登录余额鉴权服务不能正常工作，需要做业务放通，记录消费话单，允许用户继续阅读，而不是返回失败。通过服务治理的服务降级功能，即可以满足上述两种场景的需求。服务降级主要包括屏蔽降级和容错降级两种策略： 屏蔽降级：当外界的触发条件达到某个临界值时，由运维人员/开发人员决策，对某类或者某个服务进行强制降级。它的处理流程如下所示： 第1步：运维人员以管理员身份登录服务治理控制台，管理员具备服务治理的全套权限。第2步：运维人员选择服务降级菜单，在服务降级界面中选择屏蔽降级。第3步：通过服务查询界面选择需要降级的服务，注意服务的分组和版本信息，指定具体的降级策略：返回null、返回指定异常还是执行本地Mock接口实现。第4步：服务治理Portal通过服务注册中心客户端SDK，将屏蔽降级指令和相关信息发送到服务注册中心。第5、6步：服务注册中心接收到屏蔽降级消息后，以事件的形式下分别群发给服务提供者集群和服务消费者集群。第7步：服务消费者接收到屏蔽降级事件通知之后，获取相关内容，更新本地缓存的服务订阅信息。当发起远程服务调用时，需要与屏蔽降级策略做匹配，如果匹配成功，则执行屏蔽降级逻辑，不发起远程服务调用。第8步：服务提供者集群接收到屏蔽降级事件通知之后，获取相关内容，更新本地的服务发布缓存信息，将对应的服务降级属性修改为屏蔽降级。第9步：操作成功之后，服务注册中心返回降级成功的应答消息，由服务治理Portal界面展示。第10步：运维人员查询服务提供者列表，查看服务状态。第11步：服务注册中心返回服务状态为屏蔽降级状态。 容错降级：当非核心服务不可用时，可以对故障服务做业务逻辑放通，以保障核心服务的运行。容错降级的工作原理如下所示： 5.服务优先级调度当系统当前资源非常有限时，为了保证高优先级的服务能够正常运行，保障服务SLA，需要降低一些非核心服务的调度频次，释放部分资源占用，保障系统的整体运行平稳。服务在发布的时候，可以指定服务的优先级，如果用户没有指定，采用默认优先级策略，它的配置如下所示： 服务的优先级可以采用传统的低、中、高三级配置策略，每个级别的执行比例可以灵活配置，如下所示： 服务发布通过扩展priority属性的方式指定优先级，服务提供者将优先级属性注册到服务注册中心并通知消费者，由消费者缓存服务的优先级，根据不同的优先级策略进行调度。服务治理Portal通过动态修改注册中心指定服务priority属性的方式，实现运行态动态调整微服务的优先级。 总结除了上面介绍的几种常用线上治理策略，比较重要的微服务治理策略还包括：微服务超时控制：由于微服务调用通常使用RPC方式，是同步阻塞的，因此需要设置服务调用超时时间，防止对端长时间不响应导致的应用线程挂死。超时控制支持在服务端或者消费端配置，需要支持方法级超时控制。微服务路由策略：负载均衡策略是服务治理的重要特性，分布式服务框架通常会提供多种负载均衡策略，同时支持用户扩展负载均衡策略。常用的路由策略包括：随机：采用随机算法进行负载均衡，通常在对等集群组网中，随机路由算法消息分发还是比较均匀的。轮循:按公约后的权重设置轮循比率，到达边界之后，继续绕接。服务调用时延：消费者缓存所有服务提供者的服务调用时延，周期性的计算服务调用平均时延，然后计算每个服务提供者服务调用时延与平均时延的差值，根据差值大小动态调整权重，保证服务时延大的服务提供者接收更少的消息，防止消息堆积。一致性Hash:相同参数的请求总是发到同一个服务提供者，当某一台提供者宕机时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。粘滞连接:粘滞连接用于有状态服务，尽可能让客户端总是向同一提供者发起服务调用，除非该提供者宕机，再连接另一台。集群容错策略：消费者根据配置的路由策略选择某个目标地址之后，发起远程服务调用，在此期间如果发生了远程服务调用异常，则需要服务框架进行集群容错，重新进行选路和调用。集群容错是系统自动执行的，上层用户并不需要关心底层的服务调用过程。集群容错和路由策略的关系如下所示： 常用的集群容错策略如下:Failover策略:服务调用失败自动切换策略指的是当发生RPC调用异常时，重新选路，查找下一个可用的服务提供者。通常可以配置失败切换的最大次数和间隔周期，以防止E2E服务调用时延过大。Failback策略：在很多业务场景中，消费者需要能够获取到服务调用失败的具体信息，通过对失败错误码等异常信息的判断，决定后续的执行策略，例如非幂等性的服务调用。Failcache策略:Failcache策略是失败自动恢复的一种，在实际项目中它的应用场景如下：- 服务有状态路由，必须定点发送到指定的服务提供者。当发生链路中断、流控等服务暂时不可用时，服务框架将消息临时缓存起来，等待周期T，重新发送，直到服务提供者能够正常处理该消息。- 对时延要求不敏感的服务。系统服务调用失败，通常是链路暂时不可用、服务流控、GC挂住服务提供者进程等，这种失败不是永久性的失败，它的恢复是可预期的。如果消费者对服务调用时延不敏感，可以考虑采用自动恢复模式，即先缓存，再等待，最后重试。-通知类服务。例如通知粉丝积分增长、记录接口日志等，对服务调用的实时性要求不高，可以容忍自动恢复带来的时延增加。Failfast策略：在业务高峰期，对于一些非核心的服务，希望只调用一次，失败也不再重试，为重要的核心服务节约宝贵的运行资源。此时，快速失败是个不错的选择。 服务灰度发布：灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。AB test就是一种灰度发布方式，让一部用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。基于微服务的多版本管理机制灰度路由策略，即可实现基于业务规则的灰度发布。多版本管理：服务上线之后，随着业务的发展，需要对功能进行变更，或者修复线上的BUG，服务升级之后，往往需要对服务做多版本管理。服务多版本管理是分布式服务框架的重要特性，它涉及到服务的开发、部署、在线升级和服务治理。调用分组管理：可以对服务按照业务领域、部署的DC信息、服务提供商等角度对微服务进行群组化管理，同群组之间的微服务可以自由调用，跨群组的微服务需要进行审批和授权，以实现不同分组之间的微服务隔离。不同分组之间可以有同名同接口的微服务的不同实现，分组信息也是服务路由的一个因子。全在线服务文档:相对于平台产品，业务服务的升级和修改非常频繁，传统依靠Java DOC进行接口说明和传递的方式，往往会因为缺乏文档建设或API DOC没有及时刷新，导致消费者拿到的接口定义说明不准确。相比于没有文档，拿到过时、错误的API DOC文档对使用者的危害更大。为了解决这个问题，需要建立服务文档中心，方便线上运维人员查看和多团队之间的协作，它的工作原理如下： 可以基于Swagger UI，构建微服务在线文档库，如下所示： 可以参考如下链接：https://github.com/swagger-api/swagger-ui 服务上线审批下线通知机制当团队规模扩大之后，会划分成平台基线组、业务定制组等不同研发团队，一些团队甚至跨多地协同开发和运维。服务的上线和下线必须要严格管控起来，一旦不合格的服务上线并被消费者消息，再想下线就非常困难了。对于需要下线的服务管控也很重要，有些服务虽然调用频次不高，业务量也不大。但是如果贸然下线，很有可能导致依赖它的消费者业务调用失败，这会违反服务的SLA协定，给服务提供商造成损失。服务的上线审批、下线通知机制需要建立完善起来，它的工作原理如下所示： 除了以上介绍的常用服务治理措施，线下服务治理还包括：1.业务的梳理、服务划分原则和方法论；2.服务跨团队协作流程、准则、工具和方法论；3.服务的接口兼容性原则和规范；4.其它…线下服务治理依团队和业务不同，需求也不同，需要业务团队和服务框架团队长期梳理、实践和优化，才能够提升线下服务治理的效率，它的建设是个长期过程，并非一蹴而就。 云端自治理微服务弹性伸缩基于PaaS云化平台或者Docker容器服务，可以实现基于负载的微服务弹性伸缩。基于PaaS平台部署微服务架构示例如下： 基于Docker容器部署微服务示例如下： 基于云的动态资源调度，可以实现微服务的弹性伸缩：基于CPU、内存、磁盘、网络带宽等资源占用率的弹性伸缩策略。当VM或者容器的资源占用达到设置的阈值之后，自动执行扩容策略，动态创建微服务的运行环境，部署并运行新的微服务实例。基于业务指标的弹性伸缩策略。例如微服务的平均时延、吞吐量、成功率等。通过对微服务的性能指标进行分布式采集、汇总和计算，判断业务指标是否达到伸缩阈值，如果达到，则自动触发微服务的伸缩流程，执行弹性伸缩。用户自定义的弹性伸缩策略。可以对基于资源占用率的伸缩策略和基于业务指标的伸缩策略做组合，实现更复杂的弹性伸缩。基于云平台的微服务弹性伸缩流程如下所示： E2E微服务生命周期管理利用云平台对资源的动态编排和调度，可以实现基础设施自动化。利用ALM（应用生命周期管理）可以实现微服务的E2E生命周期管理。基于Docker容器的微服务基础设施自动化流程如下所示： 微服务上线运行之后，利用云平台的ALM服务，可以对微服务进行上下线、升级、回滚等生命周期管理操作： 基于云平台提供的微服务生命周期管理服务，可以实现海量微服务的高效部署、升级和管理，而不需要关心物理基础设施的环境准备和安装，以及资源规划等，极大的提升了微服务的上线运行效率，降低了微服务的管理成本。 微服务治理全景图 微服务治理涵盖的范围非常广，很多治理手段也需要业务在实际开发中积累和沉淀，并没有统一的标准，这就是实施微服务治理的困难之处。在微服务治理发展的同时，云化和容器化革命也正在进行，结合云平台的敏捷性和弹性资源调度，微服务治理将逐步由人工治理向自动化治理演进。微服务治理总体结构图如下所示： Q&amp;AQ1：请问在实际使用时，前端网关有什么来源框架，还有分布式跟踪系统，有推荐吗？A1: 前端网关，开源的有WSO2，基于Java语言的，GO语言的有Tyk。 Q2:能展开讲一下优先级调度么A1：分布式跟踪系统打印 埋点日志比较简单，但是复杂的是后端的大数据分析。采集可以基于FLume等，后端的分析可以基于HBase Spark Q3:请教一下，对应用层扩容很容易，很多时候一个服务慢了，根本原因是依赖的存储 数据库 外部接口的原因，这个时候对应用层扩容解决不了问题，paas的扩容还有什么意义呢？ 数据库扩容 涉及数据迁移，应用层连接池更新等等 paas不能简单扩容A3：PaaS层的扩容通常会有几种策略：1、基于资源使用率的扩容；2、基于服务性能指标的扩容；3、混合模式；4、业务自定义扩容策略，这种场景通常是级联扩容，也就是应用依赖的服务也需要同时做扩容，例如缓存、MQ等。但是，不是所有的PaaS都支持策略4。 Q4:怎样从传统的系统转化到云服务上，在系统设计及技术架构有什么需要注意点。A4: 不知道你讲的传统系统是不是指的非云系统。非云应用转到云化服务有几点设计考虑：1、服务化；2、利用云的动态性，例如弹性伸缩等；3、统一配置，使用云化的统一配置服务。 Q5：那mq 缓存 数据库的client都要改造 支持后端自动发现了，好多中间价的client都是配置死的，有可分享的开源实现么A5：包括前端的URL地址，MQ服务端的URL等，云化之后，MQ等服务也是一种云化服务，例如AWS的S3服务。在我们的实践中，原来的本地配置都统一放到了配置服务上，它是基于ZK的云化统一配置服务，地址都是从注册中心读取的，而不是本地配置。这样，就可以支持动态发现。 Q6：应用服务化后，涉及服务与服务之间的远程rpc，请问数据传输过程中一般采用哪种系列化方式，之间的优缺点都有哪些？还有场景A6： 几种场景考量：1、如果服务看中的是标准化、开放性，对性能要求不是特别苛刻。则建议采用 Restful JSON的方式；2、如果是内部各模块之间的服务化调用，对性能、时延要求非常高，则建议采用二进制私有协议的方式，例如可以参考或者选择ProtocolBuf、Thrift等。通常而言，服务跟协议是解耦的，也就是说某个服务，可以同时发布成多种协议。 总结：微服务上线只是微服务前期的工作，其上线后的治理和运维也是微服务重要的组成部分，如何做好治理和运维，以及这条路上的技术架构演变，李林峰在以上给了一个好的阐述，但是随着微服务概念的普及，一定不断有新的治理和运维方式出现。本博也会不断的跟进微服务的治理和运维。希望各位读者也能提出问题，交流思想和解决问题。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>微服务治理</category>
      </categories>
      <tags>
        <tag>架构实践</tag>
        <tag>技术演进</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式，集群，SOA，微服务的区别与联系]]></title>
    <url>%2F2019%2F05%2F06%2Fservice-concept-diff%2F</url>
    <content type="text"><![CDATA[分布式：不同模块部署在不同服务器上作用：分布式解决网站高并发带来问题 集群：多台服务器部署相同应用构成一个集群作用：通过负载均衡设备共同对外提供服务 SOA：业务系统分解为多个组件，让每个组件都独立提供离散，自治，可复用的服务能力，通过服务的组合和编排来实现上层的业务流程作用：简化维护,降低整体风险,伸缩灵活 微服务：架构设计概念,各服务间隔离（分布式也是隔离）,自治（分布式依赖整体组合）其它特性(单一职责,边界,异步通信,独立部署)是分布式概念的跟严格执行SOA到微服务架构的演进过程作用：各服务可独立应用，组合服务也可系统应用 简单来说，对于分布式和集群：分布式：一个业务分拆多个子业务，部署在不同的服务器上集群：同一个业务，部署在多个服务器上 那么SOA和微服务的关系：1.首先,肯定的是SOA和微服务的确是一脉相承的，大神Martin Fowler提出来这一概念可以说把SOA的理念继续升华，精进了一步。其核心思想是在应用开发领域，使用一系列微小服务来实现单个应用的方式途径，或者说微服务的目的是有效的拆分应用，实现敏捷开发和部署，可以是使用不同的编程语言编写。而SOA可能包含的意义更泛一些，更不准确一些。 2.其次，从实现方式上，两者都是中立性，语言无关，协议跨平台，相比SOA，微服务框架将能够带来更大的敏捷性，并为你构建应用提供更轻量级、更高效率的开发。而SOA更适合大型企业中的业务过程编排、应用集成。另外还有微服务甚至是去ESB、去中心化、分布式的，而SOA还是以ESB为核心，大量的WS标准实现。 3.再次，从服务粒度上，既然是微，必然微服务更倡导服务的细粒度，重用组合，甚至是每个操作（或方法）都是独立开发的服务，足够小到不能再进行拆分。而SOA没有这么极致的要求，只需要接口契约的规范化，内部实现可以更粗粒度，微服务更多为了可扩充性、负载均衡以及提高吞吐量而去分解应用，但同时也引发了打破数据模型以及维护一致性的问题。 4.最后，从部署方式上，这个是最大的不同，对比Monolithic（有人翻译为单体）的Java EE部署架构，通过展现层打包WARs，业务层划分到JARs最后部署为EAR一个大包，而微服务则打开了这个黑盒子，把应用拆分成为一个一个的单个服务，应用Docker技术，不依赖任何服务器和数据模型，是一个全栈应用，可以通过自动化方式独立部署，每个服务运行在自己的进程中，通过轻量的通讯机制联系，经常是基于HTTP资源API，这些服务基于业务能力构建，能实现集中化管理（因为服务太多啦，不集中管理就无法DevOps啦）。 知乎上关于这个的讨论更加详细，可以点击了解更深度的对比SOA和微服务架构的区别？ 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>分布式</category>
        <category>集群</category>
        <category>SOA</category>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>区别</tag>
        <tag>联系</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入解读微服务架构下分布式事务解决方案]]></title>
    <url>%2F2019%2F05%2F06%2Fmicroservice-transacton%2F</url>
    <content type="text"><![CDATA[1 微服务的发展微服务倡导将复杂的单体应用拆分为若干个功能简单、松耦合的服务，这样可以降低开发难度、增强扩展性、便于敏捷开发。当前被越来越多的开发者推崇，很多互联网行业巨头、开源社区等都开始了微服务的讨论和实践。Hailo有160个不同服务构成，NetFlix有大约600个服务。国内方面，阿里巴巴、腾讯、360、京东、58同城等很多互联网公司都进行了微服务化实践。当前微服务的开发框架也非常多，比较著名的有Dubbo、SpringCloud、thrift 、grpc等。 2 微服务落地存在的问题虽然微服务现在如火如荼，但对其实践其实仍处于探索阶段。很多中小型互联网公司，鉴于经验、技术实力等问题，微服务落地比较困难。如著名架构师Chris Richardson所言，目前存在的主要困难有如下几方面：1）单体应用拆分为分布式系统后，进程间的通讯机制和故障处理措施变的更加复杂。2）系统微服务化后，一个看似简单的功能，内部可能需要调用多个服务并操作多个数据库实现，服务调用的分布式事务问题变的非常突出。3）微服务数量众多，其测试、部署、监控等都变的更加困难。 随着RPC框架的成熟，第一个问题已经逐渐得到解决。例如dubbo可以支持多种通讯协议，springcloud可以非常好的支持restful调用。对于第三个问题，随着docker、devops技术的发展以及各公有云paas平台自动化运维工具的推出，微服务的测试、部署与运维会变得越来越容易。而对于第二个问题，现在还没有通用方案很好的解决微服务产生的事务问题。分布式事务已经成为微服务落地最大的阻碍，也是最具挑战性的一个技术难题。为此，本文将深入和大家探讨微服务架构下，分布式事务的各种解决方案，并重点为大家解读阿里巴巴提出的分布式事务解决方案—-GTS。该方案中提到的GTS是全新一代解决微服务问题的分布式事务互联网中间件。 3 SOA分布式事务解决方案3.1 基于XA协议的两阶段提交方案交易中间件与数据库通过 XA 接口规范，使用两阶段提交来完成一个全局事务， XA 规范的基础是两阶段提交协议。第一阶段是表决阶段，所有参与者都将本事务能否成功的信息反馈发给协调者；第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交或者回滚。 基于XA协议的两阶段提交方案 两阶段提交方案应用非常广泛，几乎所有商业OLTP数据库都支持XA协议。但是两阶段提交方案锁定资源时间长，对性能影响很大，基本不适合解决微服务事务问题。 3.2 TCC方案TCC方案在电商、金融领域落地较多。TCC方案其实是两阶段提交的一种改进。其将整个业务逻辑的每个分支显式的分成了Try、Confirm、Cancel三个操作。Try部分完成业务的准备工作，confirm部分完成业务的提交，cancel部分完成事务的回滚。基本原理如下图所示。 TCC方案 事务开始时，业务应用会向事务协调器注册启动事务。之后业务应用会调用所有服务的try接口，完成一阶段准备。之后事务协调器会根据try接口返回情况，决定调用confirm接口或者cancel接口。如果接口调用失败，会进行重试。TCC方案让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。 当然TCC方案也有不足之处，集中表现在以下两个方面：对应用的侵入性强。业务逻辑的每个分支都需要实现try、confirm、cancel三个操作，应用侵入性较强，改造成本高。实现难度较大。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，confirm和cancel接口必须实现幂等。 上述原因导致TCC方案大多被研发实力较强、有迫切需求的大公司所采用。微服务倡导服务的轻量化、易部署，而TCC方案中很多事务的处理逻辑需要应用自己编码实现，复杂且开发量大。 3.3 基于消息的最终一致性方案消息一致性方案是通过消息中间件保证上、下游应用数据操作的一致性。基本思路是将本地操作和发送消息放在一个事务中，保证本地操作和消息发送要么两者都成功或者都失败。下游应用向消息系统订阅该消息，收到消息后执行相应操作。 消息一致性方案 消息方案从本质上讲是将分布式事务转换为两个本地事务，然后依靠下游业务的重试机制达到最终一致性。基于消息的最终一致性方案对应用侵入性也很高，应用需要进行大量业务改造，成本较高。 4 GTS–分布式事务解决方案GTS是一款分布式事务中间件，由阿里巴巴中间件部门研发，可以为微服务架构中的分布式事务提供一站式解决方案。更多GTS资料请访问创始人微博。 4.1 GTS的核心优势性能超强GTS通过大量创新，解决了事务ACID特性与高性能、高可用、低侵入不可兼得的问题。单事务分支的平均响应时间在2ms左右，3台服务器组成的集群可以支撑3万TPS以上的分布式事务请求。 应用侵入性极低GTS对业务低侵入，业务代码最少只需要添加一行注解（@TxcTransaction）声明事务即可。业务与事务分离，将微服务从事务中解放出来，微服务关注于业务本身，不再需要考虑反向接口、幂等、回滚策略等复杂问题，极大降低了微服务开发的难度与工作量。 完整解决方案GTS支持多种主流的服务框架，包括EDAS，Dubbo，Spring Cloud等。有些情况下，应用需要调用第三方系统的接口，而第三方系统没有接入GTS。此时需要用到GTS的MT模式。GTS的MT模式可以等价于TCC模式，用户可以根据自身业务需求自定义每个事务阶段的具体行为。MT模式提供了更多的灵活性，可能性，以达到特殊场景下的自定义优化及特殊功能的实现。 容错能力强GTS解决了XA事务协调器单点问题，实现真正的高可用，可以保证各种异常情况下的严格数据一致。 4.2 GTS的应用场景GTS可应用在涉及服务调用的多个领域，包括但不限于金融支付、电信、电子商务、快递物流、广告营销、社交、即时通信、手游、视频、物联网、车联网等，详细介绍可以阅读 《GTS–阿里巴巴分布式事务全新解决方案》一文。 4.3 GTS与微服务的集成GTS包括客户端（GTS Client）、资源管理器（GTS RM）和事务协调器（GTS Server）三个部分。GTS Client主要用来界定事务边界，完成事务的发起与结束。GTS RM完成事务分支的创建、提交、回滚等操作。GTS Server主要负责分布式事务的整体推进，事务生命周期的管理。GTS和微服务集成的结构图如下所示，GTS Client需要和业务应用集成部署，RM与微服务集成部署。 GTS与微服务集成 4.4 GTS的输出形式GTS目前有三种输出形式：公有云输出、公网输出、专有云输出。 4.4.1 公有云输出这种输出形式面向阿里云用户。如果用户的业务系统已经部署到阿里云上，可以申请开通公有云GTS。开通后业务应用即可通过GTS保证服务调用的一致性。这种使用场景下，业务系统和GTS间的网络环境比较理想，达到很好性能。 公有云输出 4.4.2 公网输出这种输出形式面向于非阿里云的用户，使用更加方便、灵活，业务系统只要能连接互联网即可享受GTS提供的云服务（与公有云输出的差别在于客户端部署于用户本地，而不在云上）。在正常网络环境下，以包含两个本地事务的全局事务为例，事务完成时间在20ms左右，50个并发就可以轻松实现1000TPS以上分布式事务，对绝大多数业务来说性能是足够的。在公网环境，网络闪断很难完全避免，这种情况下GTS仍能保证服务调用的数据一致性。 公网输出 具体使用样例使用参见4.7节GTS的工程样例。 4.4.3 专有云输出这种形式主要面向于已建设了自己专有云平台的大用户，GTS可以直接部署到用户的专有云上，为专有云提供分布式事务服务。目前已经有10多个特大型企业的专有云使用GTS解决分布式事务难题，性能与稳定性经过了用户的严格检测。 4.5 GTS的使用方式GTS对应用的侵入性非常低，使用也很简单。下面以订单存储应用为例说明。订单业务应用通过调用订单服务和库存服务完成订单业务，服务开发框架为Dubbo。 4.5.1 订单业务应用在业务函数外围使用@TxcTransaction注解即可开启分布式事务。Dubbo应用通过隐藏参数将GTS的事务xid传播到服务端。123456789101112131415161718@TxcTransaction(timeout = 1000 * 10)public void Bussiness(OrderService orderService, StockService stockService, String userId) &#123; //获取事务上下文 String xid = TxcContext.getCurrentXid(); //通过RpcContext将xid传到一个服务端 RpcContext.getContext().setAttachment("xid", xid); //执行自己的业务逻辑 int productId = new Random().nextInt(100); int productNum = new Random().nextInt(100); OrderDO orderDO = new OrderDO(userId, productId, productNum, new Timestamp(new Date().getTime())); orderService.createOrder(orderDO); //通过RpcContext将xid传到另一个服务端 RpcContext.getContext().setAttachment("xid",xid); stockService.updateStock(orderDO);&#125; 4.5.2 服务提供者更新库存方法1234567891011public int updateStock(OrderDO orderDO) &#123; //获取全局事务ID，并绑定到上下文 String xid = RpcContext.getContext().getAttachment("xid"); TxcContext.bind(xid,null); //执行自己的业务逻辑 String sql = "update stock set amount = amount - ? where product_id = ?"; int ret = jdbcTemplate.update(sql,new Object[]&#123;orderDO.getNumber(), orderDO.getProductId()&#125;); TxcContext.unbind(); return ret;&#125; 4.6 GTS的应用情况GTS目前已经在淘宝、天猫、阿里影业、淘票票、阿里妈妈、1688等阿里各业务系统广泛使用，经受了16年和17年两年双十一海量请求的考验。某线上业务系统最高流量已达十万TPS（每秒钟10万笔事务）。GTS在公有云和专有云输出后，已经有了100多个线上用户，很多用户通过GTS解决SpringCloud、Dubbo、Edas等服务框架的分布式事务问题。业务领域涉及电力、物流、ETC、烟草、金融、零售、电商、共享出行等十几个行业，得到用户的一致认可。 GTS与springCloud集成案例 上图是GTS与SpringCloud集成，应用于某共享出行系统。业务共享出行场景下，通过GTS支撑物联网系统、订单系统、支付系统、运维系统、分析系统等系各统应用的数据一致性，保证海量订单和数千万流水的交易。 4.7 GTS的工程样例GTS的公有云样例可参考阿里云网站。在公网环境下提供sample-txc-simple和sample-txc-dubbo两个样例工程。注意mysql-connector-java 的版本需要和 MySQL 数据库版本匹配。样例 pom.xml 中推荐的 5.1.38 版本已经在 5.0.55、5.6.16、5.6.21 三个 MySQL 数据库版本上运行过。MySQL 数据库的库名、表名和字段名需要设置为大小写不敏感。sample-txc-dubbo样例使用 Multicast 注册中心的声明方式。如果本机使用无线网络，Dubbo 服务在绑定地址时有可能获取 IPv6 地址，可以通过 JVM 启动参数禁用。方法是配置 JVM 启动参数 -Djava.net.preferIPv4Stack=true。 4.7.1 sample-txc-simple样例4.7.1.1 样例业务逻辑该样例是GTS的入门sample，案例的业务逻辑是从A账户转账给B账户，其中A和B分别位于两个MySQL数据库中，使用GTS事务保证A和B账户钱的总数始终不变。 4.7.1.2 样例搭建方法1.准备本地数据库环境准备一台能连接公网的电脑，配置 Java 环境和 Maven 环境。安装 MySQL 数据库软件，并创建两个数据库 db1 和 db2。在 db1 和 db2 中分别创建 txc_undo_log 表。在 db1 库中创建 user_money_a 表，在 db2 库中创建 user_money_b 表 2.下载样例将sample-txc-simple(点击下载)文件下载到本地，样例中已经包含了GTS的SDK。 3.修改配置3.1数据源配置：打开sample-txc-simple/src/main/resources目录下的txc-client-context.xml，将数据源的url、username、password修改为实际值。3.2scanner配置将 txc-client-context.xml 中的 scanner 配置修改为:123456&lt;bean class="com.taobao.txc.client.aop.TxcTransactionScaner"&gt;&lt;constructor-arg value="myapp"/&gt;&lt;constructor-arg value="txc_test_public.1129361738553704.QD"/&gt;&lt;constructor-arg value="1" /&gt;&lt;constructor-arg value="https://test-cs-gts.aliyuncs.com" /&gt;&lt;/bean&gt; 4.编译样例4.1Mac OS 或 Linux 系统在 sample-txc-simple 目录下执行 build.sh 命令。4.2Windows 系统在 sample-txc-simple 目录下执行 build.bat 命令。 5.运行样例5.1Mac OS 或 Linux 系统在 txc-yun-sample/sample-txc-simple/client/bin 目录下执行 run.sh 命令。5.2Windows 系统启动 cmd.exe，在 txc-yun-sample/sample-txc-simple/client/bin 目录下执行 start.bat 命令。 4.7.2 sample-txc-dubbo 样例4.7.2.1 样例业务逻辑本案例模拟了用户下订单、减库存的业务逻辑。客户端（Client）通过调用订单服务（OrderService）创建订单，之后通过调用库存服务（StockService）扣库存。其中订单服务读写订单数据库，库存服务读写库存数据库。由 GTS 保证跨服务事务的一致性。 4.7.2.2 样例搭建方法1.准备数据库环境安装MySQL，创建两个数据库db1和db2。在db1和db2中分别创建txc_undo_log表。在db1库中创建orders表，在db2库中创建stock表。 2.下载样例将样例文件(点击下载)下载到本地机器，样例中已经包含了GTS的SDK。 3.修改配置3.1数据源配置打开sample-txc-dubbo/src/main/resources目录，将dubbo-order-service.xml、dubbo-stock-service.xml两个文件中数据源的url、username、password修改为实际值。3.2scanner配置将 resource 目录下的每个 xml 文件中的 scanner 配置修改为:123456&lt;bean class="com.taobao.txc.client.aop.TxcTransactionScaner"&gt;&lt;constructor-arg value="myapp"/&gt;&lt;constructor-arg value="txc_test_public.1129361738553704.QD"/&gt;&lt;constructor-arg value="1" /&gt;&lt;constructor-arg value="https://test-cs-gts.aliyuncs.com" /&gt;&lt;/bean&gt; 4.编译样例4.1 Mac OS 或 Linux 系统在 sample-txc-dubbo 目录下执行 build.sh 命令。4.2 Windows 系统在 sample-txc-dubbo 目录下执行 build.bat 命令。 5.运行样例5.1 Mac OS 或 Linux 系统在 txc-yun-sample/sample-txc-dubbo 目录下执行 run.sh 命令。该脚本会依次启动 order_run.sh(订单服务)、stock_run.sh(库存服务)和 client_run.sh(客户端程序)。5.2 Windows 系统启动 cmd.exe，进入 txc-yun-sample/sample-txc-dubbo/client/bin 目录。执行 order.bat 命令，运行订单服务。执行 stock.bat 命令，运行库存服务。订单服务和库存服务都启动成功后，执行 client.bat 命令， 运行客户端程序。 4.7.3 SQL4.7.3.1 建表 txc_undo_log123456789101112CREATE TABLE txc_undo_log (id bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键',gmt_create datetime NOT NULL COMMENT '创建时间',gmt_modified datetime NOT NULL COMMENT '修改时间',xid varchar(100) NOT NULL COMMENT '全局事务ID',branch_id bigint(20) NOT NULL COMMENT '分支事务ID',rollback_info longblob NOT NULL COMMENT 'LOG',status int(11) NOT NULL COMMENT '状态',server varchar(32) NOT NULL COMMENT '分支所在DB IP',PRIMARY KEY (id),KEY unionkey (xid,branch_id)) ENGINE=InnoDB AUTO_INCREMENT=211225994 DEFAULT CHARSET=utf8 COMMENT='事务日志表'; 4.7.3.2 建表 user_money_a12345CREATE TABLE user_money_a (id int(11) NOT NULL AUTO_INCREMENT,money int(11) DEFAULT NULL,PRIMARY KEY (id)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8; 4.7.3.3 建表 user_money_b12345CREATE TABLE user_money_b (id int(11) NOT NULL AUTO_INCREMENT,money int(11) DEFAULT NULL,PRIMARY KEY (id)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8; 4.7.3.4 建表 orders12345678CREATE TABLE orders (id bigint(20) NOT NULL AUTO_INCREMENT,user_id varchar(255) NOT NULL,product_id int(11) NOT NULL,number int(11) NOT NULL,gmt_create timestamp NOT NULL,PRIMARY KEY (id)) ENGINE=MyISAM AUTO_INCREMENT=351 DEFAULT CHARSET=utf8 4.7.3.5 建表 stock123456CREATE TABLE stock (product_id int(11) NOT NULL,price float NOT NULL,amount int(11) NOT NULL,PRIMARY KEY (product_id)) ENGINE=InnoDB DEFAULT CHARSET=utf8 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>事务</category>
      </categories>
      <tags>
        <tag>阿里</tag>
        <tag>TXC</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事务的解决方案]]></title>
    <url>%2F2019%2F05%2F04%2FSolution-of-Distributed-Transaction%2F</url>
    <content type="text"><![CDATA[1、什么是分布式事务分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。以上是百度百科的解释，简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。 2、分布式事务的产生的原因2.1、数据库分库分表当数据库单表一年产生的数据超过1000W，那么就要考虑分库分表，具体分库分表的原理在此不做解释，以后有空详细说，简单的说就是原来的一个数据库变成了多个数据库。这时候，如果一个操作既访问01库，又访问02库，而且要保证数据的一致性，那么就要用到分布式事务。 2.2、应用SOA化所谓的SOA化，就是业务的服务化。比如原来单机支撑了整个电商网站，现在对整个网站进行拆解，分离出了订单中心、用户中心、库存中心。对于订单中心，有专门的数据库存储订单信息，用户中心也有专门的数据库存储用户信息，库存中心也会有专门的数据库存储库存信息。这时候如果要同时对订单和库存进行操作，那么就会涉及到订单数据库和库存数据库，为了保证数据一致性，就需要用到分布式事务。 以上两种情况表象不同，但是本质相同，都是因为要操作的数据库变多了！ 3、事务的ACID特性3.1、原子性（A）所谓的原子性就是说，在整个事务中的所有操作，要么全部完成，要么全部不做，没有中间状态。对于事务在执行中发生错误，所有的操作都会被回滚，整个事务就像从没被执行过一样。 3.2、一致性（C）事务的执行必须保证系统的一致性，就拿转账为例，A有500元，B有300元，如果在一个事务里A成功转给B50元，那么不管并发多少，不管发生什么，只要事务执行成功了，那么最后A账户一定是450元，B账户一定是350元。 3.3、隔离性（I）所谓的隔离性就是说，事务与事务之间不会互相影响，一个事务的中间状态不会被其他事务感知。 3.4、持久性（D）所谓的持久性，就是说一单事务完成了，那么事务对数据所做的变更就完全保存在了数据库中，即使发生停电，系统宕机也是如此。 4、分布式事务的应用场景4.1、支付最经典的场景就是支付了，一笔支付，是对买家账户进行扣款，同时对卖家账户进行加钱，这些操作必须在一个事务里执行，要么全部成功，要么全部失败。而对于买家账户属于买家中心，对应的是买家数据库，而卖家账户属于卖家中心，对应的是卖家数据库，对不同数据库的操作必然需要引入分布式事务。 4.2、在线下单买家在电商平台下单，往往会涉及到两个动作，一个是扣库存，第二个是更新订单状态，库存和订单一般属于不同的数据库，需要使用分布式事务保证数据一致性。 5、常见的分布式事务解决方案5.1、基于XA协议的两阶段提交XA是一个分布式事务协议，由Tuxedo提出。XA中大致分为两部分：事务管理器和本地资源管理器。其中本地资源管理器往往由数据库实现，比如Oracle、DB2这些商业数据库都实现了XA接口，而事务管理器作为全局的调度者，负责各个本地资源的提交和回滚。XA实现分布式事务的原理如下： 总的来说，XA协议比较简单，而且一旦商业数据库实现了XA协议，使用分布式事务的成本也比较低。但是，XA也有致命的缺点，那就是性能不理想，特别是在交易下单链路，往往并发量很高，XA无法满足高并发场景。XA目前在商业数据库支持的比较理想，在MySQL数据库中支持的不太理想，mysql的XA实现，没有记录prepare阶段日志，主备切换回导致主库与备库数据不一致。许多nosql也没有支持XA，这让XA的应用场景变得非常狭隘。 5.2、消息事务+最终一致性所谓的消息事务就是基于消息中间件的两阶段提交，本质上是对消息中间件的一种特殊利用，它是将本地事务和发消息放在了一个分布式事务里，保证要么本地操作成功成功并且对外发消息成功，要么两者都失败，开源的RocketMQ就支持这一特性，具体原理如下： 1、A系统向消息中间件发送一条预备消息2、消息中间件保存预备消息并返回成功3、A执行本地事务4、A发送提交消息给消息中间件 通过以上4步完成了一个消息事务。对于以上的4个步骤，每个步骤都可能产生错误，下面一一分析： 步骤一出错，则整个事务失败，不会执行A的本地操作步骤二出错，则整个事务失败，不会执行A的本地操作步骤三出错，这时候需要回滚预备消息，怎么回滚？答案是A系统实现一个消息中间件的回调接口，消息中间件会去不断执行回调接口，检查A事务执行是否执行成功，如果失败则回滚预备消息步骤四出错，这时候A的本地事务是成功的，那么消息中间件要回滚A吗？答案是不需要，其实通过回调接口，消息中间件能够检查到A执行成功了，这时候其实不需要A发提交消息了，消息中间件可以自己对消息进行提交，从而完成整个消息事务 基于消息中间件的两阶段提交往往用在高并发场景下，将一个分布式事务拆成一个消息事务（A系统的本地操作+发消息）+B系统的本地操作，其中B系统的操作由消息驱动，只要消息事务成功，那么A操作一定成功，消息也一定发出来了，这时候B会收到消息去执行本地操作，如果本地操作失败，消息会重投，直到B操作成功，这样就变相地实现了A与B的分布式事务。原理如下： 虽然上面的方案能够完成A和B的操作，但是A和B并不是严格一致的，而是最终一致的，我们在这里牺牲了一致性，换来了性能的大幅度提升。当然，这种玩法也是有风险的，如果B一直执行不成功，那么一致性会被破坏，具体要不要玩，还是得看业务能够承担多少风险。 5.3、TCC编程模式所谓的TCC编程模式，也是两阶段提交的一个变种。TCC提供了一个编程框架，将整个业务逻辑分为三块：Try、Confirm和Cancel三个操作。以在线下单为例，Try阶段会去扣库存，Confirm阶段则是去更新订单状态，如果更新订单失败，则进入Cancel阶段，会去恢复库存。总之，TCC就是通过代码人为实现了两阶段提交，不同的业务场景所写的代码都不一样，复杂度也不一样，因此，这种模式并不能很好地被复用。 6、总结分布式事务，本质上是对多个数据库的事务进行统一控制，按照控制力度可以分为：不控制、部分控制和完全控制。不控制就是不引入分布式事务，部分控制就是各种变种的两阶段提交，包括上面提到的消息事务+最终一致性、TCC模式，而完全控制就是完全实现两阶段提交。部分控制的好处是并发量和性能很好，缺点是数据一致性减弱了，完全控制则是牺牲了性能，保障了一致性，具体用哪种方式，最终还是取决于业务场景。作为技术人员，一定不能忘了技术是为业务服务的，不要为了技术而技术，针对不同业务进行技术选型也是一种很重要的能力。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>事务</category>
      </categories>
      <tags>
        <tag>阿里</tag>
        <tag>TXC</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TXC分布式事务简介]]></title>
    <url>%2F2019%2F05%2F04%2Ftransaction-txc%2F</url>
    <content type="text"><![CDATA[1. TXC是什么TXC(Taobao Transaction Constructor)是阿里巴巴的一个分布式事务中间件，它可以通过极少的代码侵入，实现分布式事务。在大部分情况下，应用只需要引入TXC Client的jar包，进行几项简单配置，以及以行计的代码改造，即可轻松保证分布式数据一致性。TXC同时提供了丰富的编程和配置策略，以适应各种长尾的应用需求。 2. 背景2.1. 什么是事务？以下内容来自 维基百科相关词条。一个数据库事务通常包含了一个序列的对数据库的读/写操作。它的存在包含有以下两个目的：1.为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。2.当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。 并非任意的对数据库的操作序列都是数据库事务。数据库事务拥有以下四个特性，习惯上被称之为 ACID特性。原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。持久性（Durability）：已被提交的事务对数据库的修改应该永久保存在数据库中。 2.2 什么是分布式事务？分布式事务是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的 分布式系统 的不同节点之上。在应用程序只部署在一台计算机，数据库只部署在一台计算机的情况下，事务的ACID四个特性很容易全部满足。但是单机的处理能力很容易达到上限，此时必须使用分布式系统。在分布式环境下，应用程序可能部署在多台计算机，并且可能有多个不同的应用程序参与到同一个事务中；数据库也可能部署在多台计算机，并且多个不同的数据库可能会参与到同一个事务中。在这样的分布式环境下，高吞吐量和ACID很难被同时满足。 目前较为流行的分布式事务解决方案可以分为两类： 两阶段提交两阶段提交，是实现分布式事务的成熟方案。第一阶段是表决阶段，是所有参与者都将本事务能否成功的反馈发给协调者；第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交，或者在所有分支上回滚。 两阶段提交可以满足ACID，但代价是吞吐量。例如，数据库需要频繁地对资源上锁等等。而且更致命的是，资源被锁住的时间相对较长—-在第一阶段即需要上锁，第二阶段才能解锁，依赖于所有分支的最慢者—-这期间没有任何人可以对该资源进行修改。两阶段提交理论的一个广泛工业应用是XA协议。目前几乎所有收费的商业数据库都支持XA协议。XA协议已在业界成熟运行数十年，但目前它在互联网海量流量的应用场景中，吞吐量这个瓶颈变得十分致命，因此很少被用到。 基于XA协议的分布式事务实现详见《分布式事务解决方案》 TCCTCC（Try、Confirm、Cancel）是两阶段提交的一个变种。TCC提供了一个框架，需要应用程序按照该框架编程，将业务逻辑的每个分支都分为Try、Confirm、Cancel三个操作集。TCC让应用程序自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。以一个典型的淘宝订单为例，按照TCC框架，应用需要在Try阶段将商品的库存减去，将买家支付宝账户中的相应金额扣掉，在临时表中记录下商品的数量，订单的金额等信息；另外再编写Confirm的逻辑，即在临时表中删除相关记录，生成订单，告知CRM、物流等系统，等等；以及Cancel逻辑，即恢复库存和买家账户金额，删除临时表相关记录。我们能找到的最早出处是atomikos的ExtremeTransactions产品中提出了TCC的概念。在撰写本文时，atomikos网站不能访问，只能在百度百科里面找到一篇相关的文章。目前，TCC概念最大的应用，一般认为是支付宝XTS系统。 为了更好地介绍TCC，这里先引入这么一个概念：最终一致性最终一致性目前没有公认的定义。一般来说，它是指事务进行中，某些分支的中间状态可以被事务外观察到，即”读未提交”，从而导致多个分支的状态可能不一致，但所有分支最终会达到要么全部提交，要么全部回滚的一致状态。很明显，最终一致性部分牺牲了ACID中的C和I，但它带来了可观的收益：资源不再需要长时间上锁，极大地提高了吞吐量。最终一致性在互联网应用场景中被广泛用做吞吐量和ACID的妥协点。让我们回到前面TCC的例子。在这个流程中，商品库存和买家余额都没有被锁住，因此可以得到很高的吞吐量。但在交易进行中，商品库存和买家余额的变化就已经被外界感知到，而物流系统却可能还没有相应的记录，此时数据是不一致的，但最终（无论是Confirm阶段结束后，还是Cancel阶段结束后）它们会一致。 3. TXC的目标应用场景TXC的目标应用场景是：解决在分布式应用中，多条数据库记录被修改而可能带来的一致性问题；该分布式应用可以接受最终一致性；该应用的事务改造对工作量有较严格的限制。 最终一致性如前所述，目前业内的所有分布式事务中间件都很难在保持高吞吐量的前提下，完全满足ACID四个特性。目前几乎所有的分布式事务中间件都是以牺牲这5个指标（性能、ACID）中的一个或多个来实现事务的，它们的区别往往在于牺牲了不同的指标。例如两阶段提交就是牺牲了性能，而ExtremeTransactions和XTS则是牺牲了一致性和隔离性。在这方面，TXC也采用了最终一致性，因此部分牺牲了一致性和隔离性。在此前提下，TXC在性能、隔离性之间提供了数个平衡点，应用程序管理员可以通过配置不同的TXC策略来选择本应用所需要的平衡点。详情请见后续章节。 代码侵入少除了数据一致性之外，为了实现分布式事务而花在开发上的开销，也是一个值得考虑的因素。如果没有分布式事务中间件，要达到类似目的，需要为业务上的每一个写SQL都编写回滚操作，这些回滚操作还必须是幂等的，应用程序员还要决定什么时候调用这些回滚操作中的全部或一部分—-毫无疑问这些工作是繁琐而容易出错的。所有的中间件，其目的都是为了让业务只需要关注于业务。通过TXC的自动接口，应用程序员只需要指定事务的边界，即可实现分布式事务，代码修改可以短至1行。代码侵入少了，犯错误的概率就小，开发周期也会随之缩短。最后，和支付宝XTS类似，TXC也提供了灵活、强大的类似TCC的手动接口，以较多的代码修改获取较高的性能。TXC推荐应用程序以自动接口实现大部分逻辑，用手动接口实现对性能非常敏感的一小部分核心逻辑。TXC保证这些逻辑都可以被包含在在同一个分布式事务中。 跨多应用的事务TXC的事务可以在应用之间随着HSF服务调用被传播，使得跨多应用、跨多库的数据库操作可以被包含在同一个事务内。这些不同的应用可以采用不同的TXC配置，从而满足各种各样的应用需求。综上所述，TXC的目标场景，是最终一致性、开发简便快捷、跨多应用的事务场景。 4. 概念及术语介绍事务超时时间 – TXC中，对每个分布式事务均需设定一个超时时间，超出超时时间未被提交的事务，均会被TXC框架回滚。事务分支 – 一个分布式事务可能包含多个分支，只有当所有的分支全部成功时，分布式事务才能成功，一个分支的失败将导致分布式事务的回滚。在TXC框架下，分支可能是一个分库上执行的sql语句，或是一个手动模式分支。事务边界 – 分布式事务需要进行开启，在执行结束后需要进行结束（提交或回滚），事务开启和关闭即划定了一个事务边界。事务模式 – TXC提供的预先定义好的事务模式，不同的事务模式提供了不同的易用性和性能，不同的事务模式组合（详见最佳实践）可解决极度复杂的场景。 TXC Client – 事务发起者，用以界定分布式事务边界RM – Resource Manager, Rm是事务中的资源管理器抽象，Rm定义了资源参与到事务中的行为，不同事务模式对应不同的资源管理器。TM – Transaction Manager,泛指事务协调器，在TXC中，其责任由TXC Server承担。 5. TXC的3种模式和它们各自适用的场景5.1 标准模式（AT模式）TXC标准模式（Automatic TXC）是最主要的事务模式，通过基于TDDL的TXC数据源，它对SQL语句提供了分布式事务支持。它帮助应用方以最小的改造代价来实现TDDL下的事务的功能。在标准模式下，当开启了TXC分布式事务时，TXC框架将自动的根据执行的SQL语句，进行事务分支划分（每个物理库上的一个本地事务作为一个分布式事务分支），把各个分支统一纳入事务。分布式事务的隔离级别可以配置，读未提交（read uncommitted）和读已提交（read committed）。读未提交是缺省设置。标准模式适合于TDDL分库分表、多TDDL数据源、跨进程的多TDDL数据源等几乎任何TDDL应用场景下的分布式事务。 5.2 自定义模式（MT模式）MT（Manual TXC）模式，提供用户可以介入两阶段提交过程的一种模式。在这种模式下，用户可以根据自身业务需求自定义在TXC的两阶段中每个阶段的具体行为。MT模式提供了更多的灵活性，可能性，以达到特殊场景下的自定义优化，及特殊功能的实现。MT模式不依赖于TDDL，这是它相对于标准模式的一个最大的优势。MT模式几乎满足任何我们想到的事务场景。 5.3 重试模式RT（Retry TXC）模式严格地说，不属于传统的事务范围。在TXC将其定义为一种特殊的事务，它通过在用户指定时间内不停的异步重试失败的SQL语句，来保证SQL语句最终成功。RT模式也是基于TXC数据源的。当我们通过TDDL执行一个需要分库的SQL，假设在第一个库执行成功了，但是在第二个库执行失败了。如果采用TXC标准模式，第一个库的SQL会回滚。对用户来说，他的SQL失败了，在两个库上是一致的。如果采用RT模式，第二个库执行失败的SQL会保存下来，TXC不断重试这个SQL，直到成功。对用户来说， 他的SQL成功了，在两个库上最终是一致的。当然，TXC不会一直重试SQL，用户可以指定一个超时时间，超过这个时间限制，TXC会发送告警信息到用户。用户拿到告警信息后，可以从业务库的RT SQL表中拿到对应的SQL语句，决定下一步怎么处理。试想一下，当一个SQL涉及到分库，我们执行这个SQL失败了，通常来说，我们需要通过log查出它在哪几个库成功了哪几个库失败了，并且在失败的库上不断重试。这是很繁琐的。RT模式把用户从这种繁琐工作中解脱出来，用户不再需要关注哪些库上SQL失败了，也不需要自己重试SQL。 6. TXC的ACID权衡6.1 原子性与一致性在AT方式下，事务范围内所有分支的写SQL操作应该要么都执行，要么都不执行。在弱隔离性策略下，有微小的可能无法保证一致性，参考”弱隔离性策略”。 6.2 持久性所有分支的状态都会被持久化，整个全局事务的状态也需要被持久化，以便在TXC Server或者client出故障之后可以回滚事务。 6.3 隔离性TXC提供了丰富的配置策略，其中与隔离性有关的有3种：弱隔离性策略、强隔离性策略，和重试策略。下面对这3种策略一一进行介绍。 (a) 弱隔离性策略（缺省策略）基于追求高吞吐量的考虑，TXC的缺省策略是弱隔离性，即读未提交。在应用程序尚未执行txc_commit()之前，各个分支上写操作的结果已经可以被外界观察到，并且可以被并发地修改。当事务需要被回滚时，TXC会以反向补偿机制来让事务各个参与者保持数据的一致性。但如果事务中被修改的某条记录如果在回滚前就被别的事务并发修改了，回滚就无法进行。此时TXC系统将产生报警，需要管理员手工订正数据。我们称这个问题为” 脏写”。因此，如果应用的业务逻辑决定了不太可能对同一行记录进行并发修改，缺省模式就比较适合这样的应用；否则，如果预期对同一行记录的修改可能会相对频繁，则推荐使用强隔离性策略。 (b)强隔离性策略TXC的强隔离性策略可以防前面提到的脏写问题，避免手工订正。在TXC的强隔离性策略下，所有的写操作都会检测目标记录是否已经被某个进行中的事务修改为中间状态。如果是，则写操作会在一段时间内重试以读取一个最终状态，或者在这段时间过去之后返回失败。同样的行为也适用于select for update。值得突出的是，如果应用A发起一个事务，在它进行中，同一个事务的其他分支（即随着HSF传播出去的事务分支）也可以并发地修改同一条记录，从而实现了紧耦合的事务。目前该功能仅在支持XA的商业解决方案中才被支持。对于纯读操作，为了吞吐量考虑，TXC一般不去做类似检测。当事务进行中对某个记录进行修改时，该中间状态可能会被其他事务读到。我们称这个问题为” 脏读”。一般来说，这样的脏读不会有严重后果；但如果应用确实需要避免脏读，那么可以在select语句上加上一些特定的hint。对于带这些hint的select语句，TXC也会牺牲性能，先做检测，如果相关记录属于某个事务的中间状态，select会在一段时间内重试以读取一个最终状态，或者在这段时间过去之后返回失败。 强隔离性策略要求所有可能访问业务表的应用都部署TXC并配置强隔离性策略。对于未部署TXC的系统，它们可以在事务进行中，跳过TXC直接并发修改业务表的同一条记录，从而造成脏写。目前尚无手段避免这样的脏写。 (c)重试策略重试策略并不属于严格意义上的事务范畴，因为需要重试的操作即使失败，也不会导致事务回滚。让我们考察这样的例子：当一个用户注销的时候，需要将该用户在多个表中的相关记录删除。这样的操作即使一时会因为网络、数据库主库当机等原因失败，它最终也基本上能保证会成功，因此业务往往不希望这样的操作会引起整个事务的回滚，而是希望简单地重试该操作，直到成功。在重试策略下，TXC可以”记住”这样的操作，在适当的时候代替应用执行重试。此时，可能会在比较大的时间窗口内，原子性和一致性没有被满足。应用程序需要根据自己的情况，决定是否采用重试策略。 7. TXC事务的传播通过一些简单的配置，TXC事务即可随HSF服务传播。即，当应用A发起了一个事务，在事务进行中调用应用B的服务BS1时，如果配置得当，BS1中的所有写SQL操作将被自动纳入此事务。应用B只需要安装TXC的客户端包并进行少量必要的配置。BS1不需要为此做任何代码修改。为了避免服务在不知情的情况下被卷入事务，一个HSF服务在缺省情况下是拒绝事务传播的，即接受事务传播的配置必须显示地被打开。该事务传播可以是多分支（A调用B、A调用C，以此类推）、多层级（A调用B、B调用C，以此类推）的。对于多层级的传播，要求每层服务都部署了TXC，并且显示地配置了接受事务传播，否则传播会被中断，不能进入后面的所有层级。该事务传播可以是异步的，但在事务提交前，应用必须保证所有异步操作都必须返回。 8. TXC的非功能性特点8.1 可用性当TXC Server有一台宕机时，当前有部分事务会被回滚，但新发起的事务仍能继续进行。当TXC Server有多台同时宕机时，会有部分事务被回滚，部分新发起的事务不能继续进行。当TXC Client所在的应用机器宕机时，该机器上发起或参与的所有事务都将被回滚，其中该机器发起的事务可能会被另一台同一应用、同一数据库DBKey的机器回滚（如果有的话）。 8.2 可扩展性TXC各个Server之间是share nothing的架构，因此可以水平扩展。各应用会随机地连接到一个TXC Server。当新增TXC Server时，Diamond配置会被推送，负载很快会重新平衡。 8.3 TXC的数据安全性TXC不会保留、分析业务数据。TXC仅仅在事务进行中会暂存一部分业务数据到业务数据库，以准备可能的回滚。TXC不会试图去理解这些数据背后的业务含义。当事务结束后这些数据会被立刻删除。 8.4 TXC支持的语言和平台目前仅支持Java、Linux 8.5 兼容性考虑TXC的不同模式之间可以互相兼容。即，如果应用A是AT模式，通过HSF调用配置为MT（或RT）模式的B服务，A与B的所有写SQL操作可以被包含到同一个TXC分布式事务中。以此类推。 8.6 部署和运维TXC在部署上分两个部分，TXC Server和TXC Client。TXC Server是若干个独立的服务器，通常由北京中间件团队维护，但特殊情况下也可以部署在应用方自己的机房内，由应用方自己维护。TXC Client是一个或多个二分包，应用需要依赖这些二方包，并通过若干配置（至少需要能通过配置连接上一个TXC Server）使用分布式事务功能。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>事务</category>
      </categories>
      <tags>
        <tag>阿里</tag>
        <tag>TXC</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Influxdb使用详解]]></title>
    <url>%2F2019%2F04%2F23%2Finfluxdb-use%2F</url>
    <content type="text"><![CDATA[1 安装配置这里说一下使用docker容器运行influxdb的步骤，物理机安装请参照官方文档。拉取镜像文件后运行即可，当前最新版本是1.3.5。启动容器时设置挂载的数据目录和开放端口。InfluxDB的操作语法InfluxQL与SQL基本一致，也提供了一个类似mysql-client的名为influx的CLI。InfluxDB本身是支持分布式部署多副本存储的，本文介绍都是针对的单节点单副本。1234567891011121314151617# docker pull influxdb# docker run -idt --name influxdb -p 8086:8086 -v /Users/ssj/influxdb:/var/lib/influxdb influxdbf216e9be15bff545befecb30d1d275552026216a939cc20c042b17419e3bde31# docker exec -it influxdb /bin/bash root@f216e9be15bf:/# influxConnected to http://localhost:8086 version 1.3.5InfluxDB shell version: 1.3.5&gt; create database cadvisor ## 创建数据库cadvisor&gt; show databases name: databasesname----_internalcadvisor&gt; CREATE USER testuser WITH PASSWORD 'testpwd' ## 创建用户和设置密码&gt; GRANT ALL PRIVILEGES ON cadvisor TO testuser ## 授权数据库给指定用户&gt; CREATE RETENTION POLICY "cadvisor_retention" ON "cadvisor" DURATION 30d REPLICATION 1 DEFAULT ## 创建默认的数据保留策略，设置保存时间30天，副本为1 2 重要概念influxdb里面有一些重要概念：database，timestamp，field key， field value， field set，tag key，tag value，tag set，measurement， retention policy ，series，point。结合下面的例子数据来说明这几个概念：1234567891011name: census-————————————time butterflies honeybees location scientist2015-08-18T00:00:00Z 12 23 1 langstroth2015-08-18T00:00:00Z 1 30 1 perpetua2015-08-18T00:06:00Z 11 28 1 langstroth2015-08-18T00:06:00Z 3 28 1 perpetua2015-08-18T05:54:00Z 2 11 2 langstroth2015-08-18T06:00:00Z 1 10 2 langstroth2015-08-18T06:06:00Z 8 23 2 perpetua2015-08-18T06:12:00Z 7 22 2 perpetua timestamp既然是时间序列数据库，influxdb的数据都有一列名为time的列，里面存储UTC时间戳。 field key，field value，field setbutterflies和honeybees两列数据称为字段(fields)，influxdb的字段由field key和field value组成。其中butterflies和honeybees为field key，它们为string类型，用于存储元数据。而butterflies这一列的数据12-7为butterflies的field value，同理，honeybees这一列的23-22为honeybees的field value。field value可以为string，float，integer或boolean类型。field value通常都是与时间关联的。field key和field value对组成的集合称之为field set。如下：12345678butterflies = 12 honeybees = 23butterflies = 1 honeybees = 30butterflies = 11 honeybees = 28butterflies = 3 honeybees = 28butterflies = 2 honeybees = 11butterflies = 1 honeybees = 10butterflies = 8 honeybees = 23butterflies = 7 honeybees = 22 在influxdb中，字段必须存在。注意，字段是没有索引的。如果使用字段作为查询条件，会扫描符合查询条件的所有字段值，性能不及tag。类比一下，fields相当于SQL的没有索引的列。 tag key，tag value，tag setlocation和scientist这两列称为标签(tags)，标签由tag key和tag value组成。location这个tag key有两个tag value：1和2，scientist有两个tag value：langstroth和perpetua。tag key和tag value对组成了tag set，示例中的tag set如下：1234location = 1, scientist = langstrothlocation = 2, scientist = langstrothlocation = 1, scientist = perpetualocation = 2, scientist = perpetua tags是可选的，但是强烈建议你用上它，因为tag是有索引的，tags相当于SQL中的有索引的列。tag value只能是string类型 如果你的常用场景是根据butterflies和honeybees来查询，那么你可以将这两个列设置为tag，而其他两列设置为field，tag和field依据具体查询需求来定。 measurementmeasurement是fields，tags以及time列的容器，measurement的名字用于描述存储在其中的字段数据，类似mysql的表名。如上面例子中的measurement为census。measurement相当于SQL中的表，本文中我在部分地方会用表来指代measurement。 retention policyretention policy指数据保留策略，示例数据中的retention policy为默认的autogen。它表示数据一直保留永不过期，副本数量为1。你也可以指定数据的保留时间，如30天。 seriesseries是共享同一个retention policy，measurement以及tag set的数据集合。示例中数据有4个series，如下:123456789101112131415161718192021222324Arbitrary series numberRetention policyMeasurementTag setseries 1autogencensuslocation = 1,scientist = langstrothseries 2autogencensuslocation = 2,scientist = langstrothseries 3autogencensuslocation = 1,scientist = perpetuaseries 4autogencensuslocation = 2,scientist = perpetua pointpoint则是同一个series中具有相同时间的field set，points相当于SQL中的数据行。如下面就是一个point：1234name: census-----------------time butterflies honeybees location scientist2015-08-18T00:00:00Z 1 30 1 perpetua database上面提到的结构都存储在数据库中，示例的数据库为my_database。一个数据库可以有多个measurement，retention policy， continuous queries以及user。influxdb是一个无模式的数据库，可以很容易的添加新的measurement，tags，fields等。而它的操作却和传统的数据库一样，可以使用类SQL语言查询和修改数据。influxdb不是一个完整的CRUD数据库，它更像是一个CR-ud数据库。它优先考虑的是增加和读取数据而不是更新和删除数据的性能，而且它阻止了某些更新和删除行为使得创建和读取数据更加高效。 3 特色函数influxdb函数分为聚合函数，选择函数，转换函数，预测函数等。除了与普通数据库一样提供了基本操作函数外，还提供了一些特色函数以方便数据统计计算，下面会一一介绍其中一些常用的特色函数。 聚合函数：FILL(), INTEGRAL()，SPREAD()， STDDEV()，MEAN(), MEDIAN()等。选择函数: SAMPLE(), PERCENTILE(), FIRST(), LAST(), TOP(), BOTTOM()等。转换函数: DERIVATIVE(), DIFFERENCE()等。预测函数：HOLT_WINTERS()。 先从官网导入测试数据（注：这里测试用的版本是1.3.1，最新版本是1.3.5）:123456789101112131415161718192021$ curl https://s3.amazonaws.com/noaa.water-database/NOAA_data.txt -o NOAA_data.txt$ influx -import -path=NOAA_data.txt -precision=s -database=NOAA_water_database$ influx -precision rfc3339 -database NOAA_water_databaseConnected to http://localhost:8086 version 1.3.1InfluxDB shell 1.3.1&gt; show measurementsname: measurementsname----average_temperaturedistinctsh2o_feeth2o_pHh2o_qualityh2o_temperature&gt; show series from h2o_feet;key---h2o_feet,location=coyote_creekh2o_feet,location=santa_monica 下面的例子都以官方示例数据库来测试，这里只用部分数据以方便观察。measurement为h2o_feet，tag key为location，field key有level description和water_level两个。12345678910111213141516&gt; SELECT * FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z'name: h2o_feettime level description location water_level---- ----------------- -------- -----------2015-08-18T00:00:00Z between 6 and 9 feet coyote_creek 8.122015-08-18T00:00:00Z below 3 feet santa_monica 2.0642015-08-18T00:06:00Z between 6 and 9 feet coyote_creek 8.0052015-08-18T00:06:00Z below 3 feet santa_monica 2.1162015-08-18T00:12:00Z between 6 and 9 feet coyote_creek 7.8872015-08-18T00:12:00Z below 3 feet santa_monica 2.0282015-08-18T00:18:00Z between 6 and 9 feet coyote_creek 7.7622015-08-18T00:18:00Z below 3 feet santa_monica 2.1262015-08-18T00:24:00Z between 6 and 9 feet coyote_creek 7.6352015-08-18T00:24:00Z below 3 feet santa_monica 2.0412015-08-18T00:30:00Z between 6 and 9 feet coyote_creek 7.52015-08-18T00:30:00Z below 3 feet santa_monica 2.051 GROUP BY，FILL()如下语句中GROUP BY time(12m),* 表示以每12分钟和tag(location)分组(如果是GROUP BY time(12m)则表示仅每12分钟分组，GROUP BY 参数只能是time和tag)。然后fill(200)表示如果这个时间段没有数据，以200填充，mean(field_key)求该范围内数据的平均值(注意：这是依据series来计算。其他还有SUM求和，MEDIAN求中位数)。LIMIT 7表示限制返回的point(记录数)最多为7条，而SLIMIT 1则是限制返回的series为1个。注意这里的时间区间，起始时间为整点前包含这个区间第一个12m的时间，比如这里为 2015-08-17T:23:48:00Z，第一条为 2015-08-17T23:48:00Z &lt;= t &lt; 2015-08-18T00:00:00Z这个区间的location=coyote_creek的water_level的平均值，这里没有数据，于是填充的200。第二条为 2015-08-18T00:00:00Z &lt;= t &lt; 2015-08-18T00:12:00Z区间的location=coyote_creek的water_level平均值，这里为 （8.12+8.005）/ 2 = 8.0625，其他以此类推。而GROUP BY time(10m)则表示以10分钟分组，起始时间为包含这个区间的第一个10m的时间，即 2015-08-17T23:40:00Z。默认返回的是第一个series，如果要计算另外那个series，可以在SQL语句后面加上 SOFFSET 1。那如果时间小于数据本身采集的时间间隔呢，比如GROUP BY time(10s)呢？这样的话，就会按10s取一个点，没有数值的为空或者FILL填充，对应时间点有数据则保持不变。1234567891011121314151617181920212223## GROUP BY time(12m)&gt; SELECT mean("water_level") FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m),* fill(200) LIMIT 7 SLIMIT 1name: h2o_feettags: location=coyote_creektime mean---- ----2015-08-17T23:48:00Z 2002015-08-18T00:00:00Z 8.06252015-08-18T00:12:00Z 7.82452015-08-18T00:24:00Z 7.5675## GROUP BY time(10m)，SOFFSET设置为1&gt; SELECT mean("water_level") FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(10m),* fill(200) LIMIT 7 SLIMIT 1 SOFFSET 1name: h2o_feettags: location=santa_monicatime mean---- ----2015-08-17T23:40:00Z 2002015-08-17T23:50:00Z 2002015-08-18T00:00:00Z 2.092015-08-18T00:10:00Z 2.0772015-08-18T00:20:00Z 2.0412015-08-18T00:30:00Z 2.051 INTEGRAL(field_key, unit)计算数值字段值覆盖的曲面的面积值并得到面积之和。测试数据如下：1234567891011&gt; SELECT "water_level" FROM "h2o_feet" WHERE "location" = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z'name: h2o_feettime water_level---- -----------2015-08-18T00:00:00Z 2.0642015-08-18T00:06:00Z 2.1162015-08-18T00:12:00Z 2.0282015-08-18T00:18:00Z 2.1262015-08-18T00:24:00Z 2.0412015-08-18T00:30:00Z 2.051 使用INTERGRAL计算面积。注意，这个面积就是这些点连接起来后与时间围成的不规则图形的面积，注意unit默认是以1秒计算，所以下面语句计算结果为3732.66=2.028*1800+分割出来的梯形和三角形面积。如果unit改为1分，则结果为3732.66/60 = 62.211。unit为2分，则结果为3732.66/120 = 31.1055。以此类推。12345678910111213# unit为默认的1秒&gt; SELECT INTEGRAL("water_level") FROM "h2o_feet" WHERE "location" = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z'name: h2o_feettime integral---- --------1970-01-01T00:00:00Z 3732.66# unit为1分&gt; SELECT INTEGRAL("water_level", 1m) FROM "h2o_feet" WHERE "location" = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z'name: h2o_feettime integral---- --------1970-01-01T00:00:00Z 62.211 SPREAD(field_key)计算数值字段的最大值和最小值的差值。12345678&gt; SELECT SPREAD("water_level") FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m),* fill(18) LIMIT 3 SLIMIT 1 SOFFSET 1name: h2o_feettags: location=santa_monicatime spread---- ------2015-08-17T23:48:00Z 182015-08-18T00:00:00Z 0.0520000000000000462015-08-18T00:12:00Z 0.09799999999999986 STDDEV(field_key)计算字段的标准差。influxdb用的是贝塞尔修正的标准差计算公式 ，如下：1234567891011121314mean=(v1+v2+...+vn)/n;stddev = math.sqrt(((v1-mean)2 + (v2-mean)2 + ...+(vn-mean)2)/(n-1))&gt; SELECT STDDEV("water_level") FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m),* fill(18) SLIMIT 1;name: h2o_feettags: location=coyote_creektime stddev---- ------2015-08-17T23:48:00Z 182015-08-18T00:00:00Z 0.081317279836451862015-08-18T00:12:00Z 0.088388347648318452015-08-18T00:24:00Z 0.09545941546018377 PERCENTILE(field_key, N)选取某个字段中大于N%的这个字段值。如果一共有4条记录，N为10，则10%4=0.4，四舍五入为0，则查询结果为空。N为20，则 20% 4 = 0.8，四舍五入为1，选取的是4个数中最小的数。如果N为40，40% * 4 = 1.6，四舍五入为2，则选取的是4个数中第二小的数。由此可以看出N=100时，就跟MAX(field_key)是一样的，而当N=50时，与MEDIAN(field_key)在字段值为奇数个时是一样的。1234567891011121314151617&gt; SELECT PERCENTILE("water_level",20) FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m)name: h2o_feettime percentile---- ----------2015-08-17T23:48:00Z 2015-08-18T00:00:00Z 2.0642015-08-18T00:12:00Z 2.0282015-08-18T00:24:00Z 2.041&gt; SELECT PERCENTILE("water_level",40) FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m)name: h2o_feettime percentile---- ----------2015-08-17T23:48:00Z 2015-08-18T00:00:00Z 2.1162015-08-18T00:12:00Z 2.1262015-08-18T00:24:00Z 2.051 SAMPLE(field_key, N)随机返回field key的N个值。如果语句中有GROUP BY time()，则每组数据随机返回N个值。1234567891011121314151617&gt; SELECT SAMPLE("water_level",2) FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z';name: h2o_feettime sample---- ------2015-08-18T00:00:00Z 2.0642015-08-18T00:12:00Z 2.028&gt; SELECT SAMPLE("water_level",2) FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z' GROUP BY time(12m);name: h2o_feettime sample---- ------2015-08-18T00:06:00Z 2.1162015-08-18T00:06:00Z 8.0052015-08-18T00:12:00Z 7.8872015-08-18T00:18:00Z 7.7622015-08-18T00:24:00Z 7.6352015-08-18T00:30:00Z 2.051 CUMULATIVE_SUM(field_key)计算字段值的递增和。12345678910111213141516&gt; SELECT CUMULATIVE_SUM("water_level") FROM "h2o_feet" WHERE time &gt;= '2015-08-17T23:48:00Z' AND time &lt;= '2015-08-18T00:30:00Z';name: h2o_feettime cumulative_sum---- --------------2015-08-18T00:00:00Z 8.122015-08-18T00:00:00Z 10.1842015-08-18T00:06:00Z 18.1892015-08-18T00:06:00Z 20.3052015-08-18T00:12:00Z 28.1922015-08-18T00:12:00Z 30.222015-08-18T00:18:00Z 37.9822015-08-18T00:18:00Z 40.1082015-08-18T00:24:00Z 47.7429999999999952015-08-18T00:24:00Z 49.783999999999992015-08-18T00:30:00Z 57.283999999999992015-08-18T00:30:00Z 59.334999999999994 DERIVATIVE(field_key, unit) 和 NON_NEGATIVE_DERIVATIVE(field_key, unit)计算字段值的变化比。unit默认为1s，即计算的是1秒内的变化比。如下面的第一个数据计算方法是 (2.116-2.064)/(6*60) = 0.00014..，其他计算方式同理。虽然原始数据是6m收集一次，但是这里的变化比默认是按秒来计算的。如果要按6m计算，则设置unit为6m即可。12345678910111213141516171819&gt; SELECT DERIVATIVE("water_level") FROM "h2o_feet" WHERE "location" = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z'name: h2o_feettime derivative---- ----------2015-08-18T00:06:00Z 0.000144444444444444572015-08-18T00:12:00Z -0.000244444444444444652015-08-18T00:18:00Z 0.00027222222222222182015-08-18T00:24:00Z -0.0002361111111111112015-08-18T00:30:00Z 0.00002777777777777842&gt; SELECT DERIVATIVE("water_level", 6m) FROM "h2o_feet" WHERE "location" = 'santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z'name: h2o_feettime derivative---- ----------2015-08-18T00:06:00Z 0.0520000000000000462015-08-18T00:12:00Z -0.088000000000000082015-08-18T00:18:00Z 0.097999999999999862015-08-18T00:24:00Z -0.084999999999999962015-08-18T00:30:00Z 0.010000000000000231 而DERIVATIVE结合GROUP BY time，以及mean可以构造更加复杂的查询，如下所示:1234567891011121314&gt; SELECT DERIVATIVE(mean("water_level"), 6m) FROM "h2o_feet" WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z' group by time(12m), *name: h2o_feettags: location=coyote_creektime derivative---- ----------2015-08-18T00:12:00Z -0.119000000000000222015-08-18T00:24:00Z -0.12849999999999984name: h2o_feettags: location=santa_monicatime derivative---- ----------2015-08-18T00:12:00Z -0.006499999999999952015-08-18T00:24:00Z -0.015499999999999847 这个计算其实是先根据GROUP BY time求平均值，然后对这个平均值再做变化比的计算。因为数据是按12分钟分组的，而变化比的unit是6分钟，所以差值除以2(12/6)才得到变化比。如第一个值是 (7.8245-8.0625)/2 = -0.1190。12345678910111213141516&gt; SELECT mean("water_level") FROM "h2o_feet" WHERE time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z' group by time(12m), *name: h2o_feettags: location=coyote_creektime mean---- ----2015-08-18T00:00:00Z 8.06252015-08-18T00:12:00Z 7.82452015-08-18T00:24:00Z 7.5675name: h2o_feettags: location=santa_monicatime mean---- ----2015-08-18T00:00:00Z 2.092015-08-18T00:12:00Z 2.0772015-08-18T00:24:00Z 2.0460000000000003 NON_NEGATIVE_DERIVATIVE与DERIVATIVE不同的是它只返回的是非负的变化比:12345678910111213141516171819&gt; SELECT DERIVATIVE(mean("water_level"), 6m) FROM "h2o_feet" WHERE location='santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z' group by time(6m), *name: h2o_feettags: location=santa_monicatime derivative---- ----------2015-08-18T00:06:00Z 0.0520000000000000462015-08-18T00:12:00Z -0.088000000000000082015-08-18T00:18:00Z 0.097999999999999862015-08-18T00:24:00Z -0.084999999999999962015-08-18T00:30:00Z 0.010000000000000231&gt; SELECT NON_NEGATIVE_DERIVATIVE(mean("water_level"), 6m) FROM "h2o_feet" WHERE location='santa_monica' AND time &gt;= '2015-08-18T00:00:00Z' AND time &lt;= '2015-08-18T00:30:00Z' group by time(6m), *name: h2o_feettags: location=santa_monicatime non_negative_derivative---- -----------------------2015-08-18T00:06:00Z 0.0520000000000000462015-08-18T00:18:00Z 0.097999999999999862015-08-18T00:30:00Z 0.010000000000000231 4 连续查询4.1 基本语法连续查询(CONTINUOUS QUERY，简写为CQ)是指定时自动在实时数据上进行的InfluxQL查询，查询结果可以存储到指定的measurement中。基本语法格式如下：1234567CREATE CONTINUOUS QUERY &lt;cq_name&gt; ON &lt;database_name&gt;BEGIN &lt;cq_query&gt;ENDcq_query格式：SELECT &lt;function[s]&gt; INTO &lt;destination_measurement&gt; FROM &lt;measurement&gt; [WHERE &lt;stuff&gt;] GROUP BY time(&lt;interval&gt;)[,&lt;tag_key[s]&gt;] CQ操作的是实时数据，它使用本地服务器的时间戳、GROUP BY time()时间间隔以及InfluxDB预先设置好的时间范围来确定什么时候开始查询以及查询覆盖的时间范围。注意CQ语句里面的WHERE条件是没有时间范围的，因为CQ会根据GROUP BY time()自动确定时间范围。CQ执行的时间间隔和GROUP BY time()的时间间隔一样，它在InfluxDB预先设置的时间范围的起始时刻执行。如果GROUP BY time(1h)，则单次查询的时间范围为 now()-GROUP BY time(1h)到 now()，也就是说，如果当前时间为17点，这次查询的时间范围为 16:00到16:59.99999。下面看几个示例，示例数据如下，这是数据库transportation中名为bus_data的measurement，每15分钟统计一次乘客数和投诉数。数据文件bus_data.txt如下：123456789101112131415# DDLCREATE DATABASE transportation# DML# CONTEXT-DATABASE: transportation bus_data,complaints=9 passengers=5 1472367600bus_data,complaints=9 passengers=8 1472368500bus_data,complaints=9 passengers=8 1472369400bus_data,complaints=9 passengers=7 1472370300bus_data,complaints=9 passengers=8 1472371200bus_data,complaints=7 passengers=15 1472372100bus_data,complaints=7 passengers=15 1472373000bus_data,complaints=7 passengers=17 1472373900bus_data,complaints=7 passengers=20 1472374800 导入数据，命令如下：1234567891011121314151617root@f216e9be15bf:/# influx -import -path=bus_data.txt -precision=sroot@f216e9be15bf:/# influx -precision=rfc3339 -database=transportationConnected to http://localhost:8086 version 1.3.5InfluxDB shell version: 1.3.5&gt; select * from bus_dataname: bus_datatime complaints passengers---- ---------- ----------2016-08-28T07:00:00Z 9 52016-08-28T07:15:00Z 9 82016-08-28T07:30:00Z 9 82016-08-28T07:45:00Z 9 72016-08-28T08:00:00Z 9 82016-08-28T08:15:00Z 7 152016-08-28T08:30:00Z 7 152016-08-28T08:45:00Z 7 172016-08-28T09:00:00Z 7 20 示例1 自动缩小取样存储到新的measurement中对单个字段自动缩小取样并存储到新的measurement中。1234CREATE CONTINUOUS QUERY "cq_basic" ON "transportation"BEGIN SELECT mean("passengers") INTO "average_passengers" FROM "bus_data" GROUP BY time(1h)END 这个CQ的意思就是对bus_data每小时自动计算取样数据的平均乘客数并存储到 average_passengers中。那么在2016-08-28这天早上会执行如下流程：At 8:00 cq_basic 执行查询，查询时间范围 time &gt;= ‘7:00’ AND time &lt; ‘08:00’.cq_basic写入一条记录到 average_passengers:1234name: average_passengers------------------------time mean2016-08-28T07:00:00Z 7 At 9:00 cq_basic 执行查询，查询时间范围 time &gt;= ‘8:00’ AND time &lt; ‘9:00’.cq_basic写入一条记录到 average_passengers:123456789101112name: average_passengers------------------------time mean2016-08-28T08:00:00Z 13.75# Results&gt; SELECT * FROM "average_passengers"name: average_passengers------------------------time mean2016-08-28T07:00:00Z 72016-08-28T08:00:00Z 13.75 示例2 自动缩小取样并存储到新的保留策略（Retention Policy）中1234CREATE CONTINUOUS QUERY "cq_basic_rp" ON "transportation"BEGIN SELECT mean("passengers") INTO "transportation"."three_weeks"."average_passengers" FROM "bus_data" GROUP BY time(1h)END 与示例1类似，不同的是保留的策略不是autogen，而是改成了three_weeks(创建保留策略语法 CREATE RETENTION POLICY “three_weeks” ON “transportation” DURATION 3w REPLICATION 1)。123456&gt; SELECT * FROM "transportation"."three_weeks"."average_passengers"name: average_passengers------------------------time mean2016-08-28T07:00:00Z 72016-08-28T08:00:00Z 13.75 示例3 使用后向引用(backreferencing)自动缩小取样并存储到新的数据库中1234CREATE CONTINUOUS QUERY "cq_basic_br" ON "transportation"BEGIN SELECT mean(*) INTO "downsampled_transportation"."autogen".:MEASUREMENT FROM /.*/ GROUP BY time(30m),*END 使用后向引用语法自动缩小取样并存储到新的数据库中。语法 :MEASUREMENT 用来指代后面的表，而 /.*/则是分别查询所有的表。这句CQ的含义就是每30分钟自动查询transportation的所有表(这里只有bus_data一个表)，并将30分钟内数字字段(passengers和complaints)求平均值存储到新的数据库 downsampled_transportation中。最终结果如下：12345678&gt; SELECT * FROM "downsampled_transportation."autogen"."bus_data"name: bus_data--------------time mean_complaints mean_passengers2016-08-28T07:00:00Z 9 6.52016-08-28T07:30:00Z 9 7.52016-08-28T08:00:00Z 8 11.52016-08-28T08:30:00Z 7 16 示例4 自动缩小取样以及配置CQ的时间范围1234CREATE CONTINUOUS QUERY "cq_basic_offset" ON "transportation"BEGIN SELECT mean("passengers") INTO "average_passengers" FROM "bus_data" GROUP BY time(1h,15m)END 与前面几个示例不同的是，这里的GROUP BY time(1h, 15m)指定了一个时间偏移，也就是说 cq_basic_offset执行的时间不再是整点，而是往后偏移15分钟。执行流程如下:At 8:15 cq_basic_offset 执行查询的时间范围 time &gt;= ‘7:15’ AND time &lt; ‘8:15’.1234name: average_passengers------------------------time mean2016-08-28T07:15:00Z 7.75 At 9:15 cq_basic_offset 执行查询的时间范围 time &gt;= ‘8:15’ AND time &lt; ‘9:15’.1234name: average_passengers------------------------time mean2016-08-28T08:15:00Z 16.75 最终结果:123456&gt; SELECT * FROM "average_passengers"name: average_passengers------------------------time mean2016-08-28T07:15:00Z 7.752016-08-28T08:15:00Z 16.75 4.2 高级语法InfluxDB连续查询的高级语法如下：12345CREATE CONTINUOUS QUERY &lt;cq_name&gt; ON &lt;database_name&gt;RESAMPLE EVERY &lt;interval&gt; FOR &lt;interval&gt;BEGIN &lt;cq_query&gt;END 与基本语法不同的是，多了RESAMPLE关键字。高级语法里CQ的执行时间和查询时间范围则与RESAMPLE里面的两个interval有关系。高级语法中CQ以EVERY interval的时间间隔执行，执行时查询的时间范围则是FOR interval来确定。如果FOR interval为2h，当前时间为17:00，则查询的时间范围为15:00-16:59.999999。RESAMPLE的EVERY和FOR两个关键字可以只有一个。示例的数据表如下，比之前的多了几条记录为了示例3和示例4的测试:1234567891011121314name: bus_data--------------time passengers2016-08-28T06:30:00Z 22016-08-28T06:45:00Z 42016-08-28T07:00:00Z 52016-08-28T07:15:00Z 82016-08-28T07:30:00Z 82016-08-28T07:45:00Z 72016-08-28T08:00:00Z 82016-08-28T08:15:00Z 152016-08-28T08:30:00Z 152016-08-28T08:45:00Z 172016-08-28T09:00:00Z 20 示例1 只配置执行时间间隔12345CREATE CONTINUOUS QUERY "cq_advanced_every" ON "transportation"RESAMPLE EVERY 30mBEGIN SELECT mean("passengers") INTO "average_passengers" FROM "bus_data" GROUP BY time(1h)END 这里配置了30分钟执行一次CQ，没有指定FOR interval，于是查询的时间范围还是GROUP BY time(1h)指定的一个小时，执行流程如下：At 8:00, cq_advanced_every 执行时间范围 time &gt;= ‘7:00’ AND time &lt; ‘8:00’.1234name: average_passengers------------------------time mean2016-08-28T07:00:00Z 7 At 8:30, cq_advanced_every 执行时间范围 time &gt;= ‘8:00’ AND time &lt; ‘9:00’.1234name: average_passengers------------------------time mean2016-08-28T08:00:00Z 12.6667 At 9:00, cq_advanced_every 执行时间范围 time &gt;= ‘8:00’ AND time &lt; ‘9:00’.1234name: average_passengers------------------------time mean2016-08-28T08:00:00Z 13.75 需要注意的是，这里的 8点到9点这个区间执行了两次，第一次执行时时8:30，平均值是 (8+15+15）/ 3 = 12.6667，而第二次执行时间是9:00，平均值是 (8+15+15+17) / 4=13.75，而且最后第二个结果覆盖了第一个结果。InfluxDB如何处理重复的记录可以参见这个文档。最终结果：123456&gt; SELECT * FROM "average_passengers"name: average_passengers------------------------time mean2016-08-28T07:00:00Z 72016-08-28T08:00:00Z 13.75 示例2 只配置查询时间范围12345CREATE CONTINUOUS QUERY "cq_advanced_for" ON "transportation"RESAMPLE FOR 1hBEGIN SELECT mean("passengers") INTO "average_passengers" FROM "bus_data" GROUP BY time(30m)END 只配置了时间范围，而没有配置EVERY interval。这样，执行的时间间隔与GROUP BY time(30m)一样为30分钟，而查询的时间范围为1小时，由于是按30分钟分组，所以每次会写入两条记录。执行流程如下：At 8:00 cq_advanced_for 查询时间范围：time &gt;= ‘7:00’ AND time &lt; ‘8:00’.写入两条记录。12345name: average_passengers------------------------time mean2016-08-28T07:00:00Z 6.52016-08-28T07:30:00Z 7.5 At 8:30 cq_advanced_for 查询时间范围：time &gt;= ‘7:30’ AND time &lt; ‘8:30’.写入两条记录。12345name: average_passengers------------------------time mean2016-08-28T07:30:00Z 7.52016-08-28T08:00:00Z 11.5 At 9:00 cq_advanced_for 查询时间范围：time &gt;= ‘8:00’ AND time &lt; ‘9:00’.写入两条记录。12345name: average_passengers------------------------time mean2016-08-28T08:00:00Z 11.52016-08-28T08:30:00Z 16 需要注意的是，cq_advanced_for每次写入了两条记录，重复的记录会被覆盖。最终结果：12345678&gt; SELECT * FROM "average_passengers"name: average_passengers------------------------time mean2016-08-28T07:00:00Z 6.52016-08-28T07:30:00Z 7.52016-08-28T08:00:00Z 11.52016-08-28T08:30:00Z 16 示例3 同时配置执行时间间隔和查询时间范围12345CREATE CONTINUOUS QUERY "cq_advanced_every_for" ON "transportation"RESAMPLE EVERY 1h FOR 90mBEGIN SELECT mean("passengers") INTO "average_passengers" FROM "bus_data" GROUP BY time(30m)END 这里配置了执行间隔为1小时，而查询范围90分钟，最后分组是30分钟，每次插入了三条记录。执行流程如下：At 8:00 cq_advanced_every_for 查询时间范围 time &gt;= ‘6:30’ AND time &lt; ‘8:00’.插入三条记录123456name: average_passengers------------------------time mean2016-08-28T06:30:00Z 32016-08-28T07:00:00Z 6.52016-08-28T07:30:00Z 7.5 At 9:00 cq_advanced_every_for 查询时间范围 time &gt;= ‘7:30’ AND time &lt; ‘9:00’.插入三条记录123456name: average_passengers------------------------time mean2016-08-28T07:30:00Z 7.52016-08-28T08:00:00Z 11.52016-08-28T08:30:00Z 16 最终结果：123456789&gt; SELECT * FROM "average_passengers"name: average_passengers------------------------time mean2016-08-28T06:30:00Z 32016-08-28T07:00:00Z 6.52016-08-28T07:30:00Z 7.52016-08-28T08:00:00Z 11.52016-08-28T08:30:00Z 16 示例4 配置查询时间范围和FILL填充12345CREATE CONTINUOUS QUERY "cq_advanced_for_fill" ON "transportation"RESAMPLE FOR 2hBEGIN SELECT mean("passengers") INTO "average_passengers" FROM "bus_data" GROUP BY time(1h) fill(1000)END 在前面值配置查询时间范围的基础上，加上FILL填充空的记录。执行流程如下：At 6:00, cq_advanced_for_fill 查询时间范围：time &gt;= ‘4:00’ AND time &lt; ‘6:00’，没有数据，不填充。 At 7:00, cq_advanced_for_fill 查询时间范围：time &gt;= ‘5:00’ AND time &lt; ‘7:00’. 写入两条记录，没有数据的时间点填充1000。1234------------------------time mean2016-08-28T05:00:00Z 1000 &lt;------ fill(1000)2016-08-28T06:00:00Z 3 &lt;------ average of 2 and 4 […] At 11:00, cq_advanced_for_fill 查询时间范围：time &gt;= ‘9:00’ AND time &lt; ‘11:00’.写入两条记录，没有数据的点填充1000。1234name: average_passengers------------------------2016-08-28T09:00:00Z 20 &lt;------ average of 202016-08-28T10:00:00Z 1000 &lt;------ fill(1000) At 12:00, cq_advanced_for_fill 查询时间范围：time &gt;= ‘10:00’ AND time &lt; ‘12:00’。没有数据，不填充。最终结果:12345678910&gt; SELECT * FROM "average_passengers"name: average_passengers------------------------time mean2016-08-28T05:00:00Z 10002016-08-28T06:00:00Z 32016-08-28T07:00:00Z 72016-08-28T08:00:00Z 13.752016-08-28T09:00:00Z 202016-08-28T10:00:00Z 1000 5 参考资料https://docs.influxdata.com/influxdb/v1.3/https://zh.wikipedia.org/wiki/%E6%A8%99%E6%BA%96%E5%B7%AE 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>influxdb</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>TSDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[瓜子云平台的实践经验]]></title>
    <url>%2F2019%2F04%2F22%2Fguazi-practice%2F</url>
    <content type="text"><![CDATA[私有云平台的建设和公司在不同阶段的需求是息息相关的，瓜子云平台从 2017 年启动项目，到目前承载了公司上千个应用服务，每月服务发布次数达上万次。在公司业务爆发性增长的背景下，云平台团队从 0 到 1 的完成了平台搭建，初步实现了平台产品化，也总结出了一些云平台建设的实践和经验。 这篇文章和大家分享下瓜子云平台的一些实践经验。瓜子是在 2017 年年中启动云平台项目的，当时有如下背景：技术栈多样化，PHP、Java、Go、Python 都有使用，但只有 PHP 建立了相对统一的部署流程。业务迭代速度快，人员扩张速度快，再加上微服务化改造，项目数量激增，基于虚拟机的运维压力很大。测试环境没有统一管理，业务开发人员自行零散维护。 基于此，我们的 0.x 版本，也是原型版本选择了如下的切入点：在 CI/CD 层面，先定义出标准流程，但是并不涉及细节的规范化，便于用户学习，快速将现有流程套进去。同时支持 image 和 tar 包两种产出，为云上部署和虚拟机部署做好构建路径的兼容，在将来迁移时这部分可以做到几乎无缝。先支持测试环境的部署，在验证平台稳定性的同时，不断收集用户需求进行快速迭代。 集群核心组件的技术选型在服务编排和资源管理方面，Docker 和 Kubernetes 已经比较成熟了，基于容器的微服务化也是大势，再加上我们对强隔离没有诉求，所以跳过了 OpenStack，直接选择了 Kubernetes 方案。既然用了 Kubernetes 肯定就要解决跨节点的容器网络通信问题，因为我们是自建机房，没有公有云在网络层面的限制，所以没有考虑应用范围更广但是性能和可调试性较差的 VXLAN 方案。最开始选择的是 Macvlan + 自研 IPAM 的方式，之后转向了方案完成度更高的基于 BGP 协议的 Project Calico。 Calico 的优点如下：基于 BGP 协议，转发平面依靠主机路由表，不涉及任何封包解包操作，性能非常接近原生网卡，并且方便抓包调试。组件结构简单，对 Kubernetes 支持很好。可以和 IDC 路由器通过 BGP 协议打通，直接对外广播容器 IP，让集群内外可以通过 IP 直连。有 ACL 能力，类似 AWS 的 Security Group。 当然肯定有缺点：节点间网络最好是二层可达，否则只能使用 IP-in-IP 这种隧道技术，那就失去大半意义了。因为是基于三层转发的，无法做到二层隔离，安全诉求高的场景不适用。严重依赖 iptables，海量并发情况下 netfilter 本身可能成为瓶颈。 编排和网络解决后，集群就可以拉起来，接下来需要考虑的是流量如何进入，也就是负载均衡器的选型。在不想过多自己开发的前提下，使用 Kubernetes Ingress 是一个比较好的选择，但是不同 Ingress 之间功能和成熟度差异都很大，我们也是尝试了好几种，从 Nginx 到 Linkerd，最终选择了 Istio。我们在 0.2 版本开始用 Istio，初期它的不稳定带来了很多困扰，不过在 1.x 版本发布后相对稳定很多了。选择 Istio 的另一个原因是为将来向 Service Mesh 方向演进积累经验。 以上一个最小功能可用集群就基本完备了，接下来在 CI/CD 这块，我们封装了一个命令行工具 med，它通过读取项目中的 med.yml，自动生成 dockerfile 和 deployment.yml，之后调用 Docker 进行构建，并提交给 Kubernetes 集群进行部署。一个 med.yml 例子如下： 从上面这个例子能看到，我们将构建环节拆分成了 prepare 和 build 两个部分，这个设计其实来源于 dockerfile 的一个最佳实践：由于拉取依赖的环节往往较慢，所以我们习惯将依赖安装和编译过程放到不同的命令中，形成不同的 layer，便于下次复用，提高编译速度。prepare 阶段就是把这部分耗时但是变更不频繁的操作提出来，通过 version 字段控制，只要 version 值不变，prepare 部分就不会再重复执行，而是直接利用之前已经生成好的 prepare 镜像进行接下来的 build 环节。build 环节构建结束后将 image 提交到镜像仓库，等待用户调用 deploy 命令进行部署。在实际开发过程中用户提出同一个代码仓库构建多个产出的需求，比如一个 Java 项目同时构建出用于发布的 Web Service 和提交到 Maven 的 jar 包。所以 build 和 deploy 阶段都支持多个产出，通过 name 进行区分，参数方面则支持有限的模板变量替换。 最后，我们将 med.yml 和 GitLab CI 结合，一个 CI/CD 流程就出来了，用户提交代码后自动触发构建，上传镜像并部署到他的测试环境。 云平台产品化的一些思考云平台 1.x在测试环境跑了一段时间后，大家纷纷尝到了甜头，希望不只是在测试环境使用，生产环境也可以部署。这时简易的客户端工具就不满足需求了，新增的主要诉求围绕着权限管理和灰度发布，同时也希望有一个更加可视化的平台能看到实例运行的状态，上线的进度和监控日志等。需求细化如下：能直观看到当前项目的运行状态，部署状态。项目有权限控制。发布系统支持预发布和灰度部署。日志收集和监控报警能力。配置管理能力。定时任务支持。 于是经过几轮迭代后，可视化的控制台上线了： 这里可以详细展开下部署环节，为了满足应用上线时的小流量测试需求，最开始是通过用户换个名字手工部署一个新版本，然后灵活调整不同版本部署之间的流量百分比来实现。但是在运行一段时间后，发现太高的灵活度调整非常容易出错，比如忘记了一个小流量部署，导致线上不正常，还很难 debug，并且整个操作过程也很繁琐。于是我们收敛了功能，实现了流程发布功能： 无论是流量调整还是流程发布，Kubernetes 默认的滚动更新都是无法满足需求的。我们是通过先创建一组新版本实例，然后再变更 Istio 的流量切换规则来实现的，示意图如下： 同时为了既有项目平滑上云，我们还提供了容器和虚拟机部署的联合部署，支持一个项目在一个发布流程中同时部署到云平台和虚拟机上。再配合外部 Nginx 权重的跳转，就可以实现业务逐步将流量切换到云上，最终完全去掉虚拟机。这里有一个小插曲，由于基于 TCP 的 Dubbo 服务流量不经过 Istio 网关，而是通过注册到 ZooKeeper 里面的 IP 来进行流量调整的，上面的流程发布和联合部署对 Dubbo 服务没有意义。我们在这个版本进行了一个网络调整，将集群内部的 BGP 路由和外部打通，允许容器和虚拟机直接通信。由于 Calico 更换网段是要重新创建所有容器的，所以我们选择拉起一个新集群，将所有应用全部迁移过去，切换流量入口，再下掉旧集群。这里就体现了容器的便捷性了，数百个应用的迁移只花了十几分钟就再另一个集群完全拉起，业务几乎没有感知。 配置管理方面，最开始是通过 env 管理，后来很多应用不太方便改成读取 env 的方式，又增加了基于 ConfigMap 和配置中心的配置文件注入能力。 日志收集方面，最开始使用 ELK，只收集了容器的 stdout 和 stderr 输出，后来对于放在指定位置的文件日志也纳入了收集目标中。由于 Logstash 实在太耗资源，我们直接使用 ES 的 ingest 能力进行日志格式化，去掉了中间的 Kafka 和 Logstash，从 Filebeat 直接输出到 ES。当然 ingest pipeline 本身调试起来比较复杂，如果有较多的日志二次处理需求，不建议使用。日志展示方面，Kibana 原生的日志搜索能力已经比较强大，不过很多人还是喜欢类似 tail -f 的那种查看方法，我们使用了 Kibana 的第三方插件 Logtrail 进行了模拟，提供了一键从控制台跳转到对应容器日志查看界面的能力。监控收集和展示，也是标准的 Prometheus + Grafana，在收集 Kubernetes 暴露出来的性能指标的同时，也允许用户配置自定义监控的 metric url，应用上线后会自动抓取。报警方面，因为 Prometheus 自带的 Alert Manager 规则配置门槛比较高，所以我们开发了一个用于规则配置的项目 NieR，将常用规则由运维同学写好后模板化，然后再提供给用户订阅，当然用户也可以自行建立自己的模板。监控报警系统展开说能再写一篇文章了，这里就只放一下架构图： 云平台 2.x在 1.x 版本迭代的时候，我们发现，早期为了给用户最大灵活性的 med.yml 在用户量持续增长后带来的培训、运维成本越来越高。每一个第一次接触平台的同事都要花费半天的时间阅读文档，然后在后续的使用中还有很多文档没有描述清楚的地方要搞明白，变相提高了项目上云的门槛。另外这种侵入用户代码仓库的方式，每次调整代价都非常大，服务端控制力度也太弱。 针对上述问题，我们在 2.x 版本彻底去掉了 med.yml，实现了全部 UI 化。这里并不是说把之前的配置文件丢到一个管理页面上就算搞定了。拿构建来说，用户希望的是每种语言有一个标准的构建流程，只需要稍微修改下构建命令就可以直接使用，于是我们定义了语言模板： 然后替用户填好了大部分可以规范化的选项，当然也允许用户自行编辑： 在部署层面，除了和构建产出联动外，最大的变动是参数合理化布局，让新用户基本不用看文档就能明白各个参数的用途。 2.x 版本才刚刚起步，后续还有非常多在排期安排的事情。比如在功能方面：支持多集群部署之后如何做到跨集群调度。如何方便的能让用户快速拉起一套测试环境，乃至于构建自己的内部应用市场。监控系统能不能进一步抽象，直接通过 UI 的方式配置监控模板，能不能自动建议用户合理的监控阈值。给出各个业务的资源利用率和账单。 在基础设施层面：能不能做到不超售但是还能达成合理的资源利用率。离线计算能不能在低峰期复用在线集群资源，但是不能影响业务。ServiceMesh 的进一步研究推广。有状态服务的支持等等。 等等 以上就是瓜子云平台的整体迭代路径。在整个开发过程中，我们感触最深的一点是，需要始终以产品化而不是做工具的思想去设计和实现。技术是为了需求服务的，而不是反过来，把一个用户最痛的痛点打透比做一百个酷炫的功能有用的多。但打透说起来容易，做起来有很多脏活累活。 1.首先在需求分析阶段，基础设施的变更影响非常广泛，在征求大部分人意见的同时，如何引导大家往业界内更先进的路线上演进是要经过深思熟虑的。另外不同阶段的需求也不尽相同，不要一窝蜂的追随技术潮流，适合当前阶段的才是最好的技术选型。2.其次切入点最好是选择共识基础好，影响范围大的需求点，阻力小，成果明显。待做出成果后再一步步扩展到分歧比较严重的深水区。3.最后落地的时候要做好技术运营，做好上线前的宣传培训，帮助用户从旧系统尽量无痛的进行迁移。上线后的持续跟踪，通过数据化的手段，比如前端埋点，核心指标报表等手段观察用户的使用情况，不断调整策略。 上面这些在团队小的时候可能都不是问题，一个沟通群里直接就能聊清楚。但当团队变大后，核心功能上一个不当的设计往往带来的就是上千工时的白白消耗甚至造成线上事故，一个云平台产品能不能落地，技术架构和实现是一方面，上面这些产品和运营策略是否运用得当也是非常重要的。 Q&amp;AQ：请问瓜子私有云是一朵独立的云还是多云部署？如果是多云部署，云间网络是采用的什么技术？如何打通多云之间的网络的？A：我们在设计之初就考虑多集群 / 多 IDC 部署的，这也是选择 Calico 的一个原因。通过 BGP 协议将容器 IP 广播出去后，云间互联和普通虚拟机互联没有区别，当然这需要网络设备支持。 Q：新版本实例发布的时候怎么切Istio才能保障灰度的流量不丢失呢？A：在流程发布里面，我们相当于先新建一组新的实例，在它们的 Readiness 检查通过后，切换 Istio 规则指向新实例做到的。 Q：稳定性方面有没有出现过比较大的问题，怎么解决的？A：有两个时期稳定性故障较多，一个是 Istio 版本比较低的时候，这个只能说一路趟坑趟过来，我们甚至自己改过 Istio 代码，现在的版本已经没出过问题了；一个是集群用的太狠，当集群接近满载时，Kubernetes 会出现很多连锁问题，这个主要是靠监控来做及时扩容。 Q：自建机房的话为什么不接着使用 Macvlan + IPAM 方案呢？是为了之后上公有云做准备吗？A：当时面临一个本机 Macvlan 容器互相不通的问题，再加上有熟悉的团队已经在生产跑了很久 Calico 了，所以就直接换到了 Calico。 Q：请问几秒的时延对一些高效的服务来讲也是不可接受的。咱们有想过通过何种方式去避免灰度的流量损失问题么？A：还真没遇到过这个需求。我理解如果有一个服务如此关键，那么在整个流量变更环节（从机房入口开始）到灰度策略上都得仔细考虑。如果接受不了 Istio 这个延时，一个思路是完全抛弃 Istio Ingress，直接使用一个切换迅速的负载均衡器。因为容器 IP 是直通的，完全可以从集群外直接连进来，只要解决服务发现的问题就行。 Q：应用服务追踪怎么处理的？对接Istio？A：语言栈比较多的情况下这是个痛点，目前我们也是在尝试，即使是 Sidecar 也不能完全对业务没侵入。公司内部 Java 技术栈更喜欢 Skywalking 这种完全无侵入的方式。 Q：使用 Istio 时，怎么解决性能损坏问题的？A：目前还没有启用 Mixer 这些对性能影响比较大的组件，所以目前性能损耗还是比较小的。如果对性能有严格的要求，我们会建议他使用 service name 做直连。 Q：为什么 Kubernetes 默认的滚动更新不能满足要求？哪里有问题？A：没办法精细控制灰度粒度，比如部署了 4 个实例，我要求切 10% 的流量灰度，这就做不到了。另外，滚动更新回滚也比较慢。 Q：目前支撑的生产应用服务规模和云平台的规模能介绍下？包括一些指标，比如多少应用进行灰度更新耗时？A：应用规模的话目前超过 1000 了，每个月发布次数超过 10000。灰度更新是用户自行控制整个发布进度的，所以耗时不太有参考意义。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>实践</category>
      </categories>
      <tags>
        <tag>云平台，实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InfluxDB原理详解]]></title>
    <url>%2F2019%2F04%2F22%2Finfluxdb-principle%2F</url>
    <content type="text"><![CDATA[InfluxDB特点可以设置metric的保存时间。支持通过条件过滤以及正则表达式删除数据。支持类似 sql 的语法。可以设置数据在集群中的副本数。支持定期采样数据，写入另外的measurement，方便分粒度存储数据。 InfluxDB概念1）数据格式 Line Protocol在 InfluxDB 中，我们可以粗略的将要存入的一条数据看作一个虚拟的 key 和其对应的 value(field value)，格式如下：1234567891011121314151617181920212223242526cpu_usage,host=server01,region=us-west value=0.64 1434055562000000000``虚拟的key包括以下几个部分：**database, retention policy, measurement, tag sets, field name, timestamp。database 和 retention policy** 在上面的数据中并没有体现，通常在插入数据时在 http 请求的相应字段中指定。**database:** 数据库名，在 InfluxDB 中可以创建多个数据库，不同数据库中的数据文件是隔离存放的，存放在磁盘上的不同目录。**retention policy:** 存储策略，用于设置数据保留的时间，每个数据库刚开始会自动创建一个默认的存储策略 autogen，数据保留时间为永久，之后用户可以自己设置，例如保留最近2小时的数据。插入和查询数据时如果不指定存储策略，则使用默认存储策略，且默认存储策略可以修改。InfluxDB 会定期清除过期的数据。**measurement:** 测量指标名，例如 cpu_usage 表示 cpu 的使用率。**tag sets:** tags 在 InfluxDB 中会按照字典序排序，不管是 tagk 还是 tagv，只要不一致就分别属于两个 key，例如 host=server01,region=us-west 和 host=server02,region=us-west 就是两个不同的 tag set。**field name:** 例如上面数据中的 value 就是 fieldName，InfluxDB 中支持一条数据中插入多个 fieldName，这其实是一个语法上的优化，在实际的底层存储中，是当作多条数据来存储。**timestamp:** 每一条数据都需要指定一个时间戳，在 TSM 存储引擎中会特殊对待，以为了优化后续的查询操作。### 2）PointInfluxDB 中单条插入语句的数据结构，series + timestamp 可以用于区别一个 point，也就是说一个 point 可以有多个 field name 和 field value。### 3）Seriesseries 相当于是 InfluxDB 中一些数据的集合，在同一个 database 中，retention policy、measurement、tag sets 完全相同的数据同属于一个 series，同一个 series 的数据在物理上会按照时间顺序排列存储在一起。series 的 key 为 measurement + 所有 tags 的序列化字符串，这个 key 在之后会经常用到。代码中的结构如下：```bashtype Series struct &#123; mu sync.RWMutex Key string // series key Tags map[string]string // tags id uint64 // id measurement *Measurement // measurement&#125; 4）Shardshard 在 InfluxDB 中是一个比较重要的概念，它和 retention policy 相关联。每一个存储策略下会存在许多 shard，每一个 shard 存储一个指定时间段内的数据，并且不重复，例如 7点-8点 的数据落入 shard0 中，8点-9点的数据则落入 shard1 中。每一个 shard 都对应一个底层的 tsm 存储引擎，有独立的 cache、wal、tsm file。创建数据库时会自动创建一个默认存储策略，永久保存数据，对应的在此存储策略下的 shard 所保存的数据的时间段为 7 天，计算的函数如下：12345678func shardGroupDuration(d time.Duration) time.Duration &#123; if d &gt;= 180*24*time.Hour || d == 0 &#123; // 6 months or 0 return 7 * 24 * time.Hour &#125; else if d &gt;= 2*24*time.Hour &#123; // 2 days return 1 * 24 * time.Hour &#125; return 1 * time.Hour&#125; 如果创建一个新的 retention policy 设置数据的保留时间为 1 天，则单个 shard 所存储数据的时间间隔为 1 小时，超过1个小时的数据会被存放到下一个 shard 中。 存储引擎 - TSM Tree从 LevelDB（LSM Tree），到 BoltDB（mmap B+树），现在InfluxDB使用的是自己实现的 TSM Tree 的算法，类似 LSM Tree，针对 InfluxDB 的使用做了特殊优化。TSM Tree 是 InfluxDB 根据实际需求在 LSM Tree 的基础上稍作修改优化而来。TSM 存储引擎主要由几个部分组成： cache、wal、tsm file、compactor。 1）Shardshard 并不能算是其中的一个组件，因为这是在 tsm 存储引擎之上的一个概念。在 InfluxDB 中按照数据的时间戳所在的范围，会去创建不同的 shard，每一个 shard 都有自己的 cache、wal、tsm file 以及 compactor，这样做的目的就是为了可以通过时间来快速定位到要查询数据的相关资源，加速查询的过程，并且也让之后的批量删除数据的操作变得非常简单且高效。 在 LSM Tree 中删除数据是通过给指定 key 插入一个删除标记的方式，数据并不立即删除，需要等之后对文件进行压缩合并时才会真正地将数据删除，所以删除大量数据在 LSM Tree 中是一个非常低效的操作。而在 InfluxDB 中，通过 retention policy 设置数据的保留时间，当检测到一个 shard 中的数据过期后，只需要将这个 shard 的资源释放，相关文件删除即可，这样的做法使得删除过期数据变得非常高效。 2）Cachecache 相当于是 LSM Tree 中的 memtable，在内存中是一个简单的 map 结构，这里的 key 为 seriesKey + 分隔符 + filedName，目前代码中的分隔符为 #!~#，entry 相当于是一个按照时间排序的存放实际值的数组，具体结构如下：123456789101112131415161718192021type Cache struct &#123; commit sync.Mutex mu sync.RWMutex store map[string]*entry size uint64 // 当前使用内存的大小 maxSize uint64 // 缓存最大值 // snapshots are the cache objects that are currently being written to tsm files // they're kept in memory while flushing so they can be queried along with the cache. // they are read only and should never be modified // memtable 快照，用于写入 tsm 文件，只读 snapshot *Cache snapshotSize uint64 snapshotting bool // This number is the number of pending or failed WriteSnaphot attempts since the last successful one. snapshotAttempts int stats *CacheStatistics lastSnapshot time.Time&#125; 插入数据时，实际上是同时往 cache 与 wal 中写入数据，可以认为 cache 是 wal 文件中的数据在内存中的缓存。当 InfluxDB 启动时，会遍历所有的 wal 文件，重新构造 cache，这样即使系统出现故障，也不会导致数据的丢失。cache 中的数据并不是无限增长的，有一个 maxSize 参数用于控制当 cache 中的数据占用多少内存后就会将数据写入 tsm 文件。如果不配置的话，默认上限为 25MB，每当 cache 中的数据达到阀值后，会将当前的 cache 进行一次快照，之后清空当前 cache 中的内容，再创建一个新的 wal 文件用于写入，剩下的 wal 文件最后会被删除，快照中的数据会经过排序写入一个新的 tsm 文件中。目前的 cache 的设计有一个问题，当一个快照正在被写入一个新的 tsm 文件时，当前的 cache 由于大量数据写入，又达到了阀值，此时前一次快照还没有完全写入磁盘，InfluxDB 的做法是让后续的写入操作失败，用户需要自己处理，等待恢复后继续写入数据。 3）WALwal 文件的内容与内存中的 cache 相同，其作用就是为了持久化数据，当系统崩溃后可以通过 wal 文件恢复还没有写入到 tsm 文件中的数据。由于数据是被顺序插入到 wal 文件中，所以写入效率非常高。但是如果写入的数据没有按照时间顺序排列，而是以杂乱无章的方式写入，数据将会根据时间路由到不同的 shard 中，每一个 shard 都有自己的 wal 文件，这样就不再是完全的顺序写入，对性能会有一定影响。看到官方社区有说后续会进行优化，只使用一个 wal 文件，而不是为每一个 shard 创建 wal 文件。wal 单个文件达到一定大小后会进行分片，创建一个新的 wal 分片文件用于写入数据。 4）TSM file单个 tsm file 大小最大为 2GB，用于存放数据。TSM file 使用了自己设计的格式，对查询性能以及压缩方面进行了很多优化，在后面的章节会具体说明其文件结构。 5）Compactorcompactor 组件在后台持续运行，每隔 1 秒会检查一次是否有需要压缩合并的数据。主要进行两种操作，一种是 cache 中的数据大小达到阀值后，进行快照，之后转存到一个新的 tsm 文件中。另外一种就是合并当前的 tsm 文件，将多个小的 tsm 文件合并成一个，使每一个文件尽量达到单个文件的最大大小，减少文件的数量，并且一些数据的删除操作也是在这个时候完成。 目录与文件结构InfluxDB 的数据存储主要有三个目录。默认情况下是 meta, wal 以及 data 三个目录。meta 用于存储数据库的一些元数据，meta 目录下有一个 meta.db 文件。wal 目录存放预写日志文件，以 .wal 结尾。data 目录存放实际存储的数据文件，以 .tsm 结尾。这两个目录下的结构是相似的，其基本结构如下： wal目录结构12345678910-- wal -- mydb -- autogen -- 1 -- _00001.wal -- 2 -- _00035.wal -- 2hours -- 1 -- _00001.wal data目录结构12345678910-- data -- mydb -- autogen -- 1 -- 000000001-000000003.tsm -- 2 -- 000000001-000000001.tsm -- 2hours -- 1 -- 000000002-000000002.tsm 其中 mydb 是数据库名称，autogen 和 2hours 是存储策略名称，再下一层目录中的以数字命名的目录是 shard 的 ID 值。比如 autogen 存储策略下有两个 shard，ID 分别为 1 和 2，shard 存储了某一个时间段范围内的数据。再下一级的目录则为具体的文件，分别是 .wal 和 .tsm 结尾的文件。 1）WAL文件wal 文件中的一条数据，对应的是一个 key(measument + tags + fieldName) 下的所有 value 数据，按照时间排序。Type (1 byte): 表示这个条目中 value 的类型。Key Len (2 bytes): 指定下面一个字段 key 的长度。Key (N bytes): 这里的 key 为 measument + tags + fieldName。Count (4 bytes): 后面紧跟着的是同一个 key 下数据的个数。Time (8 bytes): 单个 value 的时间戳。Value (N bytes): value 的具体内容，其中 float64, int64, boolean 都是固定的字节数存储比较简单，通过 Type 字段知道这里 value 的字节数。string 类型比较特殊，对于 string 来说，N bytes 的 Value 部分，前面 4 字节用于存储 string 的长度，剩下的部分才是 string 的实际内容。 2）TSM文件单个 tsm 文件的主要格式如下： 主要分为四个部分： Header, Blocks, Index, Footer。其中 Index 部分的内容会被缓存在内存中，下面详细说明一下每一个部分的数据结构。 Header MagicNumber (4 bytes): 用于区分是哪一个存储引擎，目前使用的 tsm1 引擎，MagicNumber 为 0x16D116D1。Version (1 byte): 目前是 tsm1 引擎，此值固定为 1。 Blocks Blocks 内部是一些连续的 Block，block 是 InfluxDB 中的最小读取对象，每次读取操作都会读取一个 block。每一个 Block 分为 CRC32 值和 Data 两部分，CRC32 值用于校验 Data 的内容是否有问题。Data 的长度记录在之后的 Index 部分中。Data 中的内容根据数据类型的不同，在 InfluxDB 中会采用不同的压缩方式，float 值采用了 Gorilla float compression，而 timestamp 因为是一个递增的序列，所以实际上压缩时只需要记录时间的偏移量信息。string 类型的 value 采用了 snappy 算法进行压缩。Data 的数据解压后的格式为 8 字节的时间戳以及紧跟着的 value，value 根据类型的不同，会占用不同大小的空间，其中 string 为不定长，会在数据开始处存放长度，这一点和 WAL 文件中的格式相同。 IndexIndex 存放的是前面 Blocks 里内容的索引。索引条目的顺序是先按照 key 的字典序排序，再按照 time 排序。InfluxDB 在做查询操作时，可以根据 Index 的信息快速定位到 tsm file 中要查询的 block 的位置。这张图只展示了其中一部分，用结构体来表示的话类似下面的代码：12345678910111213141516type BlockIndex struct &#123; MinTime int64 MaxTime int64 Offset int64 Size uint32&#125;type KeyIndex struct &#123; KeyLen uint16 Key string Type byte Count uint32 Blocks []*BlockIndex&#125;type Index []*KeyIndex Key Len (2 bytes): 下面一个字段 key 的长度。Key (N bytes): 这里的 key 指的是 seriesKey + 分隔符 + fieldName。Type (1 bytes): fieldName 所对应的 fieldValue 的类型，也就是 Block 中 Data 内的数据的类型。Count (2 bytes): 后面紧跟着的 Blocks 索引的个数。 后面四个部分是 block 的索引信息，根据 Count 中的个数会重复出现，每个 block 索引固定为 28 字节，按照时间排序。 Min Time (8 bytes): block 中 value 的最小时间戳。Max Time (8 bytes): block 中 value 的最大时间戳。Offset (8 bytes): block 在整个 tsm file 中的偏移量。Size (4 bytes): block 的大小。根据 Offset + Size 字段就可以快速读取出一个 block 中的内容。 间接索引间接索引只存在于内存中，是为了可以快速定位到一个 key 在详细索引信息中的位置而创建的，可以被用于二分查找来实现快速检索。 offsets 是一个数组，其中存储的值为每一个 key 在 Index 表中的位置，由于 key 的长度固定为 2字节，所以根据这个位置就可以找到该位置上对应的 key 的内容。当指定一个要查询的 key 时，就可以通过二分查找，定位到其在 Index 表中的位置，再根据要查询的数据的时间进行定位，由于 KeyIndex 中的 BlockIndex 结构是定长的，所以也可以进行一次二分查找，找到要查询的数据所在的 BlockIndex 的内容，之后根据偏移量以及 block 长度就可以从 tsm 文件中快速读取出一个 block 的内容。 Footertsm file 的最后8字节的内容存放了 Index 部分的起始位置在 tsm file 中的偏移量，方便将索引信息加载到内存中。 数据查询与索引结构由于 LSM Tree 的原理就是通过将大量的随机写转换为顺序写，从而极大地提升了数据写入的性能，与此同时牺牲了部分读的性能。TSM 存储引擎是基于 LSM Tree 开发的，所以情况类似。通常设计数据库时会采用索引文件的方式（例如 LevelDB 中的 Mainfest 文件） 或者 Bloom filter 来对 LSM Tree 这样的数据结构的读取操作进行优化。InfluxDB 中采用索引的方式进行优化，主要存在两种类型的索引。 1）元数据索引一个数据库的元数据索引通过 DatabaseIndex 这个结构体来存储，在数据库启动时，会进行初始化，从所有 shard 下的 tsm file 中加载 index 数据，获取其中所有 Measurement 以及 Series 的信息并缓存到内存中。12345type DatabaseIndex struct &#123; measurements map[string]*Measurement // 该数据库下所有 Measurement 对象 series map[string]*Series // 所有 Series 对象，SeriesKey = measurement + tags name string // 数据库名&#125; 这个结构体中最主要存放的就是该数据下所有 Measurement 和 Series 的内容，其数据结构如下：123456789101112131415161718192021222324type Measurement struct &#123; Name string `json:"name,omitempty"` fieldNames map[string]struct&#123;&#125; // 此 measurement 中的所有 filedNames // 内存中的索引信息 // id 以及其对应的 series 信息，主要是为了在 seriesByTagKeyValue 中存储Id节约内存 seriesByID map[uint64]*Series // lookup table for series by their id // 根据 tagk 和 tagv 的双重索引，保存排好序的 SeriesID 数组 // 这个 map 用于在查询操作时，可以根据 tags 来快速过滤出要查询的所有 SeriesID，之后根据 SeriesKey 以及时间范围从文件中读取相应内容 seriesByTagKeyValue map[string]map[string]SeriesIDs // map from tag key to value to sorted set of series ids // 此 measurement 中所有 series 的 id，按照 id 排序 seriesIDs SeriesIDs // sorted list of series IDs in this measurement&#125;type Series struct &#123; Key string // series key Tags map[string]string // tags id uint64 // id measurement *Measurement // 所属 measurement // 在哪些 shard 中存在 shardIDs map[uint64]bool // shards that have this series defined&#125; 元数据查询InfluxDB 支持一些特殊的查询语句（支持正则表达式匹配），可以查询一些 measurement 以及 tags 相关的数据，例如123SHOW MEASUREMENTSSHOW TAG KEYS FROM "measurement_name"SHOW TAG VALUES FROM "measurement_name" WITH KEY = "tag_key" 例如我们需要查询 cpu_usage 这个 measurement 上传数据的机器有哪些，一个可能的查询语句为：1SHOW TAG VALUES FROM "cpu_usage" WITH KEY = "host" 首先根据 measurement 可以在 DatabaseIndex.measurements 中拿到 cpu_usage 所对应的 Measurement 对象。通过 Measurement.seriesByTagKeyValue 获取 tagk=host 所对应的以 tagv 为键的 map 对象。遍历这个 map 对象，所有的 key 则为我们需要获取的数据。 普通数据查询的定位对于普通的数据查询语句，则可以通过上述的元数据索引快速定位到要查询的数据所包含的所有 seriesKey，fieldName 和时间范围。举个例子，假设查询语句为获取 server01 这台机器上 cpu_usage 指标最近一小时的数据：1SELECT value FROM "cpu_usage" WHERE host='server01' AND time &gt; now() - 1h 先根据 measurement=cpu_usage 从 DatabaseIndex.measurements 中获取到 cpu_usage 对应的 Measurement 对象。之后通过 DatabaseIndex.measurements[“cpu_usage”].seriesByTagKeyValue[“host”][“server01”] 获取到所有匹配的 series 的 ID值，再通过 Measurement.seriesByID 这个 map 对象根据 series ID 获取它们的实际对象。注意这里虽然只指定了 host=server01，但不代表 cpu_usage 下只有这一个 series，可能还有其他的 tags 例如 user=1 以及 user=2，这样获取到的 series ID 实际上有两个，获取数据时需要获取所有 series 下的数据。 在 Series 结构体中的 shardIDs 这个 map 变量存放了哪些 shard 中存在这个 series 的数据。而 Measurement.fieldNames 这个 map 可以帮助过滤掉 fieldName 不存在的情况。至此，我们在 o(1) 的时间复杂度内，获取到了所有符合要求的 series key、这些 series key 所存在的 shardID，要查询数据的时间范围，之后我们就可以创建数据迭代器从不同的 shard 中获取每一个 series key 在指定时间范围内的数据。后续的查询则和 tsm file 中的 Index 的在内存中的缓存相关。 2）TSM File 索引上文中对于 tsm file 中的 Index 部分会在内存中做间接索引，从而可以实现快速检索的目的。这里看一下具体的数据结构：1234567type indirectIndex struct &#123; b []byte // 下层详细索引的字节流 offsets []int32 // 偏移量数组，记录了一个 key 在 b 中的偏移量 minKey, maxKey string minTime, maxTime int64 // 此文件中的最小时间和最大时间，根据这个可以快速判断要查询的数据在此文件中是否存在，是否有必要读取这个文件 tombstones map[string][]TimeRange // 用于记录哪些 key 在指定范围内的数据是已经被删除的&#125; b 直接对应着 tsm file 中的 Index 部分，通过对 offsets 进行二分查找，可以获取到指定 key 的所有 block 的索引信息，之后根据 offset 和 size 信息可以取出一个指定的 block 中的所有数据。123456789101112131415type indexEntries struct &#123; Type byte entries []IndexEntry&#125;type IndexEntry struct &#123; // 一个 block 中的 point 都在这个最小和最大的时间范围内 MinTime, MaxTime int64 // block 在 tsm 文件中偏移量 Offset int64 // block 的具体大小 Size uint32&#125; 在上一节中说明了通过元数据索引可以获取到所有 符合要求的 series key，它们对应的 shardID，时间范围。通过 tsm file 索引，我们就可以根据 series key 和 时间范围快速定位到数据在 tsm file 中的位置。 从 tsm file 中读取数据InfluxDB 中的所有数据读取操作都通过 Iterator 来完成。Iterator 是一个抽象概念，并且支持嵌套，一个 Iterator 可以从底层的其他 Iterator 中获取数据并进行处理，之后再将结果传递给上层的 Iterator。这部分的代码逻辑比较复杂，这里不展开说明。实际上 Iterator 底层最主要的就是通过 cursor 来获取数据。1234567891011121314151617181920212223242526type cursor interface &#123; next() (t int64, v interface&#123;&#125;)&#125;type floatCursor interface &#123; cursor nextFloat() (t int64, v float64)&#125;// 底层主要是 KeyCursor，每次读取一个 block 的数据type floatAscendingCursor struct &#123; // 内存中的 value 对象 cache struct &#123; values Values pos int &#125; tsm struct &#123; tdec TimeDecoder // 时间序列化对象 vdec FloatDecoder // value 序列化对象 buf []FloatValue values []FloatValue // 从 tsm 文件中读取到的 FloatValue 的缓存 pos int keyCursor *KeyCursor &#125;&#125; cursor 提供了一个 next() 方法用于获取一个 value 值。每一种数据类型都有一个自己的 cursor 实现。底层实现都是 KeyCursor，KeyCursor 会缓存每个 Block 的数据，通过 Next() 函数依次返回，当一个 Block 中的内容读完后再通过 ReadBlock() 函数读取下一个 Block 中的内容。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>influxdb</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>TSDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工作中如何做好技术积累]]></title>
    <url>%2F2019%2F04%2F22%2Ftech-Accumulation%2F</url>
    <content type="text"><![CDATA[引言古人云：“活到老，学到老。”互联网算是最辛苦的行业之一，“加班”对工程师来说已是“家常便饭”，同时互联网技术又日新月异，很多工程师都疲于应付，叫苦不堪。以至于长期以来流传一个很广的误解：35岁是程序员工作的终点。最近在网上看到美团大牛对于工作中如何做技术积累的文章，其中一些观点与我很赞同，所以分享出来。 如何在繁忙的工作中做好技术积累，构建个人核心竞争力，相信是很多工程师同行都在思考的问题。本文是我自己的一些总结，试图从三个方面来解答： 第一部分阐述了一些学习的原则。任何时候，遵循一些经过检验的原则，都是影响效率的重要因素，正确的方法是成功的秘诀。提升工作和学习效率的另一个重要因素是释惑和良好心态。第二部分分析了我在工作中碰到和看到的一些典型困惑。成为优秀的架构师是大部分初中级工程师的阶段性目标。第三部分剖析架构师的能力模型，让大家对目标所需能力有一个比较清晰的认知。 如何学习在繁忙的工作中，持之以恒、不断学习和进步是一件艰巨的任务，需要坚强的毅力和坚定的决心。如果方法不得当，更是事倍功半。幸好我们的古人和现在哲人已经总结了很多优秀的学习方法论，这里汇总了一些重要原则。遵循这些方法必会对大家的工作学习大有裨益。 贵在坚持有报道指出，过去几十年的知识量超过之前人类几千年的知识量总和。而计算机领域绝对是当代知识更新最快的领域之一，因此，工程师必须要接受这样一个现实，现在所掌握的深厚知识体系很快就会被淘汰。要想在计算机领域持续做优秀架构师，就必须不停的学习，掌握最新技术。总之，学不可以已。所谓“冰冻三尺，非一日之寒，水滴石穿，非一日之功”，通往架构师的道路漫长而又艰巨，轻易放弃，则所有付出瞬间付之东流。要想成为优秀的架构师，贵在坚持！虽然知识更新很快，但是基础理论的变化却非常缓慢。这就是“道”和“象”关系，纵是世间万象，道却万变不离其宗。对于那些非常基础的理论知识，我们需要经常复习，也就是“学而时习之”。 重视实践古人云：“纸上得来终觉浅，绝知此事要躬行。” 学习领域有所谓721模型：个人的成长70%来自于岗位实践，20%来自向他人学习，10%来自于培训。虽然这种理论存在争议，但对于工程师们来说，按照实践、学习和培训的方式进行重要性排序，大致是不错的。所以重视实践，在实践中成长是最重要的学习原则。 人类的认知有两种：感性认知和理性认知。 这两种认知互相不可替代性。实践很大程度来自于感性学习，看书更像是理性学习。以学开汽车做例子，很难想象什么人能够仅仅通过学习书本知识就会开汽车。书本知识主要是传道——讲述抽象原型，而对其具体应用场景的讲述往往含糊其辞，对抽象原型之间的关系也是浅尝辄止。采用同样精确的语言去描述应用场景和关联关系将会失去重点，让人摸不着头脑。所以，仅仅通过看书来获得成长就像是用一条腿走路。重视实践，充分运用感性认知潜能，在项目中磨炼自己，才是正确的学习之道。在实践中，在某些关键动作上刻意练习，也会取得事半功倍的效果。 重视交流牛顿说：“如果说我看得比别人远一些，那是因为我站在巨人的肩膀上。”我们需要从别人身上学习。从老师、领导、同事、下属甚至对手身上学习，是快速成长的重要手段。向老师和领导学习已经是人们生活习惯的一部分了。但是从同事甚至对手那里学习也很重要，因为这些人和我们自身更相似。所以要多多观察，取其所长，弃其所短。对于团队的小兄弟和下属，也要“不耻下问”。此外，在项目中积极参与具体方案讨论也非常重要。参与者先验感知了相关背景，并且讨论的观点和建议也是综合了发言者多种知识和技能。所以，讨论让参与者能够非常全面，立体地理解书本知识。同时，和高手讨论，他们的观点就会像修剪机剪树枝一样，快速的剪掉自己知识领域里面的疑惑点。 重视总结和输出工程师在实践中会掌握大量细节，但是，即使掌握了所有细节，却没有深刻的总结和思考，也会陷入到“学而不思则罔”的境地。成长的“量变”来自于对细节的逐渐深入地把控，而真正的“质变”来自于对“道”的更深层次的理解。将经验输出，接受别人的检验是高层次的总结。这种输出不仅帮助了别人，对自身更是大有裨益。总结的方式有很多，包括组织分享，撰写技术文章等等。当然“日三省吾身”也是不错的总结方式。总之，多多总结，多多分享，善莫大焉！解答别人的问题也是个人成长的重要手段。有时候，某个问题自己本来不太懂，但是在给别人讲解的时候却豁然开朗。所以，“诲人不倦”利人惠己。 重视规划凡事预则立，不预则废。对于漫长的学习生涯而言，好的计划是成功的一半。 长期规划长期规划的实施需要毅力和决心，但是做正确的长期规划还需要高瞻远瞩的眼界、超级敏感的神经和中大奖的运气。对于大部分人来说，长期规划定主要是“定方向”。但遵循如下原则能够减少犯方向性错误的概率：远离日暮西山的行业。做自己感兴趣的事情。做有积累的事情。一边走一边看，切勿一条道走到黑。 短期规划良好的短期规划应该在生活、成长、绩效和晋升之间取得平衡。大部分公司都会制定一个考核周期——少则一个月，多则一年。所以不妨以考核周期作为短期学习规划周期。本质上，规划是一个多目标优化问题，它有一系列的理论方案，这里不一一细说。基于相关理论，我给出一个简单易行的方案： 确定目标优先级。比如：成长、生活、绩效。确定每个目标的下限。从优化理论的角度来看，这被称为约束。比如绩效必须在一般以上，之前已经规划好的旅行不能更改，必须读完《Effective Java》等等。优先为下限目标分配足够的资源。比如，事先规划好的旅行需要10天，这10天就必须预算出去。按照各主目标的顺序依次分配资源。比如，最终分配给学习的时间是10天。在给定的学习预算下，制定学习目标，要激进。然后给出执行方案。比如，学习目标是掌握基本的统计学知识，并成为Java专家。具体方案为：完成《Effective Java》、《Java Performance》、《Design Pattern》、《Head First Statistics》四本书的阅读。对规划中的各学习任务按目标优先级进行排序，并最先启动优先级最高的任务。比如，最高优先级是掌握统计理论，那么就要先看《Head First Statistics》。 对于该方案，要注意以下几点：最低目标必须能够轻松达成的目标，否则，从优化理论的角度来讲，该命题无解。比如，类似“半年内完成晋级两次、绩效全部S、从菜鸟成为Java专家”就不太合适作为最低目标。总之，要区分理想和梦想。主要目标规划必须具备一定的挑战性，需要规划出不可能完成的目标。过度规划本质上是一种贪婪算法，目的是目标价值最大化。因为一切皆有变数，如果其他目标能够提前完成，就不妨利用这些时间去完成更多的学习目标。总之，前途必须光明，道路必须坎坷。各目标之间不一定共享资源，规划不一定互有冲突。 此外，短期规划还可以从如下几个方面进行优化：学习计划最好能结合工作计划，理论联系实际结合，快速学以致用。比如，本季度规划去做一些数据分析工作，那么不妨把学习目标设置为学习统计知识。要灵活对待规划的目标和具体执行步骤，需要避免“郑人买履”式的笑话。面临新的挑战和变化，规划需要不断地调整。 那些令人纠结的困惑人生是一场马拉松，在漫长的征途中，难免有很多困惑。困惑就像枷锁，使我们步履蹒跚，困惑就像死锁，让我们停滞不前。接下来我将总结自己在工作中碰到和看到的一些典型困惑。这些困惑或者长期困扰作者本人，或者困扰我身边的同事和朋友。当这些困惑被释然之后，大家都感觉如重获释，为下一阶段的征程提供满满的正能量。人生就像一场旅途，不必在乎目的地，在乎的，应该是沿途的风景，以及看风景的心情。良好的心态是技术之旅最好的伴侣。期望通过这个解惑之旅，让大家拥有一个愉快的心情去感受漫长的学习旅途。 学无止境吗必须要承认一个残酷的现实：人的生命是有限的，知识却是无限的。用有限的生命去学习无限的知识是不可能完成的任务。一想到此，有些工程师不免产生一些悲观情绪。如果方法得当并且足够勤奋，悲伤大可不必。虽然，人类的整体知识体系一直在扩张。但是就很多重要的工程细分领域，基础理论并不高深。计算机的很多重要领域，工程师有能力在有限时间内抓住核心要害。比如，密码学被认为是门非常高深的学科，但是一大类密码技术的基础是数论中一个非常简单的理论——素因数分解：给出两个素数，很容易算出它们的积，然而反过来给定两个素数的积，分解的计算量却非常惊人。“一致性”算得上是计算机领域里面最经典的难题，它是所有分布式系统的基础，从多核多CPU到多线程，从跨机器到跨机房，无所不在，几乎所有的计算机从业人员都在解决这个问题，但是Paxos给出了一个很优雅的解决方案。权限管理是很多工程师的噩梦，但如果你能搞定“Attribute Based Access Control(ABAC)”和“Role-Based Access Control(RBAC)”，也能达到相当高度。另外，技术学习是一场对抗赛，虽然学无止境，超越大部分对手就是一种胜利。所以，以正确的学习方式，长时间投入就会形成核心竞争力。 没有绝对高明的技术，只有真正的高手致力于在技术上有所成就的工程师，都梦想有朝一日成为技术高手。但技术高手的标准却存在很大的争议。这是一个有着悠久历史的误解：以某种技术的掌握作为技术高手的评判标准。我经常碰到这样一些情景：因为掌握了某些技术，比如Spring、Kafka、Elasticsearch等，一些工程师就自封为高手。有些工程师非常仰慕别的团队，原因竟是那个团队使用了某种技术。这种误解的产生有几个原因：首先，技多不压身，技术自然是掌握的越多越好，掌握很多技术的人自然不是菜鸟。其次，在互联网时代来临之前，信息获取是非常昂贵的事情。这就导致一项技能的掌握可以给个人甚至整个公司带来优势地位。互联网时代，各种框架的出现以及开源的普及快速淘汰或者降低了很多技能的价值，同时降低了很多技术的学习门槛。所以，在当前，掌握某项技能知识只能是一个短期目标。怀揣某些技能就沾沾自喜的人需要记住：骄傲使人退步。所谓“麻雀虽小，五脏俱全”。如果让你来做造物主，设计麻雀和设计大象的复杂度并没有明显区别。一个看起来很小的业务需求，为了达到极致，所需要的技术和能力是非常综合和高深的。真正的高手不是拿着所掌握的技术去卡客户需求，而是倾听客户的需求，给出精益求精的方案。完成客户的需求是一场擂台赛，真正的高手，是会见招拆招的。 不做项目就无法成长吗在项目中学习是最快的成长方式之一，很多工程师非常享受这个过程。但是一年到头都做项目，你可能是在一家外包公司。对于一个做产品的公司，如果年头到年尾都在做项目，要不然就是在初步创业阶段，要不然就是做了大量失败的项目，总之不算是特别理想的状态。正常情况，在项目之间都会有一些非项目时间。在这段时间，有些同学会产生迷茫，成长很慢。项目真的是越多越好吗？答案显然是否定的。重复的项目不会给工程师们带来新的成长。不停的做项目，从而缺乏学习新知识的时间，会导致“做而不学则殆”。真正让工程师出类拔萃的是项目的深度，而不是不停地做项目。所以，在项目之间的空档期，工程师们应该珍惜难得的喘息之机，深入思考，把项目做深、做精。如何提高项目的深度呢？一般而言，任何项目都有一个目标，当项目完成后，目标就算基本达成了。但是，客户真的满意了吗？系统的可用性、可靠性、可扩展性、可维护性已经做到极致了吗？这几个问题的答案永远是否定的。所以，任何一个有价值的项目，都可以一直深挖。深挖项目，深度思考还可以锻炼工程师的创造力。期望不停地做项目的人，就像一个致力于训练更多千里马的人是发明不出汽车的。锻炼创造力也不是一蹴而就的事情，需要长时间地思考。总之，工程师们应该总是觉得时间不够用，毕竟时间是最宝贵的资源。 职责真的很小吗很多时候，一个工程师所负责系统的数量和团队规模与其“江湖地位”正相关。但是，江湖地位与技术成长没有必然关联。提升技术能力的关键是项目深度以及客户的挑剔程度。项目越多，在单个项目中投入的时间就越少，容易陷入肤浅。特别需要避免的是“ 在其位不谋其政”的情况。团队越大，在管理方面需要投入的精力就越多。在管理技巧不成熟，技术眼界不够高的前提强行负责大团队，可能会导致个人疲于应付，团队毫无建树。最终“ 一将无能，累死三军”，效果可能适得其反。从技术发展的角度来说，技术管理者应该关注自己所能把控的活跃项目的数量，并致力于提高活跃项目的影响力和技术深度。团队人数要与个人管理能力、规划能力和需求把控能力相适应。一份工作让多个人来干，每个人的成长都受限。每个人都做简单重复的工作，对技术成长没有任何好处。团队管理和项目管理需要循序渐进，忌“拔苗助长”。 一定要当老大吗有一些工程师的人生理想是做团队里的技术老大，这当然是一个值得称赞的理想。可是，如果整个团队技术能力一般，发展潜力一般，而你是技术最强者，这与其说是幸运，不如说是悲哀。这种场景被称之为“武大郎开店”。团队里的技术顶尖高手不是不能做，但为了能够持续成长，需要满足如下几个条件：首先你得是行业里面的顶尖专家了——实在很难找到比你更强的人了！其次，你经常需要承担对你自己的能力有挑战的任务，但同时你拥有一批聪明能干的队友。虽然你的技术能力最高，但是在你不熟悉的领域，你的队友能够进行探索并扩展整个团队的知识。最后，你必须要敏而好学，不耻下问。否则，加入更强的技术团队或许是更好的选择，最少不是什么值得骄傲的事情。 平台化的传说平台化算得上是“高大上”的代名词了，很多工程师挤破头就为了和“平台化”沾点边。然而和其他业务需求相比，平台化需求并没有本质上的区别。无论是平台化需求还是普通业务需求，它的价值都来自于客户价值。不同点如下：很多平台化需求的客户来自于技术团队，普通需求的客户来自于业务方。产品经理不同。普通业务需求来自于产品经理，平台化需求的产品经理可能就是工程师自己。长期被产品经理“压迫”的工程师们，在平台化上终于找到“翻身农奴把歌唱”的感觉。很多平台化的关注点是接入能力和可扩展性，而普通业务的关注点更多。归根结底，平台化就是一种普通需求。 在实施平台化之前，一定要避免下面两个误区：平台化绝对不是诸如“统一”、“全面”之类形容词的堆砌。是否需要平台化，应该综合考虑：客户数量，为客户解决的问题，以及客户价值是否值得平台化的投入。平台化不是你做平台，让客户来服务你。一些平台化设计者的规划设计里面，把大量的平台接入工作、脏活累活交给了客户，然后自己专注于所谓“最高大上”的功能。恰恰相反，平台化应该是客户什么都不做，所有的脏活累活都由平台方来做。本质上讲，平台化的价值来自于技术深度。真正体现技术深度的恰恰是设计者能够很轻松的把所有的脏活累活搞定。所以平台化的最佳实践是：投入最少的资源，解决最多的问题。平台解决一切，客户坐享其成。 搞基础技术就一定很牛吗经常听到同学们表达对基础技术部同学的敬仰之情，而对搞业务技术的同学表现出很轻视，认为存储、消息队列、服务治理框架（比如美团点评内部使用的OCTO）、Hadoop等才能被称为真正的技术。事实并非如此，更基础的并不一定更高深。比如下面这个流传很久的段子：越高级的语言就越没有技术含量。但真是这样吗，就拿Java和C来说，这是完全不同的两种语言，所需要的技能完全不同。C或许跟操作系统更加接近一点，和CPU、内存打交道的机会更多一点。但是为了用好Java，程序员在面向对象、设计模式、框架技术方面必须要非常精通。Java工程师转到C方向确实不容易，但作者也见过很多转到Java语言的C工程师水土不服。 基础技术和业务应用技术必然会有不同的关注点，没有高低之分。之所以产生这种误解，有两个原因：基础技术相对成熟，有比较完整的体系，这给人一个高大上的感觉。业务应用技术相对来说，由于每个团队使用的不一样，所以成熟度参差不齐，影响力没有那么大。基础技术的门槛相对来说高一点，考虑到影响面，对可靠性、可用性等有比较高的最低要求。但是门槛高不代表技术含量高，另外成熟技术相对来说在创新方面会受到很大的约束。但是最先进的技术都来自活跃的创新。对比下来，业务技术和基础技术各有千秋。但真正的高手关注的是解决问题，所有的技术都是技能而已。 可行性调研的那些坑工作中开展可行性调研时有发生。做可行性调研要避免如下情况：把可行性调研做成不可行性调研。这真的非常糟糕。不可行性的结论往往是：因为这样或者那样的原因，所以不可行。避免“老鼠给猫挂铃铛”式的高风险可行性方案。“天下大事必作于细”，可行性调研一定要细致入微，避免粗枝大叶。避免调研时间过长。如果发现调研进展进入到指数级复杂度，也就是每前进一步需要之前两倍的时间投入，就应该果断的停止调研。 可行性调研的结论应该是收益与成本的折衷，格式一般如下：首先明确预期的结果，并按照高中低收益进行分级。阐述达成每种预期结果需要采取的措施和方案。给出实施各方案需要付出的成本。 工程师天生不善沟通吗实际工作中，沟通所导致的问题层出不穷。工程师有不少是比较内向的，总是被贴上“不善沟通”的标签。实际上，沟通能力是工程师最重要的能力之一，良好的沟通是高效工作学习的基础，也是通过学习可以掌握的。下面我按工程师的语言说说沟通方面的经验。第一类常见的问题是沟通的可靠性。从可靠性的角度来讲，沟通分为TCP模式和UDP模式。TCP模式的形象表述是：我知道你知道。UDP模式的形象表述是：希望你知道。TCP模式当然比较可靠，不过成本比较高，UDP模式成本低，但是不可靠。在沟通可靠性方面，常见错误有如下两种：经常听到的这样的争论。一方说：“我已经告诉他了”，另一方说：“我不知道这个事情呀”。把UDP模式被当作TCP模式来使用容易产生扯皮。过度沟通。有些同学对沟通的可靠性产生了过度焦虑，不断的重复讨论已有结论问题。把TCP模式当成UDP来使用，效率会比较低。 第二类沟通问题是时效性问题。从时效性讲，沟通分为：同步模式和异步模式。同步沟通形象地说就是：你现在给我听好了。异步沟通的形象表述是：记得给我做好了。在沟通时效性方面，有如下两种常见错误：已经出现线上事故，紧急万分。大家你一言，我一语，感觉事故可能和某几个人有关，但是也不能完全确定，所以没有通知相关人员。最终，一个普通的事故变成了严重事故。对于紧急的事情，必须要同步沟通。半夜三点你正在熟睡，或者周末正在逛街，接到一个电话：“现在有个需求，能否立刻帮忙做完。”这会非常令人郁闷，因为那并不是紧急的事情。不是所有的需求都需要立刻解决。 有效沟通的一个重要原则是提前沟通。沟通本质是信息交流和处理，可以把被沟通对象形象地比喻成串行信息处理的CPU。提前沟通，意味着将处理请求尽早放入处理队列里面。下面的例子让很多工程师深恶痛绝：一个需求策划了1个月，产品设计了2周。当开发工程是第一次听说该需求的时候，发现开发的时间是2天。工程师据理力争，加班加点1周搞定。最后的结论是工程师非常不给力，不配合。就像工程师讨厌类似需求一样。要协调一个大项目，希望获得别人的配合，也需要尽早沟通。 有效沟通的另外一个重点是“不要跑题”。很多看起来很接近的问题，本质上是完全不同的问题。比如：一个会议的主题是“如何实施一个方案”，有人却可能提出“是否应该实施该方案”。 “如何实施”和“是否应该实施”是完全不同的两个问题，很多看起来相关的问题实际上跑题很远。“跑题”是导致无效沟通的重要原因。 良好沟通的奥秘在于能掌握TCP模式和UDP模式精髓，正确判断问题的紧急性，尽量提前沟通，避免跑题。 带人之道有些初为导师的工程师由于担心毕业生的能力太弱，安排任务时候谆谆教诲，最后感觉还是有所顾虑，干脆自己写代码。同样的事情发生在很多刚刚管理小团队的工程师身上。最终的结果他们：写完所有的代码，让下属无代码可写。“ 事必躬亲”当然非常糟糕，最终的往往是团队的整体绩效不高，团队成员的成长很慢，而自己却很累。古人说：“用人不疑，疑人不用。”这句话并非“放之四海而皆准”。在古代，受限于通信技术，反馈延迟显著，而且信息在传递过程中有大量噪音，变形严重。在这种情况下，如果根据短期内收集的少量变形的信息做快速决断，容易陷于草率。在公司里，这句话用于选人环节更为恰当，应该改为：录用不疑，疑人不录。考虑到招聘成本，就算是在录用层面，有时候也无法做到。作为一个小团队的管理者，能够快速准确的获取团队成员的各种反馈信息，完全不需要“用人不疑，疑人不用”。用人的真正理论基础来自于“探索和利用”(Exploration and Exploitation )。不能因为下属能做什么就只让他做什么，更不能因为下属一次失败就不给机会。 根据经典的“探索和利用”(Exploration and Exploitation )理论，良好的用人方式应该如下：首选选择相信，在面临失败后，收缩信任度。查找失败的原因，提供改进意见，提升下属的能力。总是给下属机会，在恰当地时机给下属更高的挑战。总之，苍天大树来自一颗小种子，要相信成长的力量。 效率、效率、效率经常看到有些同学给自己的绩效评分是100分——满分，原因是在过去一段时间太辛苦了，但最终的绩效却一般般。天道酬勤不错，但是天道更酬巧。工程师们都学过数据结构，不同算法的时间复杂度的差距，仅仅通过更长的工作时间是难以弥补的。 为了提升工作学习效率，我们需要注意以下几点：主要关注效率提升。很多时候，与效率提升所带来的收益相比，延长时间所带来的成果往往不值得一提。要有清晰的结果导向思维。功劳和苦劳不是一回事。做正确的事情，而不仅仅正确地做事情。这是一个被不断提起的话题，但是错误每天都上演。为了在规定的时间内完成一个大项目，总是要有所取舍。如果没有重点，均匀发力，容易事倍功半。如果“南辕北辙”，更是可悲可叹。 架构师能力模型前面我们已经讲完了原则和一些困惑，那么工程师到底应该怎么提升自己呢？成为优秀的架构师是大部分初中级工程师的阶段性目标。优秀的架构师往往具备八种核心能力：编程能力、调试能力、编译部署能力、性能优化能力、业务架构能力、在线运维能力、项目管理能力和规划能力。这几种能力之间的关系大概如下图。编程能力、调试能力和编译部署能力属于最基础的能力。不能精通掌握这三种能力，很难在性能优化能力和业务架构能力方面有所成就。具备了一定的性能优化能力和业务架构能力之后，才能在线运维能力和项目管理能力方面表现优越。团队管理能力是最高能力，它对项目管理能力的依赖度更大。 编程能力对工程师而言，编程是最基础的能力，必备技能。其本质是一个翻译能力，将业务需求翻译成机器能懂的语言。提升编程能力的书籍有很多。精通面向对象和设计模式是高效编程的基础。初级工程师应该多写代码、多看代码。找高手做Code Review，也是提升编程水平的捷径。 调试能力程序代码是系统的静态形式，调试的目的是通过查看程序的运行时状态来验证和优化系统。本质上讲，工程师们通过不断调试可以持续强化其通过静态代码去预测运行状态的能力。所以调试能力也是工程师编程能力提升的关键手段。很早之前有个传说：“调试能力有多强，编程能力就有多强。”不过现在很多编辑器的功能很强大，调试能力的门槛已经大大降低。调试能力是项目能否按时、高质量提交的关键。即使一个稍具复杂度的项目，大部分工程师也无法一次性准确无误的完成。大项目都是通过不断地调试进行优化和纠错的。所以调试能力是不可或缺的能力。多写程序，解决Bug，多请教高手是提升调试能力的重要手段。 编译部署能力编译并在线上部署运行程序是系统上线的最后一个环节。随着SOA架构的普及以及业务复杂度的增加，大部分系统只是一个完整业务的一个环节，因此，本地编译和运行并不能完全模拟系统在线运行。为了快速验证所编写程序的正确性，编译并在线上部署就成了必要环节。所以编译部署能力是一个必备技能。让盘根错节的众多子系统运行起来是个不小的挑战。得益于SOA架构的普及以及大量编译、部署工具的发展，编译部署的门槛已经大大降低。基于应用层进行开发的公司，已经很少有“编译工程师”的角色了。但是对于初级工程师而言，编译部署仍然不是一个轻松的事情。 性能优化能力衡量一个系统成功的一个重要指标是使用量。随着使用量的增加和业务复杂度的增加，大部分系统最终都会碰到性能问题。 性能优化能力是一个综合能力。因为： 影响系统性能的因素众多，包括：数据结构、操作系统、虚拟机、CPU、存储、网络等。为了对系统性能进行调优，架构师需要掌握所有相关的技术。精通性能优化意味着深刻理解可用性、可靠性、一致性、可维护性、可扩展性等的本质。性能优化与业务强耦合，最终所采取的手段是往往折衷的结果。所以，性能优化要深谙妥协的艺术。可以说，性能优化能力是工程师们成长过程中各种技能开始融会贯通的一个标志。 这方面可以参考之前的博客文章“常见性能优化策略的总结”。市场上还有很多与性能优化相关的书籍，大家可以参考。多多阅读开源框架中关于性能优化方面的文档和代码也不失为好的提升手段。动手解决线上性能问题也是提升性能优化能力的关键。如果有机会，跟着高手学习，分析性能优化解决方案案例，也是快速提升性能优化能力的手段。 在线运维能力如果说性能优化能力体现的是架构师的静态思考能力，在线运维能力考验的就是动态反应能力。残酷的现实是，无论程序多么完美，Bug永远存在。与此同时，职位越高、责任越大，很多架构师需要负责非常重要的在线系统。对于线上故障，如果不能提前预防以及快速解决，损失可能不堪设想，所以在线运维能力是优秀架构师的必备技能。为了对线上故障进行快速处理，标准化的监控、上报、升级，以及基本应对机制当然很重要。通过所观察到的现象，快速定位、缓解以及解决相关症状也相当关键。这要求架构师对故障系统的业务、技术具备通盘解读能力。解决线上故障的架构师就好比一个在参加比赛F1的车手。赛车手必须要了解自身、赛车、对手、同伴、天气、场地等所有因素，快速决策，不断调整。架构师必须要了解所有技术细节、业务细节、处理规范、同伴等众多因素，快速决断，迅速调整。 在线运维本质上是一个强化学习的过程。很多能力都可以通过看书、查资料来完成，但在线运维能力往往需要大量的实践来提升。 业务架构能力工程师抱怨产品经理的故事屡见不鲜，抱怨最多的主要原因来自于需求的频繁变更。需求变更主要有两个来源：第一个原因是市场改变或战略调整，第二个原因是伪需求。对于第一个原因，无论是工程师还是产品经理，都只能无奈的接受。优秀的架构师应该具备减少第二种原因所导致的需求变更的概率。 伪需求的产生有两个原因：第一个原因是需求传递变形。从信息论的角度来讲，任何沟通都是一个编码和解码的过程。典型的需求从需求方到产品经理，最终到开发工程师，最少需要经历三次编码和解码过程。而信息的每一次传递都存在一些损失并带来一些噪音，这导致有些时候开发出来的产品完全对不上需求。此外，需求方和产品经理在需求可行性、系统可靠性，开发成本控制方面的把控比较弱，也会导致需求变形。第二个原因就是需求方完全没有想好自己的需求。 优秀的架构师应该具备辨别真伪需求的能力。应该花时间去了解客户的真实业务场景，具备较强的业务抽象能力，洞悉客户的真实需求。系统的真正实施方是工程师，在明确客户真实需求后，高明的架构师应该具备准确判断项目对可行性、可靠性、可用性等方面的要求，并能具备成本意识。最后，由于需求与在线系统的紧耦合关系，掌握在线系统的各种细节也是成功的业务架构的关键。随着级别的提升，工程师所面对的需求会越来越抽象。承接抽象需求，提供抽象架构是架构师走向卓越的必经之途。市场上有一些关于如何成为架构师的书，大家可以参考。但是架构能力的提升，实践可能是更重要的方式。业务架构师应该关注客户的痛点而不是PRD文档，应该深入关注真实业务。掌握现存系统的大量技术和业务细节也是业务架构师的必备知识。 项目管理能力作为工业时代的产物，分工合作融入在互联网项目基因里面。架构师也需要负责几个重大项目才能给自己正名。以架构师角色去管理项目，业务架构能力当然是必备技能。此外，人员管理和成本控制意识也非常重要。项目管理还意味着要有一个大心脏。重大项目涉及技术攻关、人员变动、需求更改等众多可变因素。面临各种变化，还要在确保目标顺利达成，需要较强的抗压能力。人员管理需要注意的方面包括：知人善用，优化关系，简化沟通，坚持真理。知人善用意味着架构师需要了解每个参与者的硬技能和软素质。同时，关注团队成员在项目过程中的表现，按能分配优化关系意味着管理团队的情绪，毕竟项目的核心是团队，有士气的团队才能高效达成目标。简化沟通意味着快速决策，该妥协的时候妥协，权责分明。坚持真理意味着顶住压力，在原则性问题上绝不退步。 成本控制意味着对项目进行精细化管理，需要遵循如下几个原则：以终为始、确定里程碑。为了达成目标，所有的计划必须以终为始来制定。将大项目分解成几个小阶段，控制每个阶段的里程碑可以大大降低项目失败的风险。把控关键路径和关键项目。按照关键路径管理理论（CPM）的要求，架构师需要确定每个子项目的关键路径，确定其最早和最晚启动时间。同时，架构师需要关注那些可能会导致项目整体延期的关键节点，并集中力量攻破。掌控团队成员的张弛度。大项目持续时间会比较长，也包含不同工种。项目实施是一个不断变化的动态过程，在这个过程中不是整个周期都很紧张，不是所有的工种都一样忙。优秀的架构师必须要具备精细阅读整体项目以及快速反应和实时调整的能力。这不仅仅可以大大降低项目成本，还可以提高产出质量和团队满意度。总体来说，“前紧后松”是项目管理的一个重要原则。项目管理方面的书籍很多。但是，提高业务架构能力同样重要。积极参与大项目并观察别人管理项目的方式也是非常重要的提升手段。 团队管理能力不想做CTO的工程师不是一个好的架构师。走向技术管理应该是工程师的一个主流职业规划。团队管理的一个核心能力就是规划能力，这包括项目规划和人员规划。良好的规划需要遵循如下原则：规划是利益的博弈。良好的规划上面对得起老板，中间对得起自己，下面对得起团队。在三者利益者寻找平衡点，实现多方共赢考验着管理者的智慧和精细拿捏的能力。任何规划都比没有规划好。没有规划的团队就是没头的苍蝇，不符合所有人的利益。规划不是本本主义。市场在变，团队在变，规划也不应该一成不变。 客户至上的是项目规划的出发点。就人员规划而言，规划需要考量团队成员的能力、绩效、成长等多方面的因素。市场上有很多规划管理方面的书籍，值得阅读。最优化理论虽然是技术书籍，但它是规划的理论基础，所以不妨多看看翻阅一下。从自我规划开始，多多学习别人的规划也是规划能力提升的重要手段。 总结关于工作中如何做到积累大概这么多，以后还会接着把自己的观点和一些精品文章观点分享。期望对工程师们的工作学习有所帮助。需要申明的是，文章内容挂一漏万，所谓的架构师能力模型也是作者的个人观点。欢迎大家交流分享自己在学习成长方面的心得。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>随想</category>
      </categories>
      <tags>
        <tag>干货</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工程师能力模型告诉你写代码这条路怎么走多远]]></title>
    <url>%2F2019%2F04%2F22%2Fengineers-way%2F</url>
    <content type="text"><![CDATA[职场危机感似乎是每个人在职业生涯都会遇到的话题，我对这种危机处境和自己曾遇到的问题进行了一番思考， 参考了其他人的一些结论并结合自身的经历，设计了应对的初步方案。通过这篇文章，希望能给大家一些启发，也欢迎大家一起讨论、发表建议，化紧张为动力，让危机变机遇。 问题分析与定义要解决这个问题，需要从问题本身出发，分析为何会有职场危机感，以及应该构建哪些能力来进行应对。是否会遭遇职场危机，与职业本身特性有很大关系。而大部分危机，来自于下面两点：1.能力习得速度快的行业，后辈更容易挑战前辈，形成危机感。2.技能演进速度快的行业，手里的秘籍容易成为废纸，形成危机感，如下图所示：图1 技能演进速度快的行业 因此，就这两个关键点，对一些行业进行定性分析，如下图所示：图2 常见职业在能力习得和演变象限中的分布 从能力习得速度这方面看，码农虽然身处高科技行业，但得益于大量的开源技术和丰富的培训，比如”0基础上手机器学习”、”2周培训助你成为深度学习专家”，使得编程变为容易上手的技能，从而导致行业的门槛变低，能力的习得速度也变得很快。另一个方面，从技术演进速度，确实没有哪个行业能与之匹敌。 解决问题的三条出路解决问题要从问题根本入手，从上面的图中按图索骥，找到让两个演变速度慢下来的出路。无论怎么腾挪，都要从狭义的码农职业转换到宏观的码农职业，前人已经总结了给出三条可行路线，概括为3P，即Paper、Politics、PPT。 Paper 路线：简单来说，能够跟学术界搭上点儿关系，在工业界的title就是技术专家。具体的形式包括栖身学术圈在顶级会议上发论文并与大佬们谈笑风生、或者在技术决策上具有团队影响力和话语权的架构师等。这条路线相对来说门槛高一些，需要的扎实的专业技能和灵活的业务建模能力，年龄和经验往往是优势。虽然学术新技术也层出不穷，但如果习得核心技能，职场瓶颈来得并不猛烈。Politics 路线：这条路线，是从生产力岗位转入生产关系岗位，从面向系统到面向人，一般来说就是各种管理岗。这条路性价比高，竞争也激烈，要有意识地构建好自己的能力，才有机会进入。PPT 路线：这条路线，是从写代码的岗位转入项目管理、运营增长的岗位，需要较强的策划和执行能力，比如产品、运营等。 适合于走哪个路线，需要结合自己的性格来定。工程师都有一个共同的性格特点，追求简单，追求完美，思维方式上比较理性和逻辑性，看问题比较趋向于非黑即白。对这类性格的人来说，走技术专家是一条捷径。其面对的是复杂的系统和业务问题，如果能静下心来仔细钻研技术，一定能在某个方面做得比别人好。这个路线对工程师性格的人来说，其实就是在解决简单的复杂问题。 怎么才能算得上技术专家呢？以算法工程师为例，划分为三个演进层次，分别为：图4 算法工程师能力演进的三个层次 在机器学习领域，算法工程师脚下的进阶之路是清晰的：当你掌握了工具、会改造模型，进而可以驾驭新问题的建模，就能成长为最优秀的人才，达到技术专家的水平，中年危机也就离你越来越远。因此，做到技术专家需要具备什么样的能力呢？其能力模型应该是怎么样的呢？下面我们要构建一个形而上的能力框架，培养能力背后的能力。 技术专家的整体能力模型技术专家的整体的能力模型如下图所示：图5 算法工程师能力模型 处于中心的是行为处事的准则，也就是正确的思想观念。其中最核心的观念就是要把职业生涯当作自己的事业，为自己而工作，把提升自身能力作为事业的目标。围绕着这一核心的还包括以结果导向、主动承担责任的观念和既精又专的技术、团队协作能力等。事实上，这些道理大家都明白，但是为何事情还是做不好呢？最主要的原因是大部分人只是领略了道理的表层意思，而并没有将这些道理融入到自己的潜意识里面。在做决策时，也就不能够按照这些道理来处事。因此，首先需要树立正确的观念，并能够将其转化为潜意识，作为本能的一部分。在行动的时候，通过潜意识来指导为人处事的行为准则，就可以保证做正确的事，并且正确的做事。为了能够达到这个效果，需要合理的习惯和技能来保证。习惯和技能相辅相成，影响着平时的一言一行，通过规范言行来帮助将这些观念深入本能。而为了保证上述的方案能够落地执行，需要执行详细的规划并反复实践，并通过复盘等手段总结得失，查缺补漏。大厦需要有坚实的基础，否则只是能空中楼阁。渴望成功的欲望、优秀的执行力和健康的身体便是这一切的基础。 能力模型分解和培养1. 树立正确的观念并转化成本能为自己工作 首先，工作不是为老板工作，而是为自己工作。工作是属于公司的，而职业生涯却是属于你自己的。当把这件事情想明白的时候，你的职业发展将会焕发新的青春。在这个过程中，学习如何像企业一样思考，如何提升自己的技能，让公司持续地购买你的服务。结果导向 公司付钱的目的是要带来价值，你提供的服务最终要能产出结果。公司里面功劳大于苦劳，结果大于过程。正所谓是为过程喝彩为结果付酬。承担责任 责任与重要性呈正比。当你的责任越大，承担的事情越多，公司对你的依赖也就越重，也更能够让你脱颖而出，得到更好的资源和机会。所以，需要主动的承担更多的责任，不要退缩，敢于顶上去。而且勇于承担责任，做出引人注目的成绩，成为问题的解决者，并不断更新自己，也更容易获得晋升。即专又精 要揽瓷器活，得有金刚钻。做事情做深入专一，这样提供的服务才能够足够的优秀，才值得别人pay for money。切忌什么都会，但什么又不懂。团队协作 要相信团队的能量是无穷的，创建一个好的环境，合理的激励措施，好的成长路线。每个人都能激发并释放自己的能量，让优秀的人脱颖而出。 2. 养成良好习惯，提升执行效率在习惯这方面，大概可以分为3部分：（1）第一部分是工作习惯，包括提升执行、保持专注。目标就是提升生产力。在这块无论是方法论还是工具系统都有很多的资料，在这里不再累述。（2）第二部分是学习习惯，包括不断学习、深入思考、持续输出、技术社交。开篇就提到了，码农的技术演变速度很快，所以在这个行业不断学习。学习的重要性大家都知道，而且对于学习的方法大家都各有一套，毕竟都是一路考过来的。但在繁忙的工作中以及飞速发展的技术中，应该学什么以及怎么学还是非常值得研究的。关于学习的这四个习惯是相辅相成的。首先得先有持续学习的主动性，并且不能浮于表面，需要深入本质，思考背后的模式和原理，并举一反三，融会贯通。持续输出是保证深入思考的重要措施，也能够积累自己的技术体系。最后，搞技术不能闭门造车，技术这玩意也需要社交，得接受吸收码神们的指导。 关于学习有两个关键点：第一点是学习方式，我们要谨记的是， 教会他人永远是最高效的学习方式！这个正是费曼学习法的精髓所在。第二点是持续思考。现在层出不穷的技术，比如RNN、LSTM、attention、transformer再到bert，如果只学算法本身，是永远慢人一步的。而如果深入思考背后的原理，则很容易融会贯通。一旦了解得比别人深，就容易看到问题本质，产生信心，激发乐趣。这时候你的解决方案就比别人漂亮，逐渐建立起了影响力，成为了“专家”。因此公司里的疑难杂症会主动找上门来。你就比别人得到了更多的解决问题的机会，从而更快地提升能力。一旦进入良性循环，你的进步就比别人快，但付出的却不一定比别人多。这时候你已经走上了捷径。 （3）第三部分是生活习惯，包括经常健身等。 3. 强化技能，提升生产效率能力建设也是一个体系，共分为3个层面：（1）个人能力：包括系统化思维全面分析问题，拆解问题，逐步分解和执行，并能够有效沟通协调上下游资源把事情做成，拿到结果。事情做完了，需要包装，进行自我营销。（2）团队技能：当个人的贡献有了，负责的范围越来越多，承担的责任越来越大，这个时候得向上管理，和老板把事情讲清楚，获得老板在资源和人力的上的支持。当队伍壮大了，就需要做出应有的贡献，所以团队管理、提升组织效能就变得重要了。（3）进阶技能：做完这些后，就会感觉小有成就，马上就踏入人生巅峰了。但这个时候需要停止当前成功的喜悦，延迟满足，对自己提出更高的要求，来获得更大的成功。提到延迟满足，大家首先想到的可能是著名的“棉花糖”实验。但是这里所说的延迟满足，指的是延迟个人在成功上的满足感，不要止步于当前的成功，多给自己加一些挑战。比如这个项目已经拿到了+5%，那还能不能再提升到+8%，或者解决方案能不能更优美一些，约束能不能少一些。这样不断给自己拔高目标，就能让自己获得更大的成功。 4. 落地执行，保证方案完美执行好的方案需要有完善的机制来保证其落地执行。在具体执行的时候，需要：首先，对培养的习惯和完善的技能拆分下来，并制定可行的条例。比如下方的图： 然后，制定 check 规范并定期复盘和总结，查缺补漏，看自己是否达到预期的目标。下面就是典型的 check 规范，对每个都有具体可落地可执行的标准和准则，在做总结的时候，逐条进行检查，看自己有没有做到。 5. 大厦之基 —— 欲望+执行力+健康完成上面的方法，需要3个根基：成功的欲望、高效的执行、健康的身体。欲望是人类进步的阶梯，在做事情之前，需要渴望成功，才可能成功。佛系的人，随遇而安，得过且过，也是比较难取得超乎常人的成绩的。高效的执行力保证想法能够被很好的实现，只说不做，做的不够好，做的不够高效，一样都不能够完美的得到结果。身体就是所有的一切，所以，程序员们，为了自己，为了公司，为了国家都应该运动起来。 总结不要随心所欲地生活，也不要随遇而安地行走在职业生涯的漫漫长路上。没有明确的方向，你走的每一步都是徒劳的。对工程师来说，没有以不变应万变的方法，唯一不变的就是改变，树立长远的目标，持之以恒，踏实前行，方能达到最终的目标。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>随想</category>
      </categories>
      <tags>
        <tag>干货</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[饿了么 Influxdb 实践之路]]></title>
    <url>%2F2019%2F04%2F16%2Finfluxdb-elema%2F</url>
    <content type="text"><![CDATA[前言Influxdb是一个基于 golang 编写，没有额外依赖的开源时序数据库，用于记录 metrics、events，进行数据分析。这篇文章谈论的 influxdb 版本在1.2.0以上。这篇文章只谈论 influxdb 在监控中的数据存储应用，不会谈论 influxdb 提供的整套监控方案。本文主要谈论五个方面：时序数据库选型、influxdb 基本概念、存储引擎、实践、数据聚合。 选型Influxdb vs Prometheusinfluxdb 集成已有的概念，比如查询语法类似 sql，引擎从 LSM 优化而来，学习成本相对低。influxdb 支持的类型有 float，integers，strings，booleans，prometheus 目前只支持 float。influxdb 的时间精度是纳秒，prometheus 的则是毫秒。influxdb 仅仅是个数据库，而 prometheus 提供的是整套监控解决方案，当然 influxdb 也提供了整套监控解决方案。influxdb 支持的 math function 比较少，prometheus 相对来说更多，influxdb 就目前使用上已经满足功能。2015年 prometheus 还在开发阶段，相对来说 influxdb 更加稳定。influxdb 支持 event log，prometheus 不支持。 更详细的对比请参考：可参考以下做对比https://db-engines.com/en/system/Graphite%3BInfluxDB%3BPrometheus我们其实仅仅需要的是一个数据库，其他组件都是自己开发的，而且存储的数据类型不仅仅是数字，因此选择了 influxdb。希望上面的比较对大家有帮助。 Influxdb 基本概念Database数据库是个逻辑容器，包含了 measurement、retention policies、continuous queries、time series data，类似于 mysql 的 database。 Measurement描述了相关数据的存储结构，类似于 mysql 的 table，但是不需要创建，写入数据的时候自动创建。关于 schema 的设计建议参考：设计建议可参考https://docs.influxdata.com/influxdb/v1.2/concepts/schema_and_data_layout/ Line ProtocolLine Protocol 定义了 influxdb 的数据写入格式，如下：123weather,location=us,server=host1 temperature=82 1465839830100400200 | -------------------- -------------- | | | | | | | | |+-----------+--------+-+---------+-+---------+|measurement_name|,tag_set| |field_set| |timestamp|+-----------+--------+-+---------+-+---------+ Tag上面的 location 和 server 就是 tag key，us 和 host1 是 tag value，tag 是可选的。不过写入数据时最好加上 tag，因为它可以被索引。tag 的类型只能是字符串。 Field上面的 temperature 是 field key，82是 field value。field value 会用于展示，value 支持的类型有 floats，integers，strings，booleans。 Timestamp格式是：RFC3339 UTC。默认精确到纳秒，可选。 Seriesmeasurement, tag set, retention policy 相同的数据集合算做一个 series。理解这个概念至关重要，因为这些数据存储在内存中，如果 series 太多，会导致 OOM。 Retention Policy保留策略包括设置数据保存的时间以及在集群中的副本个数。默认配置是：RP 是 autogen，保留时间是永久，副本为1。这些配置在创建数据库时可以修改。 Continuous QueryCQ 是预先配置好的一些查询命令，定期自动执行这些命令并将查询结果写入指定的 measurement 中，这个功能主要用于数据聚合。具体参考：https://docs.influxdata.com/influxdb/v1.2/query_language/continuous_queries/ Shard存储一定时间间隔的数据，每个目录对应一个 shard，目录的名字就是shard id。每一个 shard 都有自己的 cache、wal、tsm file 以及 compactor，目的就是通过时间来快速定位到要查询数据的相关资源，加速查询的过程，并且也让之后的批量删除数据的操作变得非常简单且高效。 存储引擎概述TSM Tree 是在 LSM Tree 的基础上稍作修改优化而来。它主要包含四个部分：cache、wal、tsm file、compactor。 Cache插入数据时，先往 cache 中写入再写入 wal 中，可以认为 cache 是 wal 文件中的数据在内存中的缓存。 WAL预写日志，对比 mysql 的 binlog。其作用就是为了持久化数据，当系统崩溃后可以通过 wal 文件恢复 cache。 TSM File每个 tsm 文件的大小上限是 2GB。当达到cache-snapshot-memory-size,cache-max-memory-size的限制时会触发将 cache 写入 tsm 文件。 Compactor主要进行两种操作，一种是 cache 数据达到阀值后，进行快照，生成一个新的 tsm 文件。另外一种就是合并当前的 tsm 文件，将多个小的 tsm 文件合并成一个，减少文件的数量，并且进行一些数据删除操作。这些操作都在后台自动完成。 目录结构InfluxDB 的数据存储有三个目录，分别是 meta、wal、data。meta 用于存储数据库的一些元数据，meta目录下有一个 meta.db 文件。wal 目录存放预写日志文件，以 .wal 结尾。data 目录存放实际存储的数据文件，以 .tsm 结尾。基本结构如下：12345-- wal -- test -- autogen -- 1 -- _00001.wal -- 2 -- _00002.wal-- data -- test -- autogen -- 1 -- 000000001-000000001.tsm -- 2 -- 000000001-000000010.tsm-- meta -- meta.db 其中 test 是数据库名称，autogen 是存储策略名称，再下一层目录中的以数字命名的目录是 shard 的 ID 值，比如 autogen 存储策略下有两个 shard，ID 分别为 1 和 2，shard 存储了某一个时间段范围内的数据。再下一级的目录则为具体的文件，分别是 .wal和 .tsm结尾的文件。 更详细的参考InfluxDB 详解之SM 存储引擎解析 实践项目介绍gateway用于检测和压缩influxdb的数据，用于跨机房传输，采用udp接受数据。influxdb-relay是官方提供的高可用方案，但是它只提供简单的写入功能。influxdb-proxy是用于替代 influxdb-relay 的高可用方案。 前期架构图 使用问题influxdb-relay 是官方提供的高可用方案，但是它只提供简单的写入功能。在初期使用时，并没有多大的问题，随着 influxdb 在公司的推广，接入方越来越多，意味着查询方越来越多，这就带来了以下问题：grafana 需要配置很多个数据源。用户不能根据 measurement 来订阅数据。数据库挂掉，就需要修改 grafana 的数据源。维护困难，比如需要新增数据库，用户需要配置多个数据源，不能统一接入点。用户查询直连数据库，用户select *数据库直接 OOM，数据库会重启。relay提供的重写功能，数据是保留在内存中，一旦 influxdb 挂掉，就会导致relay机器内存疯涨。 踩过的坑max-row-limit不为0，会导致 influxdb OOM。目前这个问题已经修复，但是 grafana 展示时会存在问题，配置时请设置为0。配置查询限制参数时，会导致一些奇怪的问题，官方是不限制，请保留默认配置。没有制定 schema 规范，接入方把 field写成 tag 了，导致内存疯涨，最后 OOM。理解 series 的概念很重要。写入超时时间默认是10s，有时候数据写入了但返回 500。可以将这个时间设置成大点。 优化后的架构图 influxdb-proxy 是为了解决上面的使用问题而开发出来的。具有以下功能：同时支持写和查询功能，统一接入点，类似cluster。支持重写功能，写入失败时写入文件，后端恢复时再写入。限制部分查询命令和全部删除操作。以 measurement 为粒度区分数据，支持按需订阅。measurement 优先精确匹配，然后前缀匹配。提供数据统计，比如 qps，耗时等等。 数据聚合CQinfluxdb 提供数据聚合的功能，就是上面基本概念里提到的 Continuous Query。预先定义好cq，就可以定期根据不同的tag进行聚合数据。目前它有个设计问题：cq 是顺序执行的，cq 越多，数据延迟越高，一般延迟在几分钟内。如果需要更实时的聚合，cq 不能满足，需要引入其他工具，比如spark。关于 cq 的语法请参考：https://docs.influxdata.com/influxdb/v1.2/query_language/continuous_queries/ Spark经过内部调研，发现 spark+kafka 是个更好的聚合方案。spark支持流式处理且支持 sql 功能，我们只需要将cq改成sql就行。目前这个处于尝试阶段，已经上线部分功能。目前的处理流程如下： 总结上文讲的整套架构已经支撑起饿了么2万台机器的监控，目前每秒写入的点数是300k。后端 influxdb 的机器数量是20台左右，维护成本基本趋于零。我们的焦点目前已经从 influxdb 转移到数据聚合和分析上。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>influxdb</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>TSDB</tag>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[influxdb高可用层InfluxDB Relay使用介绍]]></title>
    <url>%2F2019%2F04%2F16%2Finflux-relay-introduce%2F</url>
    <content type="text"><![CDATA[前言influx-relay作为influxdb的高可用层最初由influxdata创建提出，但之后不再维护。其架构图如下：图中负载设备（load Balancer）常用nginx,对于读请求直接转发到inflxudb，对于写请求转发到relay图中是通过地址转发，也可以通过域名来转发 1234567891011121314151617181920212223242526272829 ┌─────────────────┐ │writes &amp; queries │ └─────────────────┘ │ ▼ ┌───────────────┐ │ │ ┌────────│ Load Balancer │─────────┐ │ │ │ │ │ └──────┬─┬──────┘ │ │ │ │ │ │ │ │ │ │ ┌──────┘ └────────┐ │ │ │ ┌─────────────┐ │ │┌──────┐│ │ │/write or UDP│ │ ││/query││ ▼ └─────────────┘ ▼ │└──────┘│ ┌──────────┐ ┌──────────┐ │ │ │ InfluxDB │ │ InfluxDB │ │ │ │ Relay │ │ Relay │ │ │ └──┬────┬──┘ └────┬──┬──┘ │ │ │ | | │ │ │ | ┌─┼──────────────┘ | │ │ │ │ └──────────────┐ │ │ │ ▼ ▼ ▼ ▼ │ │ ┌──────────┐ ┌──────────┐ │ │ │ │ │ │ │ └─▶│ InfluxDB │ │ InfluxDB │◀─┘ │ │ │ │ └──────────┘ └──────────┘ 1.安装golang环境2.下载编译influxdb-relayhttps://github.com/influxdata/influxdb-relay 3.编辑配置文件relay.toml12345678910111213141516171819202122232425262728293031323334353637[[http]]# Name of the HTTP server, used for display purposes only.name = "example-http"# TCP address to bind to, for HTTP server.bind-addr = "127.0.0.1:9096"# Array of InfluxDB instances to use as backends for Relay.output = [ # name: name of the backend, used for display purposes only. # location: full URL of the /write endpoint of the backend # timeout: Go-parseable time duration. Fail writes if incomplete in this time. &#123; name="local1", location="http://127.0.0.1:8086/write", timeout="10s" &#125;, &#123; name="local2", location="http://127.0.0.1:7086/write", timeout="10s" &#125;,][[udp]]# Name of the UDP server, used for display purposes only.name = "example-udp"# UDP address to bind to.bind-addr = "127.0.0.1:9096"# Socket buffer size for incoming connections.read-buffer = 0 # default# Precision to use for timestampsprecision = "n" # Can be n, u, ms, s, m, h# Array of InfluxDB instances to use as backends for Relay.output = [ # name: name of the backend, used for display purposes only. # location: host and port of backend. # mtu: maximum output payload size &#123; name="local1", location="127.0.0.1:8089", mtu=512 &#125;, &#123; name="local2", location="127.0.0.1:7089", mtu=1024 &#125;,] 4.运行：1nohup ./influxdb-relay -config relay.toml &amp; 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>influxdb</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>TSDB</tag>
        <tag>高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[influxdb-relay性能瓶颈分析]]></title>
    <url>%2F2019%2F04%2F14%2Finfluxdb-relay-optimize%2F</url>
    <content type="text"><![CDATA[最近使用influxdb的方案做项目的TSDB，考虑到influxdb的多活设计，我们找到了influxdb提供的一款高可用层influxdb-relay，这个开源项目是influxdb提出来的，后提供出来很久不再维护了。我们在做单点influxdb测试的时候，整体表现良好。加入influxdb-relay后在做大数据量测试的时候发现有很大问题，特别是在插入的时候内存消耗很大，而且插入速度也会变慢，出现了严重的写问题。因为influxdb-relay是用go语言编写的，在网上找了很多资料，找到以下资料后按照步骤对源码做了修改，重新做大数据量测试表现良好。现贡献给使用influxdb-relay的各位。 起因为了让influxdb能够达到高可用，我便考虑在influxdb外面套一层，比如nginx。然而发现官方已经开发了一个influxdb-relay的东西，于是决定索性使用这个东西。 首先看一张官方README.md中的图。 1234567891011121314151617181920212223242526272829 ┌─────────────────┐ │writes &amp; queries │ └─────────────────┘ │ ▼ ┌───────────────┐ │ │ ┌────────│ Load Balancer │─────────┐ │ │ │ │ │ └──────┬─┬──────┘ │ │ │ │ │ │ │ │ │ │ ┌──────┘ └────────┐ │ │ │ ┌─────────────┐ │ │┌──────┐│ │ │/write or UDP│ │ ││/query││ ▼ └─────────────┘ ▼ │└──────┘│ ┌──────────┐ ┌──────────┐ │ │ │ InfluxDB │ │ InfluxDB │ │ │ │ Relay │ │ Relay │ │ │ └──┬────┬──┘ └────┬──┬──┘ │ │ │ | | │ │ │ | ┌─┼──────────────┘ | │ │ │ │ └──────────────┐ │ │ │ ▼ ▼ ▼ ▼ │ │ ┌──────────┐ ┌──────────┐ │ │ │ │ │ │ │ └─▶│ InfluxDB │ │ InfluxDB │◀─┘ │ │ │ │ └──────────┘ └──────────┘ influxdb-relay只做数据的冗余写入，并在后端的influxdb宕机时，将数据存储在内存，当influxdb恢复时，将宕机期间的数据重新写回influxdb。客户端在访问influxdb的时候，实际访问的是一个 Load Balancer 比如 Nginx , 然后 Load Balancer 根据不同的path，选择influxdb或者influxdb-relay。 然而，这influxdb-relay的性能却很奇怪。1234567891011121314151617POST /writeRequests [total, rate] 180000, 3000.02Duration [total, attack, wait] 1m20.11586256s, 59.999606572s, 20.116255988sLatencies [mean, 50, 95, 99, max] 1.404856127s, 1.214236ms, 5.609988425s, 43.430687195s, 50.347228848sBytes In [total, mean] 0, 0.00Bytes Out [total, mean] 3958680, 21.99Success [ratio] 54.98%Status Codes [code:count] 204:98967 0:81033GET /pingRequests [total, rate] 180000, 3000.02Duration [total, attack, wait] 59.999785977s, 59.999606599s, 179.378µsLatencies [mean, 50, 95, 99, max] 150.317µs, 145.68µs, 166.688µs, 194.65µs, 2.344456msBytes In [total, mean] 0, 0.00Bytes Out [total, mean] 0, 0.00Success [ratio] 100.00%Status Codes [code:count] 204:180000 过程在之前的文章里面提过，最新版influxdb，即便是在 vegeta 5000/s 的攻击下，还是能保证99%以上的成功率， 然而 influxdb-relay 却在 3000/s 的速度的时候就已经撑不住了。作为influxdb的前置程序，至少要达到或者 超过influxdb的处理性能，才算合理吧。所以我做了个对比试验，在处理Http请求的 http.go 里加入了 ping 处理逻辑。经过测试，可以看出，go的http库，应该不慢，所以还是在relay处理write的逻辑上有问题。 /ping 这个url官方的代码里面，没有加入，这是我在 relay/http.go 加的，代码如下:123456789func (h *HTTP) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; start := time.Now() if r.URL.Path == "/ping" &amp;&amp; (r.Method == "GET" || r.Method == "HEAD") &#123; w.Header().Add("X-InfluxDB-Version", "relay") w.WriteHeader(http.StatusNoContent) return &#125;&#125; 实际这段代码，根据 influxdb-relay 的设计理念是不需要的。 再回看一下vegeta的report中的Error Set:123456Error Set:Post http://127.0.0.1:9096/write?db=test: dial tcp 0.0.0.0:0-&gt;127.0.0.1:9096: bind: can't assign requested addressPost http://127.0.0.1:9096/write?db=test: dial tcp 0.0.0.0:0-&gt;127.0.0.1:9096: socket: too many open filesPost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:59803-&gt;127.0.0.1:9096: read: connection reset by peerPost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:59805-&gt;127.0.0.1:9096: read: connection reset by peerPost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:59806-&gt;127.0.0.1:9096: read: connection reset by peer 这里既有 socket: too many open files , 又有 read: connection reset by peer , 还有 bind: can’t assign requested address , 然后结合代码。123456789101112131415161718192021222324252627var responses = make(chan *responseData, len(h.backends))for _, b := range h.backends &#123; b := b go func() &#123; defer wg.Done() resp, err := b.post(outBytes, query, authHeader) if err != nil &#123; log.Printf("Problem posting to relay %q backend %q: %v", h.Name(), b.name, err) &#125; else &#123; if resp.StatusCode/100 == 5 &#123; log.Printf("5xx response for relay %q backend %q: %v", h.Name(), b.name, resp.StatusCode) &#125; responses &lt;- resp &#125; &#125;()&#125;go func() &#123; wg.Wait() close(responses) putBuf(outBuf)&#125;()var errResponse *responseDatafor resp := range responses &#123; 首先这里，开了一个 channel , var responses = make(chan *responseData, len(h.backends)) , 只有当 所有的backends都回复了之后，至二个 responses channel 才会关闭，客户端才能拿到结果，然而一旦某一个 backends卡壳了，就要等待go的http client timeout了，这个timeout默认时间是10s, 相当于说客户端至少要等待 10s，然而实际并不止这样。在看看 retry.go 中的部分代码:12345678910111213141516171819interval := r.initialIntervalfor &#123; resp, err := r.p.post(buf.Bytes(), batch.query, batch.auth) if err == nil &amp;&amp; resp.StatusCode/100 != 5 &#123; batch.resp = resp atomic.StoreInt32(&amp;r.buffering, 0) batch.wg.Done() break &#125; if interval != r.maxInterval &#123; interval *= r.multiplier if interval &gt; r.maxInterval &#123; interval = r.maxInterval &#125; &#125; time.Sleep(interval)&#125; 当超时等statusCode &gt;= 500的错误发生时，retry会将这个请求加入bufer中，然后由run方法获取batch并向后端influxdb请求。这时的逻辑是，一旦请求失败，就sleep一定时间，而这个一定时间就是初始时间乘以一个放大因子，放大因子默认是2，于是客户端 就会在不断等待中，最后超时。而在vegeta疯狂的攻击下，是经不起等待的。所以我改了下http.go中的逻辑，客户端请求后，直接 返回204，让客户端不再等待。123(&amp;responseData&#123; StatusCode: 204,&#125;).Write(w) 删除 responses channel , 以及对应的代码。 貌似有了一定的改善。1234567891011121314151617181920Requests [total, rate] 180000, 3000.02Duration [total, attack, wait] 1m17.299212505s, 59.999606586s, 17.299605919sLatencies [mean, 50, 95, 99, max] 672.645729ms, 185.598µs, 345.300005ms, 30.003589182s, 36.777965011sBytes In [total, mean] 0, 0.00Bytes Out [total, mean] 6231240, 34.62Success [ratio] 86.55%Status Codes [code:count] 204:155781 0:24219 Error Set:Post http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:57421-&gt;127.0.0.1:9096: read: connection reset by peerPost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:57406-&gt;127.0.0.1:9096: read: connection reset by peerPost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:57407-&gt;127.0.0.1:9096: read: connection reset by peerPost http://127.0.0.1:9096/write?db=test: write tcp 127.0.0.1:57404-&gt;127.0.0.1:9096: write: broken pipePost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:57399-&gt;127.0.0.1:9096: read: connection reset by peerPost http://127.0.0.1:9096/write?db=test: write tcp 127.0.0.1:57413-&gt;127.0.0.1:9096: write: broken pipePost http://127.0.0.1:9096/write?db=test: write tcp 127.0.0.1:57418-&gt;127.0.0.1:9096: write: broken pipePost http://127.0.0.1:9096/write?db=test: write tcp 127.0.0.1:57416-&gt;127.0.0.1:9096: write: broken pipePost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:57398-&gt;127.0.0.1:9096: read: connection reset by peerPost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:57396-&gt;127.0.0.1:9096: read: connection reset by peerPost http://127.0.0.1:9096/write?db=test: write tcp 127.0.0.1:57402-&gt;127.0.0.1:9096: write: broken pipePost http://127.0.0.1:9096/write?db=test: read tcp 127.0.0.1:57415-&gt;127.0.0.1:9096: read: connection reset by peer 但是还是很糟糕，毕竟之前influxdb的数据与这个还是有一定差距的。 于是我把目光放到的 retry.go 中12345678910111213141516171819func (r *retryBuffer) post(buf []byte, query string, auth string) (*responseData, error) &#123; if atomic.LoadInt32(&amp;r.buffering) == 0 &#123; resp, err := r.p.post(buf, query, auth) // TODO A 5xx caused by the point data could cause the relay to buffer forever if err == nil &amp;&amp; resp.StatusCode/100 != 5 &#123; return resp, err &#125; atomic.StoreInt32(&amp;r.buffering, 1) &#125; // already buffering or failed request batch, err := r.list.add(buf, query, auth) if err != nil &#123; return nil, err &#125; batch.wg.Wait() return batch.resp, nil&#125; 如果没有buffering那么，直接发送请求给influxdb，不然就把请求放到buffer中，如果buffer满了，就返回错误。既然已经在客户端那边 直接返回了204那么，这个没有buffer的raw的请求就没有必要再单独处理了，索性一并放到buffer中去，buffer有一个好处，就是能把多个 请求合并成一个请求提交给后端的influxdb，这样就能减少请求次数了。代码改成如下：123456789func (r *retryBuffer) post(buf []byte, query string, auth string) (*responseData, error) &#123; batch, err := r.list.add(buf, query, auth) if err != nil &#123; return nil, err &#125; batch.wg.Wait() return batch.resp, nil&#125; 用2000/s速度测试，结果如下：12345678Requests [total, rate] 120000, 2000.02Duration [total, attack, wait] 1m0.000271382s, 59.999499926s, 771.456µsLatencies [mean, 50, 95, 99, max] 304.395µs, 259.447µs, 460.682µs, 1.044402ms, 42.391318msBytes In [total, mean] 0, 0.00Bytes Out [total, mean] 4800000, 40.00Success [ratio] 100.00%Status Codes [code:count] 204:120000 Error Set: 其实我没法用更快的速度测试，如果是3000/s，那么就会出下面的问题。12345678910111213142016/08/13 17:52:22 starting relays...2016/08/13 17:52:22 Starting HTTP relay "example-http" on 127.0.0.1:9096panic: runtime error: invalid memory address or nil pointer dereference[signal 0xb code=0x1 addr=0x0 pc=0x837d8]goroutine 38179 [running]:panic(0x370fc0, 0xc820014200) /Users/shane/.gvm/gos/go1.6.2/src/runtime/panic.go:481 +0x3e6github.com/influxdata/influxdb-relay/relay.(*retryBuffer).post(0xc820010b90, 0xc8202de254, 0x3c, 0x40, 0xc820393700, 0x7, 0x0, 0x0, 0xc82002d500, 0x0, ...) /Users/shane/Documents/gosrc/influxdb-relay/src/github.com/influxdata/influxdb-relay/relay/retry.go:56 +0x118github.com/influxdata/influxdb-relay/relay.(*HTTP).ServeHTTP.func1(0xc820393710, 0xc8200c9ce0, 0xc8202de254, 0x3c, 0x40, 0xc820393700, 0x7, 0x0, 0x0, 0xc820022280) /Users/shane/Documents/gosrc/influxdb-relay/src/github.com/influxdata/influxdb-relay/relay/http.go:210 +0xe8created by github.com/influxdata/influxdb-relay/relay.(*HTTP).ServeHTTP /Users/shane/Documents/gosrc/influxdb-relay/src/github.com/influxdata/influxdb-relay/relay/http.go:218 +0xce6 这块地方正是我修改的代码，而出错的那行是这样的:1batch.wg.Wait() invalid memory address or nil , 我在这行代码前面加几行。1234if batch == nil &#123; log.Print("batch is nil")&#125;batch.wg.Wait() 果然打出了日志12016/08/13 18:06:28 batch is nil 这个错误很有意思了，batch是通过 bufferList 的 add 方法得到，并且在方法的末尾，有空值检查。123456789101112if *cur == nil &#123; // new tail element *cur = newBatch(buf, query, auth)&#125; else &#123; // append to current batch b := *cur b.size += len(buf) b.bufs = append(b.bufs, buf)&#125;l.cond.L.Unlock()return *cur, nil 首先要排除，我的修改有没有问题，把代码回退，用2000/s的速度测试。但是很不幸，这个速度会让influxdb-relay直接挂起，所以索性把 http.go 请求influxdb的代码改了。123456789101112func (b *simplePoster) post(buf []byte, query string, auth string) (*responseData, error) &#123; time.Sleep(time.Microsecond * time.Duration(rand.Intn(400))) if auth == "hello" &#123; return &amp;responseData&#123; StatusCode: 204, &#125;, nil &#125; else &#123; return &amp;responseData&#123; StatusCode: 502, &#125;, nil &#125;&#125; 这里要模拟一个场景：第一次请求的时候均失败，在run方法请求的时候均成功，time.Sleep模拟请求耗时。为了甄别请求的调用者，这里在auth这个参数上做了点文章。所以要修改下 retry.go 中的 run 方法的调用，把 “hello” 作为参数传递给 SimplePoster.post 方法。123for &#123; resp, err := r.p.post(buf.Bytes(), batch.query, "hello") if err == nil &amp;&amp; resp.StatusCode/100 != 5 &#123; 然后用2000/s的速度测试，果然出问题了。12345678910111213142016/08/14 09:11:40 starting relays...2016/08/14 09:11:40 Starting HTTP relay "example-http" on 127.0.0.1:9096panic: runtime error: invalid memory address or nil pointer dereference[signal 0xb code=0x1 addr=0x0 pc=0x83463]goroutine 77131 [running]:panic(0x370cc0, 0xc820014200) /Users/shane/.gvm/gos/go1.6.2/src/runtime/panic.go:481 +0x3e6github.com/influxdata/influxdb-relay/relay.(*retryBuffer).post(0xc820010b90, 0xc820164000, 0x3c, 0x200, 0xc8205efbb0, 0x7, 0x0, 0x0, 0xc82002c000, 0x0, ...) /Users/shane/Documents/gosrc/influxdb-relay/src/github.com/influxdata/influxdb-relay/relay/retry.go:66 +0x273github.com/influxdata/influxdb-relay/relay.(*HTTP).ServeHTTP.func1(0xc8205efbc0, 0xc8200d5d00, 0xc820164000, 0x3c, 0x200, 0xc8205efbb0, 0x7, 0x0, 0x0, 0xc820022280) /Users/shane/Documents/gosrc/influxdb-relay/src/github.com/influxdata/influxdb-relay/relay/http.go:211 +0xe8created by github.com/influxdata/influxdb-relay/relay.(*HTTP).ServeHTTP /Users/shane/Documents/gosrc/influxdb-relay/src/github.com/influxdata/influxdb-relay/relay/http.go:219 +0xce6 然后把用来模拟http请求耗时的time.Sleep去掉，异常又不发生了。以我这三脚猫的go语言功底，一时间难以发现错误的原因，但是直觉很重要。我在 BufferList.add 的 l.cond.L.Unlock 后面加了一个 time.Sleep , 情况会怎样呢。12345678func (l *bufferList) add(buf []byte, query string, auth string) (*batch, error) &#123; // ... l.cond.L.Unlock() time.Sleep(time.Microsecond * time.Duration(rand.Intn(100))) return *cur, nil&#125; 启动之后，一请求就把报错。 经过一番仔细思考，我得出一个结论。 BufferList.add 方法返回了执行 Batch 的指针，而 Unlock 之后， BufferList.pop 方法就会改变 BufferList 中数据的，这时候post方法中，获取的地址指向的 Batch 已经被 pop 方法改变，很可能已经是nil，所以就报错了。知道了原因修改起来就相对容易了，把 Unlock 调用置后，在 return 之后，也就是 post 方法中获取到值之后，再 Unlock 。1234567func (l *bufferList) add(buf []byte, query string, auth string) (*batch, error) &#123; // ... defer l.cond.L.Unlock() return *cur, nil&#125; 测试之后果然没有再出现之前的错误了。 回到之前的故事。我把所有的请求都扔到了 BufferList 中，这样由于发送速度相对较快，那么必然出现请求合并的场景，这样减少请求次数，增加influxdb的稳定性。但是当Buffer满的时候，这种情况在请求速度大于消费速度(比如influxdb宕机)的情况下就会发生。如果按照之前的逻辑，那么客户端是不知道自己的这次请求因为 BufferList 满了，而没有成功。为了解决这个问题，我把 http.go 中用来处理response的代码，加回来，并修改了 retry.go 中的 post 方法。123456789101112131415161718func (r *retryBuffer) post(buf []byte, query string, auth string) (*responseData, error) &#123; pb := getBuf() pb.Write(buf) batch, err := r.list.add(pb.Bytes(), query, auth) if err != nil &#123; putBuf(pb) return nil, err &#125; go func() &#123; batch.wg.Wait() putBuf(pb) &#125;() return &amp;responseData&#123; StatusCode: 204, &#125;, nil&#125; 下面分别是 vegeta 在3000/s, 5000/s, 10000/s的测试结果12345678910111213141516171819202122232425262728293031323334353637383940414243444546Requests [total, rate] 180000, 3000.02Duration [total, attack, wait] 59.999890163s, 59.999606586s, 283.577µsLatencies [mean, 50, 95, 99, max] 290.602µs, 232.224µs, 402.502µs, 1.371521ms, 16.056569msBytes In [total, mean] 0, 0.00Bytes Out [total, mean] 7200000, 40.00Success [ratio] 100.00%Status Codes [code:count] 204:180000 Error Set:&gt; select count(value) from cpuname: cpu---------time count0 180000Requests [total, rate] 300000, 5000.02Duration [total, attack, wait] 1m0.000013963s, 59.999799896s, 214.067µsLatencies [mean, 50, 95, 99, max] 258.591µs, 191.622µs, 350.592µs, 1.479882ms, 14.940625msBytes In [total, mean] 0, 0.00Bytes Out [total, mean] 12000000, 40.00Success [ratio] 100.00%Status Codes [code:count] 204:300000 Error Set:&gt; select count(value) from cpuname: cpu---------time count0 299997Requests [total, rate] 600000, 10000.02Duration [total, attack, wait] 1m0.000158017s, 59.999899912s, 258.105µsLatencies [mean, 50, 95, 99, max] 329.228µs, 185.111µs, 745.028µs, 4.522189ms, 18.195853msBytes In [total, mean] 0, 0.00Bytes Out [total, mean] 24000000, 40.00Success [ratio] 100.00%Status Codes [code:count] 204:600000 Error Set:&gt; select count(value) from cpuname: cpu---------time count0 599989 我的influxdb承受不了vegeta 6000/s以上的攻击，而现在套了influxdb-relay之后就能承受10000/s+的攻击了，虽然真实场景可能更为复杂， 尤其是读和写都会发生的情况，单从上面的实验可以看出修改版的influxdb-relay已经基本能满足需求了。 参考文档：https://xusheng.org/blog/2016/08/12/influxdb-relay-performance-bottle-neck-analysing/ 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>influxdb</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>数据库</tag>
        <tag>TSDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抽象类和接口的区别]]></title>
    <url>%2F2019%2F04%2F14%2Fabstract-inner-class%2F</url>
    <content type="text"><![CDATA[我们在多态的学习过程中认识到抽象类和接口都是实现java多态特性的关键部分，两者都包含抽象方法，只关注方法的声明而不关注方法的具体实现，那么这两者又有什么区别呢？？我们在编写java程序的时候又该如何抉择呢？ 1.语法层面上的区别 1.一个类只能继承一个抽象类，而一个类却可以实现多个接口。 2.抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的；且必须给其初值，所以实现类中不能重新定义，也不能改变其值；抽象类中的变量默认是 friendly 型，其值可以在子类中重新定义，也可以重新赋值。 3.抽象类中可以有非抽象方法，接口中则不能有非抽象方法。 4.接口可以省略abstract 关键字，抽象类不能。 5.接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法； 2.设计层面上的区别 1）抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。 举个简单的例子，飞机和鸟是不同类的事物，但是它们都有一个共性，就是都会飞。那么在设计的时候，可以将飞机设计为一个类Airplane，将鸟设计为一个类Bird，但是不能将飞行这个特性也设计为类，因此它只是一个行为特性，并不是对一类事物的抽象描述。 此时可以将 飞行 设计为一个接口Fly，包含方法fly( )，然后Airplane和Bird分别根据自己的需要实现Fly这个接口。然后至于有不同种类的飞机，比如战斗机、民用飞机等直接继承Airplane即可，对于鸟也是类似的，不同种类的鸟直接继承Bird类即可。 从这里可以看出，继承是一个 “是不是”的关系，而 接口 实现则是 “有没有”的关系。 如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是有没有、具备不具备的关系，比如鸟是否能飞（或者是否具备飞行这个特点），能飞行则可以实现这个接口，不能飞行就不实现这个接口。 2）设计层面不同，抽象类作为很多子类的父类，它是一种模板式设计。而接口是一种行为规范，它是一种辐射式设计。 什么是模板式设计？ 最简单例子，大家都用过ppt里面的模板，如果用模板A设计了ppt B和ppt C，ppt B和ppt C公共的部分就是模板A了，如果它们的公共部分需要改动，则只需要改动模板A就可以了，不需要重新对ppt B和ppt C进行改动。 辐射式设计: 比如某个电梯都装了某种报警器，一旦要更新报警器，就必须全部更新。也就是说对于抽象类，如果需要添加新的方法，可以直接在抽象类中添加具体的实现，子类可以不进行变更； 而对于接口则不行，如果接口进行了变更，则所有实现这个接口的类都必须进行相应的改动。 下面看一个网上流传最广泛的例子：门和警报的例子：门都有open( )和close( )两个动作，此时我们可以定义通过抽象类和接口来定义这个抽象概念：1234abstract class Door &#123; public abstract void open(); public abstract void close();&#125; 或者：1234interface Door &#123; public abstract void open(); public abstract void close();&#125; 但是现在如果我们需要门具有报警alarm( )的功能，那么该如何实现？下面提供两种思路： 1）将这三个功能都放在抽象类里面，但是这样一来所有继承于这个抽象类的子类都具备了报警功能，但是有的门并不一定具备报警功能； 2）将这三个功能都放在接口里面，需要用到报警功能的类就需要实现这个接口中的open( )和close( )，也许这个类根本就不具备open( )和close( )这两个功能，比如火灾报警器。 从这里可以看出，Door的open()、close()和alarm()根本就属于两个不同范畴内的行为，open()和close()属于门本身固有的行为特性，而alarm()属于延伸的附加行为。 因此最好的解决办法是单独将报警设计为一个接口，包含alarm()行为,Door设计为单独的一个抽象类，包含open和close两种行为。再设计一个报警门继承Door类和实现Alarm接口。 1234567891011121314151617181920interface Alram &#123; void alarm();&#125; abstract class Door &#123; void open(); void close();&#125; class AlarmDoor extends Door implements Alarm &#123; void oepn() &#123; //.... &#125; void close() &#123; //.... &#125; void alarm() &#123; //.... &#125;&#125; 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础内部类详解]]></title>
    <url>%2F2019%2F04%2F14%2Finner-class%2F</url>
    <content type="text"><![CDATA[一.成员内部类 内部类中最常见的就是成员内部类，也称为普通内部类。我们来看如下代码：运行结果为： 从上面的代码中我们可以看到，成员内部类的使用方法： 1.Inner 类定义在 Outer 类的内部，相当于 Outer 类的一个成员变量的位置，Inner 类可以使用任意访问控制符，如 public 、 protected 、 private 等 2.Inner 类中定义的 test() 方法可以直接访问 Outer 类中的数据，而不受访问控制符的影响，如直接访问 Outer 类中的私有属性a 3.定义了成员内部类后，必须使用外部类对象来创建内部类对象，而不能直接去 new 一个内部类对象，即：内部类 对象名 = 外部类对象.new 内部类( ); 4.编译上面的程序后，会发现产生了两个.class文件 其中，第二个是外部类的 .class 文件，第一个是内部类的 .class 文件，即成员内部类的 .class 文件总是这样：外部类名$内部类名.class 另外，友情提示哦： 1.外部类是不能直接使用内部类的成员和方法滴。如： 那么外部类如何使用内部类的成员和方法呢？？ 答：可先创建内部类的对象，然后通过内部类的对象来访问其成员变量和方法。 Java 编译器在创建内部类对象时，隐式的把其外部类对象的引用也传了进去并一直保存着。这样就使得内部类对象始终可以访问其外部类对象，同时这也是为什么在外部类作用范围之外向要创建内部类对象必须先创建其外部类对象的原因。 2.如果外部类和内部类具有相同的成员变量或方法，内部类默认访问自己的成员变量或方法，如果要访问外部类的成员变量，可以使用 this 关键字。如： 运行结果： 二.静态内部类 静态内部类是 static 修饰的内部类，这种内部类的特点是： 1.静态内部类不能直接访问外部类的非静态成员，但可以通过 new 外部类().成员 的方式访问。 2.如果外部类的静态成员与内部类的成员名称相同，可通过“类名.静态成员”访问外部类的静态成员；如果外部类的静态成员与内部类的成员名称不相同，则可通过“成员名”直接调用外部类的静态成员。 3.创建静态内部类的对象时，不需要外部类的对象，可以直接创建 内部类 对象名= new 内部类();运行结果: 三.方法内部类 方法内部类就是内部类定义在外部类的方法中，方法内部类只在该方法的内部可见，即只在该方法内可以使用。 注意：由于方法内部类不能在外部类的方法以外的地方使用，因此方法内部类不能使用访问控制符和static修饰符。 匿名内部类 匿名类是不能有名称的类，所以没办法引用他们。必须在创建时，作为new语句的一部分来声明他们。但使用匿名内部类还有个前提条件：必须继承一个父类或实现一个接口。这就要采用另一种形式的new语句，如：new &lt;类或接口&gt; &lt;类的主体&gt;这种形式的new语句声明一个新的匿名类，他对一个给定的类进行扩展，或实现一个给定的接口。他还创建那个类的一个新实例，并把他作为语句的结果而返回。要扩展的类和要实现的接口是 new语句的操作数，后跟匿名类的主体。 注意匿名类的声明是在编译时进行的，实例化在运行时进行。这意味着 for循环中的一个new语句会创建相同匿名类的几个实例，而不是创建几个不同匿名类的一个实例。从技术上说，匿名类可被视为非静态的内部类，所以他们具备和方法内部声明的非静态内部类相同的权限和限制。 假如要执行的任务需要一个对象，但却不值得创建全新的对象（原因可能是所需的类过于简单，或是由于他只在一个方法内部使用），匿名类就显得很有用。匿名类尤其适合在Swing应用程式中快速创建事件处理程式。以下是一个匿名内部类的实例： 1.匿名内部类的基本实现：运行结果：可以看到，我们直接将抽象类Person中的方法在大括号中实现了，这样便可以省略一个类的书写，并且，匿名内部类还能用于接口上。 2.在接口上使用匿名内部类：运行结果：由上面的例子可以看出，只要一个类是抽象的或是一个接口，那么其子类中的方法都可以使用匿名内部类来实现。 在使用匿名内部类的过程中，我们需要注意如下几点： 1.使用匿名内部类时，我们必须是继承一个类或者实现一个接口，但是两者不可兼得，同时也只能继承一个类或者实现一个接口。 2.匿名内部类中是不能定义构造函数的。 3.匿名内部类中不能存在任何的静态成员变量和静态方法。 4.匿名内部类为局部内部类（即方法内部类），所以局部内部类的所有限制同样对匿名内部类生效。 5.匿名内部类不能是抽象的，它必须要实现继承的类或者实现的接口的所有抽象方法。 四.匿名内部类重点难点：1.如果是在一个方法的匿名内部类，可以利用这个方法传进你想要的参数，不过记住，这些参数必须被声明为final 。 使用的形参为何要为final？？ 我们给匿名内部类传递参数的时候，若该形参在内部类中需要被使用，那么该形参必须要为final。也就是说：当所在的方法的形参需要被内部类里面使用时，该形参必须为final。 首先我们知道在内部类编译成功后，它会产生一个class文件，该class文件与外部类并不是同一class文件，仅仅只保留对外部类的引用。当外部类传入的参数需要被内部类调用时，从java程序的角度来看是直接被调用： 123456789public class OuterClass &#123; public void display(final String name,String age)&#123; class InnerClass&#123; void display()&#123; System.out.println(name); &#125; &#125; &#125; &#125; 从上面代码中看好像name参数应该是被内部类直接调用？其实不然，在java编译之后实际的操作如下：12345678910public class OuterClass$InnerClass &#123; public InnerClass(String name,String age)&#123; this.InnerClass$name = name; this.InnerClass$age = age; &#125; public void display()&#123; System.out.println(this.InnerClass$name + "----" + this.InnerClass$age ); &#125; &#125; 所以从上面代码来看，内部类并不是直接调用方法传递的参数，而是利用自身的构造器对传入的参数进行备份，自己内部方法调用的实际上时自己的属性而不是外部方法传递进来的参数。 直到这里还没有解释为什么是final。在内部类中的属性和外部方法的参数两者从外表上看是同一个东西，但实际上却不是，所以他们两者是可以任意变化的，也就是说在内部类中我对属性的改变并不会影响到外部的形参，而然这从程序员的角度来看这是不可行的，毕竟站在程序的角度来看这两个根本就是同一个，如果内部类该变了，而外部方法的形参却没有改变这是难以理解和不可接受的，所以为了保持参数的一致性，就规定使用final来避免形参的不改变。 简单理解就是，拷贝引用，为了避免引用值发生改变，例如被外部类的方法修改等，而导致内部类得到的值不一致，于是用final来让该引用不可改变。故如果定义了一个匿名内部类，并且希望它使用一个其外部定义的参数，那么编译器会要求该参数引用是final的。 2.匿名内部类中使用初始化代码块 我们一般都是利用构造器来完成某个实例的初始化工作的，但是匿名内部类是没有构造器的！那怎么来初始化匿名内部类呢？使用构造代码块！利用构造代码块能够达到为匿名内部类创建一个构造器的效果。1234567891011121314151617181920212223242526272829303132public class OutClass &#123; public InnerClass getInnerClass(final int age,final String name)&#123; return new InnerClass() &#123; int age_ ; String name_; //构造代码块完成初始化工作 &#123; if(0 &lt; age &amp;&amp; age &lt; 200)&#123; age_ = age; name_ = name; &#125; &#125; public String getName() &#123; return name_; &#125; public int getAge() &#123; return age_; &#125; &#125;; &#125; public static void main(String[] args) &#123; OutClass out = new OutClass(); InnerClass inner_1 = out.getInnerClass(201, "chenssy"); System.out.println(inner_1.getName()); InnerClass inner_2 = out.getInnerClass(23, "chenssy"); System.out.println(inner_2.getName()); &#125; &#125; 五、内部类总结 学习了上面四种类型的内部类，我们知道了如何使用各个内部类，那么为什么要使用内部类呢？？ 首先举一个简单的例子，如果你想实现一个接口，但是这个接口中的一个方法和你构想的这个类中的一个 方法的名称，参数相同，你应该怎么办？ 这时候，你可以建一个内部类实现这个接口。由于内部类对外部类的所有内容都是可访问的，所以这样做可以完成所有你直 接实现这个接口的功能。 不过你可能要质疑，更改一下方法的不就行了吗？的确，以此作为设计内部类的理由，实在没有说服力。 真正的原因是这样的，java中的内部类和接口加在一起，可以的解决常被C++程序员抱怨java中存在的一个问题——没有多继承。实际上，C++的多继承设计起来很复杂，而java通过内部类加上接口，可以很好的实现多继承的效果。 内部类：一个内部类的定义是定义在另一个内部的类。 原因是： 1.一个内部类的对象能够访问创建它的对象的实现，包括私有数据。 2.对于同一个包中的其他类来说，内部类能够隐藏起来。 3.匿名内部类可以很方便的定义回调。 4.使用内部类可以非常方便的编写事件驱动程序。 内部类可以让你更优雅地设计你的程序结构。下面从以下几个方面来介绍： 首先看这个例子：12345678910111213141516171819202122232425262728293031323334353637public interface Contents &#123; int value(); &#125; public interface Destination &#123; String readLabel(); &#125;``` ```bashpublic class Goods &#123; private valueRate=2; private class Content implements Contents &#123; private int i = 11 * valueRate; public int value() &#123; return i; &#125; &#125; protected class GDestination implements Destination &#123; private String label; private GDestination(String whereTo) &#123; label = whereTo; &#125; public String readLabel() &#123; return label; &#125; &#125; public Destination dest(String s) &#123; return new GDestination(s); &#125; public Contents cont() &#123; return new Content(); &#125; &#125; 在这个例子里类 Content 和 GDestination 被定义在了类 Goods 内部，并且分别有着 protected 和 private 修饰符来控制访问级别。Content 代表着 Goods 的内容，而 GDestination 代表着 Goods 的目的地。它们分别实现了两个接口Content和Destination。在后面的main方法里，直接用 Contents c 和 Destination d进行操作，你甚至连这两个内部类的名字都没有看见！这样，内部类的第一个好处就体现出来了——隐藏你不想让别人知道的操作，也即封装性。 非静态内部类对象有着指向其外部类对象的引用修改上面的例子：12345678910111213141516171819202122232425262728public class Goods &#123; private valueRate=2; private class Content implements Contents &#123; private int i = 11 * valueRate; public int value() &#123; return i; &#125; &#125; protected class GDestination implements Destination &#123; private String label; private GDestination(String whereTo) &#123; label = whereTo; &#125; public String readLabel() &#123; return label; &#125; &#125; public Destination dest(String s) &#123; return new GDestination(s); &#125; public Contents cont() &#123; return new Content(); &#125; &#125; 在这里我们给 Goods 类增加了一个 private 成员变量 valueRate，意义是货物的价值系数，在内部类 Content 的方法 value() 计算价值时把它乘上。我们发现，value() 可以访问 valueRate，这也是内部类的第二个好处——一个内部类对象可以访问创建它的外部类对象的内容，甚至包括私有变量！这是一个非常有用的特性，为我们在设计时提供了更多的思路和捷径。要想实现这个功能，内部类对象就必须有指向外部类对象的引用。Java 编译器在创建内部类对象时，隐式的把其外部类对象的引用也传了进去并一直保存着。这样就使得内部类对象始终可以访问其外部类对象，同时这也是为什么在外部 类作用范围之外向要创建内部类对象必须先创建其外部类对象的原因。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础面向对象三大特性之多态]]></title>
    <url>%2F2019%2F04%2F14%2Fjava-polymorphic%2F</url>
    <content type="text"><![CDATA[java里的多态主要表现在两个方面： 1.引用多态 父类的引用可以指向本类的对象； 父类的引用可以指向子类的对象； 这两句话是什么意思呢，让我们用代码来体验一下，首先我们创建一个父类Animal和一个子类Dog，在主函数里如下所示：注意：我们不能使用一个子类的引用来指向父类的对象，如： 这里我们必须深刻理解引用多态的意义，才能更好记忆这种多态的特性。为什么子类的引用不能用来指向父类的对象呢？我在这里通俗给大家讲解一下：就以上面的例子来说，我们能说“狗是一种动物”，但是不能说“动物是一种狗”，狗和动物是父类和子类的继承关系，它们的从属是不能颠倒的。当父类的引用指向子类的对象时，该对象将只是看成一种特殊的父类（里面有重写的方法和属性），反之，一个子类的引用来指向父类的对象是不可行的！！ 2.方法多态 根据上述创建的两个对象：本类对象和子类对象，同样都是父类的引用，当我们指向不同的对象时，它们调用的方法也是多态的。 创建本类对象时，调用的方法为本类方法； 创建子类对象时，调用的方法为子类重写的方法或者继承的方法； 使用多态的时候要注意：如果我们在子类中编写一个独有的方法（没有继承父类的方法），此时就不能通过父类的引用创建的子类对象来调用该方法！！！ 注意：继承是多态的基础。 A.引用类型转换 了解了多态的含义后，我们在日常使用多态的特性时经常需要进行引用类型转换。 引用类型转换： 1.向上类型转换(隐式/自动类型转换)，是小类型转换到大类型。 就以上述的父类Animal和一个子类Dog来说明，当父类的引用可以指向子类的对象时，就是向上类型转换。如： 2.向下类型转换(强制类型转换)，是大类型转换到小类型(有风险,可能出现数据溢出)。将上述代码再加上一行，我们再次将父类转换为子类引用，那么会出现错误，编译器不允许我们直接这么做，虽然我们知道这个父类引用指向的就是子类对象，但是编译器认为这种转换是存在风险的。如：那么我们该怎么解决这个问题呢，我们可以在animal前加上（Dog）来强制类型转换。如： 但是如果父类引用没有指向该子类的对象，则不能向下类型转换，虽然编译器不会报错，但是运行的时候程序会出错，如：其实这就是上面所说的子类的引用指向父类的对象，而强制转换类型也不能转换！！ 还有一种情况是父类的引用指向其他子类的对象，则不能通过强制转为该子类的对象。如： 这是因为我们在编译的时候进行了强制类型转换，编译时的类型是我们强制转换的类型，所以编译器不会报错，而当我们运行的时候，程序给animal开辟的是Dog类型的内存空间，这与Cat类型内存空间不匹配，所以无法正常转换。这两种情况出错的本质是一样的，所以我们在使用强制类型转换的时候要特别注意这两种错误！！下面有个更安全的方式来实现向下类型转换。。。。 3.instanceof运算符，来解决引用对象的类型，避免类型转换的安全性问题。instanceof是Java的一个二元操作符，和==，&gt;，&lt;是同一类东东。由于它是由字母组成的，所以也是Java的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回boolean类型的数据。 我们来使用instanceof运算符来规避上面的错误，代码修改如下：利用if语句和instanceof运算符来判断两个对象的类型是否一致。 补充说明：在比较一个对象是否和另一个对象属于同一个类实例的时候，我们通常可以采用instanceof和getClass两种方法通过两者是否相等来判断，但是两者在判断上面是有差别的。Instanceof进行类型检查规则是:你属于该类吗？或者你属于该类的派生类吗？而通过getClass获得类型信息采用==来进行检查是否相等的操作是严格的判断,不会存在继承方面的考虑； 总结：在写程序的时候，如果要进行类型转换，我们最好使用instanceof运算符来判断它左边的对象是否是它右边的类的实例，再进行强制转换。 B.抽象类 定义：抽象类前使用abstract关键字修饰，则该类为抽象类。 使用抽象类要注意以下几点： 1.抽象类是约束子类必须有什么方法，而并不关注子类如何实现这些方法。 2.抽象类应用场景： a.在某些情况下，某个父类只是知道其子类应该包含怎样的方法，但无法准确知道这些子类如何实现这些方法(可实现动态多态)。 b.从多个具有相同特征的类中抽象出一个抽象类，以这个抽象类作为子类的模板，从而避免子类设计的随意性。 3.抽象类定义抽象方法，只有声明，不需要实现。抽象方法没有方法体以分号结束，抽象方法必须用abstract关键字来修饰。如: 4.包含抽象方法的类是抽象类。抽象类中可以包含普通的方法，也可以没有抽象方法。如： 5.抽象类不能直接创建，可以定义引用变量来指向子类对象，来实现抽象方法。以上述的Telephone抽象类为例： 123456public abstract class Telephone &#123; public abstract void call();//抽象方法，方法体以分号结束，只有声明，不需要实现 public void message()&#123; System.out.println("我是抽象类的普通方法"); &#125;//抽象类中包含普通的方法&#125; 12345678public class Phone extends Telephone &#123; public void call() &#123;//继承抽象类的子类必须重写抽象方法 // TODO Auto-generated method stub System.out.println("我重写了抽象类的方法"); &#125; &#125; 以上是Telephone抽象类和子类Phone的定义，下面我们看main函数里： 运行结果（排错之后）： C.接口1.概念 接口可以理解为一种特殊的类，由全局常量和公共的抽象方法所组成。也可理解为一个特殊的抽象类，因为它含有抽象方法。 如果说类是一种具体实现体，而接口定义了某一批类所需要遵守的规范，接口不关心这些类的内部数据，也不关心这些类里方法的实现细节，它只规定这些类里必须提供的某些方法。（这里与抽象类相似） 2.接口定义的基本语法1234567 [修饰符] [abstract] interface 接口名 [extends父接口1,2....]（多继承）&#123; 0…n常量 (public static final) 0…n 抽象方法(public abstract) &#125; 其中[ ]里的内容表示可选项，可以写也可以不写;接口中的属性都是常量，即使定义时不添加public static final 修饰符，系统也会自动加上； 接口中的方法都是抽象方法，即使定义时不添加public abstract修饰符，系统也会自动加上。 3.使用接口 一个类可以实现一个或多个接口，实现接口使用implements关键字。java中一个类只能继承一个父类，是不够灵活的，通过实现多个接口可以补充。 继承父类实现接口的语法为：123 [修饰符] class 类名 extends 父类 implements 接口1，接口2...&#123; 类体部分//如果继承了抽象类，需要实现继承的抽象方法；要实现接口中的抽象方法 &#125; 注意：如果要继承父类，继承父类必须在实现接口之前,即extends关键字必须在implements关键字前 补充说明：通常我们在命名一个接口时，经常以I开头，用来区分普通的类。如：IPlayGame 以下我们来补充在上述抽象类中的例子，我们之前已经定义了一个抽象类Telephone和子类Phone，这里我们再创建一个IPlayGame的接口，然后在原来定义的两个类稍作修改，代码如下：1234public interface IPlayGame &#123; public void paly();//abstract 关键字可以省略，系统会自动加上 public String name="游戏名字";//static final关键字可以省略，系统会自动加上 &#125; 123456789101112public class Phone extends Telephone implements IPlayGame&#123; public void call() &#123;//继承抽象类的子类必须重写抽象方法 // TODO Auto-generated method stub System.out.println("我重写了抽象类的方法"); &#125; @Override public void paly() &#123; // TODO Auto-generated method stub System.out.println("我重写了接口的方法"); &#125;&#125; 12345678public class train &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub IPlayGame i=new Phone();//用接口的引用指向子类的对象 i.paly();//调用接口的方法 System.out.println(i.name);//输出接口的常量&#125;&#125; 运行结果： 4.接口和匿名内部类配合使用 接口在使用过程中还经常和匿名内部类配合使用。匿名内部类就是没有没名字的内部类，多用于关注实现而不关注实现类的名称。 语法格式：123456Interface i =new interface()&#123; Public void method&#123; System.out.println(“利用匿名内部类实现接口1”); &#125;&#125;;i.method(); 还有一种写法：（直接把方法的调用写在匿名内部类的最后） 12345Interface i =new interface()&#123;Public void method&#123; System.out.println(“利用匿名内部类实现接口1”); &#125;&#125;.method(); 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础面向对象三大特性之继承]]></title>
    <url>%2F2019%2F04%2F14%2Fjava-inherit%2F</url>
    <content type="text"><![CDATA[1.继承的概念 继承是类与类的一种关系，是一种“is a”的关系。比如“狗”继承“动物”，这里动物类是狗类的父类或者基类，狗类是动物类的子类或者派生类。如下图所示：注：java中的继承是单继承，即一个类只有一个父类。 2.继承的好处 子类拥有父类的所有属性和方法（除了private修饰的属性不能拥有）从而实现了实现代码的复用； 3.语法规则，只要在子类加上extends关键字继承相应的父类就可以了： A.方法的重写 子类如果对继承的父类的方法不满意（不适合），可以自己编写继承的方法，这种方式就称为方法的重写。当调用方法时会优先调用子类的方法。 重写要注意： a.返回值类型 b.方法名 c.参数类型及个数 都要与父类继承的方法相同，才叫方法的重写。 重载和重写的区别： 方法重载：在同一个类中处理不同数据的多个相同方法名的多态手段。 方法重写：相对继承而言，子类中对父类已经存在的方法进行区别化的修改。 B.继承的初始化顺序 1.初始化父类再初始化子类 2.先执行初始化对象中属性，再执行构造方法中的初始化。 基于上面两点，我们就知道实例化一个子类，java程序的执行顺序是： 父类对象属性初始化—-&gt;父类对象构造方法—-&gt;子类对象属性初始化—&gt;子类对象构造方法 下面有个形象的图： C.final关键字 使用final关键字做标识有“最终的”含义。 1.final 修饰类，则该类不允许被继承。 2.final 修饰方法，则该方法不允许被覆盖(重写)。 3.final 修饰属性，则该类的该属性不会进行隐式的初始化，所以 该final 属性的初始化属性必须有值，或在构造方法中赋值(但只能选其一，且必须选其一，因为没有默认值！)，且初始化之后就不能改了，只能赋值一次。 4.final 修饰变量，则该变量的值只能赋一次值，在声明变量的时候才能赋值，即变为常量。 D.super关键字 在对象的内部使用，可以代表父类对象。 1.访问父类的属性：super.age 2.访问父类的方法：super.eat() super的应用： 首先我们知道子类的构造的过程当中必须调用父类的构造方法。其实这个过程已经隐式地使用了我们的super关键字。 这是因为如果子类的构造方法中没有显示调用父类的构造方法，则系统默认调用父类无参的构造方法。 那么如果自己用super关键字在子类里调用父类的构造方法，则必须在子类的构造方法中的第一行。 要注意的是：如果子类构造方法中既没有显示调用父类的构造方法，而父类没有无参的构造方法，则编译出错。 （补充说明，虽然没有显示声明父类的无参的构造方法，系统会自动默认生成一个无参构造方法，但是，如果你声明了一个有参的构造方法，而没有声明无参的构造方法，这时系统不会动默认生成一个无参构造方法，此时称为父类有没有无参的构造方法。） E.Object类 Object类是所有类的父类，如果一个类没有使用extends关键字明确标识继承另一个类，那么这个类默认继承Object类。 Object类中的方法，适合所有子类！！！ 那么Object类中有什么主要的方法呢？ 1.toString() a.在Object类里面定义toString()方法的时候返回的对象的哈希code码(对象地址字符串)。 我们可以发现，如果我们直接用System.out.print（对象）输出一个对象，则运行结果输出的是对象的对象地址字符串，也称为哈希code码。 哈希码是通过哈希算法生成的一个字符串，它是用来唯一区分我们对象的地址码，就像我们的身份证一样。 b.可以通过重写toString()方法表示出对象的属性。 如果我们希望输出一个对象的时候，不是它的哈希码，而是它的各个属性值，那我们可以通过重写toString()方法表示出对象的属性。 2.equals() a.equals（）—-返回值是布尔类型。 b.默认的情况下，比较的是对象的引用是否指向同一块内存地址——-对象实例化时，即给对象分配内存空间，该内存空间的地址就是内存地址。使用方法如：dog.equals(dog2); c.如果是两个对象，但想判断两个对象的属性是否相同，则重写equals（）方法。 以Dog类为例，重写后的equals（）方法如下（当然你可以根据自己想比较的属性来重写，这里我以age属性是否相同来重写equals（）方法）： 上面有四个判断，它们的含义分别是： 1.判断地址是否相同—-if (this == obj)，相同则返回true 2.判断对象是否为空—-if (obj == null)，为空则返回false 3.getClass（）可以得到类对象，判断类型是否一样—–if (getClass() != obj.getClass())，不一样则返回false 4.判断属性值是否一样—-if (age != other.age)，不一样返回false 5.如果地址相同，对象不为空，类型一样，属性值一样则返回true 这里要注意的是，理解obj.getClass()得到的类对象和类的对象的区别，以下用图形表示： 可以看到，对于类对象我们关心它属于哪个类，拥有什么属性和方法，比如我和你都是属于“人”这个类对象；而类的对象则是一个类的实例化的具体的一个对象。比如我和你是两个不同的人。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基础面向对象三大特性之封装]]></title>
    <url>%2F2019%2F04%2F14%2Fjava-encapsulation%2F</url>
    <content type="text"><![CDATA[java的封装，继承，多态是java重要的三大特性，也是java的重要思想。今天开始就来详细的全面的有深度的介绍下这三大特性。 首先是封装。 1.概念：将类的某些信息隐藏在类内部，不允许外部程序直接访问，而是通过该类提供的方法来实现对隐藏信息的操作和访问。 2.好处：只能通过规定的方法访问数据。隐藏类的实例细节，方便修改和实现。 3.封装的实现步骤需要注意：对封装的属性不一定要通过get/set方法，其他方法也可以对封装的属性进行操作。当然最好使用get/set方法，比较标准。 A.访问修饰符从表格可以看出从上到下封装性越来越差。 B.this关键字1.this关键字代表当前对象 this.属性 操作当前对象的属性 this.方法 调用当前对象的方法。2.封装对象的属性的时候，经常会使用this关键字。3.当getter和setter函数参数名和成员函数名重合的时候，可以使用this区别。如： C.Java中的内部类内部类（ Inner Class ）就是定义在另外一个类里面的类。与之对应，包含内部类的类被称为外部类。那么问题来了：那为什么要将一个类定义在另一个类里面呢？清清爽爽的独立的一个类多好啊！！ 答：内部类的主要作用如下： 1.内部类提供了更好的封装，可以把内部类隐藏在外部类之内，不允许同一个包中的其他类访问该类。 2.内部类的方法可以直接访问外部类的所有数据，包括私有的数据。 3.内部类所实现的功能使用外部类同样可以实现，只是有时使用内部类更方便。 内部类可分为以下几种： 成员内部类 静态内部类 方法内部类 匿名内部类 各个内部类的具体介绍参见另一篇文章：java基础内部类详解 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filebeat原理]]></title>
    <url>%2F2019%2F04%2F10%2Ffilebeat-principle%2F</url>
    <content type="text"><![CDATA[Filebeat是本地文件的日志数据采集器。 作为服务器上的代理安装，Filebeat监视日志目录或特定日志文件，tail file，并将它们转发给Elasticsearch或Logstash进行索引、kafka 等。 工作原理：Filebeat由两个主要组件组成：prospector 和harvester。这些组件一起工作来读取文件（tail file）并将事件数据发送到您指定的输出启动Filebeat时，它会启动一个或多个查找器，查看您为日志文件指定的本地路径。 对于prospector 所在的每个日志文件，prospector 启动harvester。每个harvester都会为新内容读取单个日志文件，并将新日志数据发送到libbeat，后者将聚合事件并将聚合数据发送到您为Filebeat配置的输出。 harvesterharvester :负责读取单个文件的内容。读取每个文件，并将内容发送到 the output每个文件启动一个harvester, harvester 负责打开和关闭文件，这意味着在运行时文件描述符保持打开状态如果文件在读取时被删除或重命名，Filebeat将继续读取文件。这有副作用，即在harvester关闭之前，磁盘上的空间被保留。默认情况下，Filebeat将文件保持打开状态，直到达到close_inactive状态关闭harvester会产生以下结果：1）如果在harvester仍在读取文件时文件被删除，则关闭文件句柄，释放底层资源。2）文件的采集只会在scan_frequency过后重新开始。3）如果在harvester关闭的情况下移动或移除文件，则不会继续处理文件。要控制收割机何时关闭，请使用close_ *配置选项 prospectorprospector 负责管理harvester并找到所有要读取的文件来源。比如类型是日志，prospector 就会遍历制定路径下的所有匹配要求的文件。如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester。每个prospector都在自己的Go协程中运行。以下示例将Filebeat配置为从与指定的匹配的所有日志文件中收集行：12345filebeat.prospectors:- type: log paths: - /var/log/*.log - /var/path2/*.log Filebeat目前支持两种prospector类型：log和stdin。每个prospector类型可以定义多次。日志prospector检查每个文件以查看harvester是否需要启动，是否已经运行，或者该文件是否可以被忽略。只有在harvester关闭后文件的大小发生了变化，才会读取到新行。 注意：Filebeat prospector只能读取本地文件，没有功能可以连接到远程主机来读取存储的文件或日志。Filebeat如何保持文件的状态？Filebeat保持每个文件的状态，并经常刷新注册表文件中的磁盘状态。状态用于记住 harvester 正在读取的最后偏移量，并确保发送所有日志行。如果输出（例如Elasticsearch或Logstash）无法访问，Filebeat会跟踪最后发送的行，并在输出再次可用时继续读取文件。在Filebeat运行时，每个prospector内存中也会保存的文件状态信息，当重新启动Filebeat时，将使用注册文件的数据来重建文件状态，Filebeat将每个harvester在从保存的最后偏移量继续读取。每个prospector为它找到的每个文件保留一个状态。由于文件可以被重命名或移动，因此文件名和路径不足以识别文件。对于每个文件，Filebeat存储唯一标识符以检测文件是否先前已采集过。如果您的使用案例涉及每天创建大量新文件，您可能会发现注册文件增长过大。请参阅注册表文件太大？编辑有关您可以设置以解决此问题的配置选项的详细信息。 Filebeat如何确保至少一次交付Filebeat 将每个事件的传递状态存储在注册表文件中。所以它能保证事件至少传递一次到配置的输出，没有数据丢失。在输出阻塞或未确认所有事件的情况下，Filebeat将继续尝试发送事件，直到接收端确认已收到。如果Filebeat在发送事件的过程中关闭，它不会等待输出确认所有收到事件。发送到输出但在Filebeat关闭前未确认的任何事件在重新启动Filebeat时会再次发送。这可以确保每个事件至少发送一次，但最终会将重复事件发送到输出。也可以通过设置shutdown_timeout选项来配置Filebeat以在关闭之前等待特定时间。 注意：Filebeat的至少一次交付保证包括日志轮换和删除旧文件的限制。如果将日志文件写入磁盘并且写入速度超过Filebeat可以处理的速度，或者在输出不可用时删除了文件，则可能会丢失数据。在Linux上，Filebeat也可能因inode重用而跳过行。有关inode重用问题的更多详细信息，请参阅filebeat常见问题解答。 参考官网how-filebeat-works，可以了解更加详细的内容。 原创不易，转载请注明出处。加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>Elastic Stack</category>
        <category>监控</category>
      </categories>
      <tags>
        <tag>beats系列</tag>
        <tag>filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类（Class）和实例（Instance）区别与联系]]></title>
    <url>%2F2019%2F04%2F10%2Fclass-instace%2F</url>
    <content type="text"><![CDATA[面向对象最重要的概念就是类（Class）和实例（Instance），其中类是抽象的模板，而实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 我们在判断两个对象是否为同一类型，时常用到getclass 和 instanceof ，而这两个函数又是时常让人混淆。下面从一个例子说明两者的区别： 123456789101112131415public class Test_drive &#123; public static void main(String[] args)&#123; A a = new A(); B b = new B(); System.out.println(b.getClass().equals(A.class)); System.out.println(b.getClass().equals(B.class)); System.out.println(b instanceof A); System.out.println(b instanceof B); &#125;&#125;class A&#123; &#125;class B extends A&#123; &#125; 在这里，上面四个语句分别输出：false , true , true , true 为什么呢？因为，instanceof判断是否是某一类型的实例时，该类型可以是父类或者接口。而getclass 用于判断准确的类型。同时，在这里必须说明的是，getclass判断的是该变量实际指向的对象的类型（即运行时类型），跟声明该变量的类型无关。即，上面的代码中： B b = new B();改为A a = new B();各语句结果不变。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>java基础</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中的rpm包和deb]]></title>
    <url>%2F2019%2F04%2F10%2Flinux-rpm-deb%2F</url>
    <content type="text"><![CDATA[RMPRPM包（Redhat Linux Packet Manager，就是Redhat的包管理器），后缀是.rpm。它是linux下的一种软件的可执行程序，你只要安装它就可以了。 RPM是Red Hat公司随Redhat Linux推出了一个软件包管理器，通过它能够更加轻松容易地实现软件的安装。1.安装软件：执行rpm -ivh rpm包名，如：1rpm -ivh mysql57-community-release-el7-10.noarch.rpm 2.升级软件：执行rpm -Uvh rpm包名。3.反安装：执行rpm -e rpm包名。4.查询软件包的详细信息：执行rpm -qpi rpm包名5.查询某个文件是属于那个rpm包的：执行rpm -qf rpm包名6.查该软件包会向系统里面写入哪些文件：执行 rpm -qpl rpm包名 DEBdeb是Unix系统(其实主要是Linux)下的安装包，基于tar包，因此本身会记录文件的权限(读/写/可执行)以及所有者/用户组。由于Unix类系统对权限、所有者、组的严格要求，而deb格式安装包又经常会涉及到系统比较底层的操作，所以权限等的设置尤其重要。 deb 包本身有三部分组成：1.数据包，包含实际安装的程序数据，文件名为 data.tar.XXX；2.安装信息及控制脚本包，包含 deb 的安装说明，标识，脚本等，文件名为 control.tar.gz；3.最后一个是deb文件的一些二进制数据，包括文件头等信息，一般看不到，在某些软件中打开可以看到。 deb本身可以使用不同的压缩方式。tar格式并不是一种压缩格式，而是直接把分散的文件和目录集合在一起，并记录其权限等数据信息。之前提到过的 data.tar.XXX，这里XXX就是经过压缩后的后缀名。deb默认使用的压缩格式为gzip格式，所以最常见的就是data.tar.gz。常有的压缩格式还有bzip2和lzma，其中lzma压缩率最高，但压缩需要的CPU资源和时间都比较长。 data.tar.gz包含的是实际安装的程序数据，而在安装过程中，该包里的数据会被直接解压到根目录(即 / )，因此在打包之前需要根据文件所在位置设置好相应的文件/目录树。 而control.tar.gz则包含了一个deb安装的时候所需要的控制信息。一般有 5 个文件：control，用了记录软件标识，版本号，平台，依赖信息等数据；preinst，在解包data.tar.gz 前运行的脚本；postinst，在解包数据后运行的脚本；prerm，卸载时，在删除文件之前运行的脚本；postrm，在删除文件之后运行的脚本； 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux wget 命令用法详解]]></title>
    <url>%2F2019%2F04%2F09%2Flinux-wget%2F</url>
    <content type="text"><![CDATA[Linux wget是一个下载文件的工具，它用在命令行下。对于Linux用户是必不可少的工具，尤其对于网络管理员，经常要下载一些软件或从远程服务器恢复备份到本地服务器。如果我们使用虚拟主机，处理这样的事务我们只能先从远程服务器下载到我们电脑磁盘，然后再用ftp工具上传到服务器。这样既浪费时间又浪费精力，那不没办法的事。而到了Linux VPS，它则可以直接下载到服务器而不用经过上传这一步。wget工具体积小但功能完善，它支持断点下载功能，同时支持FTP和HTTP下载方式，支持代理服务器和设置起来方便简单。下面我们以实例的形式说明怎么使用wget。 1.使用wget下载单个文件以下的例子是从网络下载一个文件并保存在当前目录1wget http://cn.wordpress.org/wordpress-3.1-zh_CN.zip 在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。 2.使用wget -O下载并以不同的文件名保存wget默认会以最后一个符合”/”的后面的字符来命令，对于动态链接的下载通常文件名会不正确。错误：下面的例子会下载一个文件并以名称download.php?id=1080保存1wget http://www.centos.bz/download?id=1080 即使下载的文件是zip格式，它仍然以download.php?id=1080命名。正确：为了解决这个问题，我们可以使用参数-O来指定一个文件名：1wget -O wordpress.zip http://www.centos.bz/download.php?id=1080 3.使用wget –limit-rate限速下载当你执行wget的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。1wget –limit-rate=300k http://cn.wordpress.org/wordpress-3.1-zh_CN.zip 4.使用wget -c断点续传使用wget -c重新启动下载中断的文件:1wget -c http://cn.wordpress.org/wordpress-3.1-zh_CN.zip 对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c参数。 5.使用wget -b后台下载对于下载非常大的文件的时候，我们可以使用参数-b进行后台下载。123wget -b http://cn.wordpress.org/wordpress-3.1-zh_CN.zip Continuing in background, pid 1840. Output will be written to `wget-log’. 你可以使用以下命令来察看下载进度1tail -f wget-log 6.伪装代理名称下载有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过–user-agent参数伪装。1wget –user-agent=”Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16″ 下载链接 7.使用wget –spider测试下载链接当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加–spider参数进行检查。1wget –spider URL 如果下载链接正确，将会显示123456wget –spider URL Spider mode enabled. Check if remote file exists. HTTP request sent, awaiting response… 200 OK Length: unspecified [text/html] Remote file exists and could contain further links, but recursion is disabled — not retrieving. 这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误1234wget –spider url Spider mode enabled. Check if remote file exists. HTTP request sent, awaiting response… 404 Not Found Remote file does not exist — broken link!!! 你可以在以下几种情况下使用spider参数： a.定时下载之前进行检查 b.间隔检测网站是否可用 c.检查网站页面的死链接 8.使用wget –tries增加重试次数如果网络有问题或下载一个大文件也有可能失败。wget默认重试20次连接下载文件。如果需要，你可以使用–tries增加重试次数。1wget –tries=40 URL 9.使用wget -i下载多个文件首先，保存一份下载链接文件12345cat &gt; filelist.txt url1 url2 url3 url4 接着使用这个文件和参数-i下载1wget -i filelist.txt 10.使用wget –mirror镜像网站下面的例子是下载整个网站到本地。1wget –mirror -p –convert-links -P ./LOCAL URL –miror:开户镜像下载-p:下载所有为了html页面显示正常的文件–convert-links:下载后，转换成本地的链接-P ./LOCAL：保存所有文件和目录到本地指定目录 11.使用wget –reject过滤指定格式下载你想下载一个网站，但你不希望下载图片，你可以使用以下命令。1wget –reject=gif url 12.使用wget -o把下载信息存入日志文件你不希望下载信息直接显示在终端而是在一个日志文件，可以使用以下命令：1wget -o download.log URL 13.使用wget -Q限制总下载文件大小当你想要下载的文件超过5M而退出下载，你可以使用以下命令:1wget -Q5m -i filelist.txt 注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。 14.使用wget -r -A下载指定格式文件可以在以下情况使用该功能 下载一个网站的所有图片下载一个网站的所有视频下载一个网站的所有PDF文件1wget -r -A.pdf url 15.使用wget FTP下载你可以使用wget来完成ftp链接的下载。使用wget匿名ftp下载1wget ftp-url 使用wget用户名和密码认证的ftp下载1wget –ftp-user=USERNAME –ftp-password=PASSWORD url wget是在Linux下开发的开放源代码的软件，作者是Hrvoje Niksic，后来被移植到包括Windows在内的各个平台上。它有以下功能和特点：（1）支持断点下传功能；这一点，也是网络蚂蚁和FlashGet当年最大的卖点，现在，Wget也可以使用此功能，那些网络不是太好的用户可以放心了；（2）同时支持FTP和HTTP下载方式；尽管现在大部分软件可以使用HTTP方式下载，但是，有些时候，仍然需要使用FTP方式下载软件；（3）支持代理服务器；对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能；（4）设置方便简单；可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标；（5）程序小，完全免费；程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的； wget虽然功能强大，但是使用起来还是比较简单的，基本的语法是：wget [参数列表] URL。下面就结合具体的例子来说明一下wget的用法。 1.下载整个http或者ftp站点。1wget http://place.your.url/here 这个命令可以将http://place.your.url/here 首页下载下来。使用-x会强制建立服务器上一模一样的目录，如果使用-nd参数，那么服务器上下载的所有内容都会加到本地当前目录。1wget -r http://place.your.url/here 这 个命令会按照递归的方法，下载服务器上所有的目录和文件，实质就是下载整个网站。这个命令一定要小心使用，因为在下载的时候，被下载网站指向的所有地址同 样会被下载，因此，如果这个网站引用了其他网站，那么被引用的网站也会被下载下来！基于这个原因，这个参数不常用。可以用-l number参数来指定下载的层次。例如只下载两层，那么使用-l 2。 要是您想制作镜像站点，那么可以使用－m参数，例如：1wget -m http://place.your.url/here 这时wget会自动判断合适的参数来制作镜像站点。此时，wget会登录到服务器上，读入robots.txt并按robots.txt的规定来执行。 2.断点续传。当文件特别大或者网络特别慢的时候，往往一个文件还没有下载完，连接就已经被切断，此时就需要断点续传。wget的断点续传是自动的，只需要使用-c参数，例如：1wget -c http://the.url.of/incomplete/file 使用断点续传要求服务器支持断点续传。-t参数表示重试次数，例如需要重试100次，那么就写-t 100，如果设成-t 0，那么表示无穷次重试，直到连接成功。-T参数表示超时等待时间，例如-T 120，表示等待120秒连接不上就算超时。 3.批量下载。如果有多个文件需要下载，那么可以生成一个文件，把每个文件的URL写一行，例如生成文件download.txt，然后用命令：1wget -i download.txt 这样就会把download.txt里面列出的每个URL都下载下来。（如果列的是文件就下载文件，如果列的是网站，那么下载首页） 4.选择性的下载。可以指定让wget只下载一类文件，或者不下载什么文件。例如：1wget -m –reject=gif http://target.web.site/subdirectory 表示下载http://target.web.site/subdirectory，但是忽略gif文件。–accept=LIST 可以接受的文件类型，–reject=LIST拒绝接受的文件类型。 5.密码和认证。wget只能处理利用用户名/密码方式限制访问的网站，可以利用两个参数：–http-user=USER设置HTTP用户–http-passwd=PASS设置HTTP密码对于需要证书做认证的网站，就只能利用其他下载工具了，例如curl。 6.利用代理服务器进行下载。如果用户的网络需要经过代理服务器，那么可以让wget通过代理服务器进行文件的下载。此时需要在当前用户的目录下创建一个.wgetrc文件。文件中可以设置代理服务器：http-proxy = 111.111.111.111:8080ftp-proxy = 111.111.111.111:8080分别表示http的代理服务器和ftp的代理服务器。如果代理服务器需要密码则使用：–proxy-user=USER设置代理用户–proxy-passwd=PASS设置代理密码这两个参数。使用参数–proxy=on/off 使用或者关闭代理。wget还有很多有用的功能，需要用户去挖掘。 附录：命令格式：wget [参数列表] [目标软件、网页的网址] -V,–version 显示软件版本号然后退出；-h,–help显示软件帮助信息；-e,–execute=COMMAND 执行一个 “.wgetrc”命令 -o,–output-file=FILE 将软件输出信息保存到文件；-a,–append-output=FILE将软件输出信息追加到文件；-d,–debug显示输出信息；-q,–quiet 不显示输出信息；-i,–input-file=FILE 从文件中取得URL； -t,–tries=NUMBER 是否下载次数（0表示无穷次）-O –output-document=FILE下载文件保存为别的文件名-nc, –no-clobber 不要覆盖已经存在的文件-N,–timestamping只下载比本地新的文件-T,–timeout=SECONDS 设置超时时间-Y,–proxy=on/off 关闭代理 -nd,–no-directories 不建立目录-x,–force-directories 强制建立目录 –http-user=USER设置HTTP用户–http-passwd=PASS设置HTTP密码–proxy-user=USER设置代理用户–proxy-passwd=PASS设置代理密码 -r,–recursive 下载整个网站、目录（小心使用）-l,–level=NUMBER 下载层次 -A,–accept=LIST 可以接受的文件类型-R,–reject=LIST拒绝接受的文件类型-D,–domains=LIST可以接受的域名–exclude-domains=LIST拒绝的域名-L,–relative 下载关联链接–follow-ftp 只下载FTP链接-H,–span-hosts 可以下载外面的主机-I,–include-directories=LIST允许的目录-X,–exclude-directories=LIST 拒绝的目录 中文文档名在平常的情况下会被编码， 但是在 –cut-dirs 时又是正常的，123456wget -r -np -nH –cut-dirs=3 ftp://host/test/ 测试.txt wget -r -np -nH -nd ftp://host/test/ %B4%FA%B8%D5.txt wget “ftp://host/test/*” %B4%FA%B8%D5.txt 由于不知名的原因，可能是为了避开特殊档名，wget 会自动将抓取档名的部分用encode_string处理过，所以该patch就把被encode_string处理成“%3A”这种东西，用decode_string还原成 “:”，并套用在目录与档案名称的部分，decode_string是wget内建的函式。1wget -t0 -c -nH -x -np -b -m -P /home/sunny/NOD32view/ http://downloads1.kaspersky-labs.com/bases/ -o wget.log 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql Client工具无法连接Mysql Server的问题解决]]></title>
    <url>%2F2019%2F04%2F09%2Fmysqlproblem%2F</url>
    <content type="text"><![CDATA[上篇讲了如何安装Mysql server，这篇讲下在使用Client工具连接Mysql Server出现的问题。Client不重要，反正都是工具。 错误一：安装启动完Mysql服务后，使用命令行的方式去操作数据库当然是没有问题的。但是使用navicat等工具去连接的时候就会报错：Host ‘xxx.xxx.xx.xx’ is not allowed to connect to this MySQL server。这个报错的原因在于本地IP（xxx.xxx.xx.xx）没有访问远程数据库的权限。于是下面开启本地IP（xxx.xxx.xx.xx）对远程mysql数据库的访问权限。 1mysql -u root -p 输入密码登录进去，然后输入1use mysql 查询user表中的信息及权限。1select * from user; 其实你只看user,host就可以了。下面赋予权限给我当前Navicat工具所在的IP地址xxx.xxx.xx.xx。1grant all privileges on *.* to root@”xxx.xxx.xx.xx” identified by “your passwd”; 或者1grant all privileges on *.* to root@”xxx.xxx.xx.xx” identified by “your passwd” with grant option; 以上.表示所有权限，包括远程访问权限。如果好多机器都在使用，那把xxx.xxx.xx.xx改为%即可，此时%代表所有IP。 添加完毕，再次查询就可以看到自己的ip了。1select user,host from user 最后使用Navicat工具再次连接数据库，就可以成功连接了。 错误二：如果还是连接不到，那是不是MySQL Server绑定了本地地址，打开 /etc/my.cnf，不同版本my.cnf的位置可能不同，有些在/etc/mysql/my.cnf，找到：bind-address = 127.0.0.1去除 IP 地址绑定，把它改为：bind-address = 0.0.0.0然后重启MySQL Server，再次连接。 错误三：mysqld数据库服务没有启动这个错误就不说了。防火墙开启了也会造成无法访问，此时防火墙需要允许3306端口连接。 原创不易，转载请注明出处。加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>问题解决</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7忘记root用户密码的解决方法]]></title>
    <url>%2F2019%2F04%2F08%2FcentosSetNewPasswd%2F</url>
    <content type="text"><![CDATA[好久没有启动虚拟机了，再启动忘记密码了。我的centos7有好多镜像，重新删除再建可得费不少力气，于是就找了方法去找回密码。下面分享一下这个解决方法： 首先，打开centos7，在选择进入系统的界面的时候按字母“e”进入编辑页面。如下图： 然后找到以“Linux16”开头的行，在该行的最后面添加“init=/bin/sh”添加完成后根据提示使用ctrl+x退出进入单用户模式。 接下来再输入“mount -o remount,rw /”(注意mount与－o之间和rw与/之间的有空格) 然后再输入“passwd”回车，接下来就可以重置root用户密码了，输入一次重置密码，再输入一次确认密码完成。（请忽略我的乱码） 接下来再输入touch /.autorelabel,回车输入exec /sbin/init,回车 回车后稍等几分钟，系统会自动重启，然后使用新密码进行登录即可。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Yum Repository在Centos7上安装Mysql Server]]></title>
    <url>%2F2019%2F04%2F04%2Fmysqlhandle%2F</url>
    <content type="text"><![CDATA[以前在VM虚拟机上安装了CentOS7，但是一直没有安装数据库，今天安装了Mysql Server，所以记录下来一个教程。Mysql官网提供了比较详细的Mysql安装教程，以下是我安装步骤及遇到的问题： 1.安装Mysql Server关于版本的问题，可以根据需要选择mysql提供的各种版本。首先安装yum的资源库，yum repository 提供用于安装mysql服务器、客户机、mysql Workbench、mysql实用程序、mysql路由器、mysql shell、connector/odbc、connector/python等的rpm包。12wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpmyum -y install mysql57-community-release-el7-10.noarch.rpm 在这之前需要安装wget工具，wget 是一个从网络上自动下载文件的自由工具，支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议 下载，并可以使用 HTTP 代理。”wget” 这个名称来源于 “World Wide Web” 与 “get” 的结合。1yum install wget 然后安装Mysql server1yum -y install mysql-community-server 此时已完成Mysql server的安装 2.Mysql数据库启动及状态启动Mysql数据库1systemctl start mysqld.service Mysql数据库状态检查1systemctl status mysqld.service Mysql数据库停止1systemctl stop mysqld.service 3.Mysql数据库的设置Mysql数据库会在第一次安装启动后自动生成root用户的初始密码，可用于初次登录Mysql数据库，但是不能做其他操作，需要修改这个初始密码。初始密码生成后会保存在mysql.log中。 使用以上密码进入数据库：1mysql -uroot -p 接下来修改密码，MySQL有密码设置的规范，具体是与validate_password_policy的值有关： 可以通过以下命令查看当前的密码设置情况：1SHOW VARIABLES LIKE 'validate_password%'; 初始情况如下： 密码的长度是由validate_password_length决定的，而validate_password_length的计算公式是：1validate_password_length = validate_password_number_count + validate_password_special_char_count + (2 * validate_password_mixed_case_count) 按照以上的密码策略设置最新密码，设置密码命令如下：1ALTER USER 'root'@'localhost' IDENTIFIED BY 'your new password'; 如果你的密码设置的过于简单，就会提示你密码设置过于简单的。所以按照这种策略设置密码比较麻烦，而且不方便，还容易忘记。所以如果没有特殊要求限制，可以手动修改这个策略：12set global validate_password_policy=0;set global validate_password_length=1; 这样就可以设置一个简单易记密码啦。 4.删除Yum Repository因为安装了Yum Repository，有时候yum操作会出现自动更新，可以把这个卸载掉：1yum -y remove mysql57-community-release-el7-10.noarch 原创不易，转载请注明出处。加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>使用教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何做一个有深度的分享]]></title>
    <url>%2F2019%2F04%2F01%2FshareTips%2F</url>
    <content type="text"><![CDATA[最近太忙了，晚上的时间几乎被全部占去，没有时间更新博客。今天中午终于可以抽个时间啦。博客开了十几天，看着自己博客才更新了二十几篇博客已经有600的访问人数及1500的访问量。心里很开心，但也很担心，担心打脸。所以我最近也在考虑一个问题，现在博客这么多，稂莠不齐，什么样的博客才是高质量的博客，如何才能做到一个有深度的分享呢？ 我也算用心想了想，总结了以下几点： 1.为什么要用这个产品或者方案？它解决了什么问题？有哪些优缺点？2.同类产品或者方案的对比，我的场景下用哪一种是最优的？3.当前使用产品的原理是什么？（包括使用语言，底层是什么技术？为什么对比起来这么优秀？等等）4.具体介绍产品的使用教程或者详细介绍方案（包括可扩展性，高并发，高峰处理，性能，成本等等）5.那我用的产品是哪个版本version（一定要有版本），具体场景介绍，表现如何（性能，效率，压测等等方面），使用的注意事项有哪些？基于当前这些有没有更好的优化方案？6.社区中或者网上能够提供哪些方案或者产品？具体的场景是什么？优缺点在哪？ 通过以上几点，在做一个知识点的博客时就能写出一个系列出来，这样有很多好处，从一个知识点与别的知识点进行对比贯通，让你对整个知识点体系更熟悉，达到举一反三的效果。以这种方式去思考问题，去汇总知识，相信没有你进不去的门。同时也是最重要的能够养成了很好的学习和工作习惯，多问几个为什么，你每次就比别人多知道一点，积累起来就是自己比别人多的财富，达到别人没有达到的高度。 原创不易，转载请注明出处。加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>干货</category>
      </categories>
      <tags>
        <tag>计划</tag>
        <tag>分享</tag>
        <tag>干货</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用后端+移动端的性能优化指标，以及性能优化方法]]></title>
    <url>%2F2019%2F03%2F29%2Fapplicationoptimal%2F</url>
    <content type="text"><![CDATA[性能问题简介应用性能是产品用户体验的基石，性能优化的终极目标是优化用户体验。当我们谈及性能，最直观能想到的一个词是“快”，Strangeloop在对众多的网站做性能分析之后得出了一个著名的3s定律“页面加载速度超过3s，57%的访客会离开”,可见页面加载速度对于互联网产品的重要性。 性能指标性能优化是个系统性工程，涉及到后端、前端、移动端、系统网络及各种基础设施，每一块都需要做各自的性能优化。当我们系统的分析性能问题时，可以通过以下指标来衡量： 1.Web端：首屏时间、白屏时间、可交互时间、完全加载时间等。首屏时间是指从用户打开网页开始到浏览器第一屏渲染完成的时间，是最直接的用户感知体验指标，也是性能领域公认的最重要的核心指标。首屏时间 = DNS时间 + 建立连接时间 + 后端响应时间 + 网络传输时间 + 首屏页面渲染时间 2.移动端：Crash率、内存使用率、FPS（Frames Per Second, 每秒传输帧数）、端到端响应时间等。Native相比于H5在交互体验方面有更多的优势，FPS是体现页面顺畅程度的一个重要指标，另外移动端开发同学还需要关注App进程的CPU使用率、内存使用率等系统性能指标。端到端响应时间是衡量一个API性能的关键指标，比纯后端响应时间更全面，它会受到DNS、网络带宽、网络链路、HTTP Payload等多个因素的影响。端到端响应时间是DNS解析时间、网络传输时间及后端响应时间的总和。 3.后端：响应时间（RT）、吞吐量（TPS）、并发数等。QPS: 每秒钟处理完请求的次数；注意这里是处理完。具体是指发出请求到服务器处理完成功返回结果。可以理解在server中有个counter，每处理一个请求加1，1秒后counter=QPS。 TPS：每秒钟处理完的事务次数，一般TPS是对整个系统来讲的。一个应用系统1s能完成多少事务处理，一个事务在分布式处理中，可能会对应多个请求，对于衡量单个接口服务的处理能力，用QPS比较多。 并发量：系统能同时处理的请求数 RT：响应时间，处理一次请求所需要的平均处理时间。后端系统响应时间是指系统对请求做出响应的时间（应用延迟时间），对于面向用户的Web服务，响应时间能很好度量应用性能， 会受到数据库查询、RPC调用、网络IO、逻辑计算复杂度、JVM垃圾回收等多方面因素影响。 对于高并发的应用和系统，吞吐量(TPS)是个非常重要的指标，它与request对CPU、内存资源的消耗，调用的外部接口及IO等紧密关联。 计算关系： QPS = 并发量 / 平均响应时间 并发量 = QPS * 平均响应时间 影响性能的因素互联网产品是创意、设计、研发、系统、网络、硬件、运维等众多资源相互交织的集合体，性能受多方面因素影响，犹如一只木桶，木桶能盛多少水，取决于最短的那块木板，也可称之为短板效应。影响产品性能的因素有： 1. 产品逻辑与用户行为产品逻辑过于复杂、功能交互过于丰富、产品设计过于绚丽、页面元素素材过多等都会影响产品性能。 2. 基础网络中国的基础网络是世界上最复杂的基础网络，国内的网络运营商众多且各自为政，互联互通成本很高。对于境外业务来说更是要面对国内国际网络交互的情况，再加上GFW的存在，网络延迟、丢包现象非常严重。 3. 代码及应用开发语言瓶颈、代码质量及系统架构等都会影响系统性能，常见的代码及应用问题有：架构不合理。业务发展超越架构支撑能力而导致系统负荷过载，进而导致出现系统奔溃、响应超时等现象。另外不合理的架构如：单点、无cache、应用混部署、没有考虑分布式、集群化等也都会影响性能。研发功底和经验不足。开发的App、Server效率和性能较低、不稳定也是常见的事情。没有性能意识，只实现了业务功能不注意代码性能，新功能上线后整体性能下降，或当业务上量后系统出现连锁反应，导致性能问题叠加，直接影响用户体验。多数的性能问题发生在数据库上。由慢SQL（详情可以参考阿里P8架构师谈：MySQL慢查询优化、索引优化、以及表等优化总结）、过多查询等原因造成的数据库瓶颈，没有做读写分离、分库分表等。 4. 移动端环境移动互联网时代，移动端环境的复杂性对产品的性能影响也很大，比如用户的设备类型、设备性能、操作系统类型、系统版本及网络类型等。 5. 硬件及云环境硬件的发展遵循着摩尔定律，生命周期一般都很短，服务器老化或其他硬件问题经常会导致应用故障。IDC、机架、服务器、内存、磁盘、网卡等不同硬件和操作系统上运行的应用性能差距可以达到数十倍之多。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>优化</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>前端</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[史上最全阿里技术面试题目]]></title>
    <url>%2F2019%2F03%2F29%2Faliinterview%2F</url>
    <content type="text"><![CDATA[收集了一些阿里面试题，以后也会逐步更新这些面试题。并会慢慢的提供这些面试题的答案，请关注哦！ 一：阿里技术一面(基础掌握牢固)常用的异常类型? session java锁 gc原理 hashmap listlink arraylist 区别 aop 原理 多线程 kafka 原理和容错 spark hadoop 原理 redis 同步机制 classLoader 机制 Http 协议 cookie的限制 如何设计一个分步式登录系统？ Spring加载过程？ 自己有没有写过类似Spring这样的AOP事务？ spring的加载过程？ atomic 与 volatile的区别？ Thread的 notify()给notifyAll()的区别? notifiy()是唤醒的那一个线程? Thread.sleep()唤醒以后是否需要重新竞争？ 单例有多少种写法? 有什么区别? 你常用哪一种单例，为什么用这种？ 问一个Thread.join()相关的问题? 写一个JAVA死锁的列子? 如何解决死锁? GC回收算法,及实现原理? HashMap数据存储结构? key重复了怎么办? 是如何解决的? Spring AOP的实现原理，底层用什么实现的？ Java的线程池说一下，各个参数的作用，如何进行的? 同步与异步区别？ HashMap的实现原理，HashMap是如何解决hash冲突的问题？ Redis讲一下，项目使用场景，以及对应的算法？ 分布式系统的全局id如何实现？用zookeeper如何实现的呢，机器号+时间戳即可？ 分布式锁的方案，redis和zookeeper那个好，如果是集群部署，高并发情况下哪个性能更好？ kafka了解么，了解哪些消息队列？ 乐观锁，悲观锁? IO和NIO的却别，以及NIO的原理，有了解过mina？ JVM内存模型，JVM加载原理，回收算法了解？ 阿里技术二面(技术原理、个人擅长的项目)重点是面试技术原理，以及对技术的热情和专研程度： Java的高级知识 开源框架的原理 JVM 多线程 高并发 中间件 之前项目经历，运用的技术，遇到的问题，如何解决，个人有什么收获和成长； 对于技术的热情（平时是否看些技术书籍，逛论坛，写博客，写源代码或程序等）； 介绍你做的项目和其中的难点？ 反射的作用是什么？ 数据仓库，多线程和并发工具等？ 私有云，docker和k8s等？ 了解哪些中间件，dubbo，rocketmq，mycat等？ dubbo中的rpc如何实现？ 自己实现rpc应该怎么做？ dubbo的服务注册与发现？ 听说我是非科班，于是问了些排序算法 阿里技术三面不是面试，而是笔试，耗时三个小时，考的是Java核心的基础。大概说一下就是有几个考点： Java并发的知识点 集合类 线程池 多线程之间的通信 面试耗时将近40分钟。 阿里HR面：聊人生谈理想，主要还是考察你对工作是否持积极的态度，以及你是否稳定,自信发挥就可以了。 面试耗时将近30分钟。 JAVA开发技术面试可能问到的问题？我们主要考核的是网络nio 分布式数据库高并发大数据 自定义表格的实现? 动态表单设计? in-jvm（必考）以及jmm缓存模型如何调优? 常用的RPC框架 nio和io 并发编程，设计模式 地图组件? hashmap有什么漏洞会导致他变慢？ 如何给hashmap的key对象设计他的hashcode？ 泛型通配符?在什么情况下使用？ 后端方面：redis?分布式框架dubbo(阿里巴巴开源框架)?设计模式? 场景式的问题:秒杀,能列出常见的排队、验证码、库存扣减方式对系统高并发的影响? 能根据实际的需要构建缓存结构提高提高网站的访问速度，熟练使用ehcache、oscache，了解memcache。 了解基于dns轮询的负载均衡，熟练配置web服务器实现负载均衡，程序级能综合使用基于hash或取模等手段实现软负载。 熟悉分布式数据库设计和优化技术，熟练使用mysql、oracle、SqlServer等主流数据库，熟悉hadoop hbase mangodb redis ehcache、oscache memcache。对于大数据量的数据库处理采用分表分库、数据库读写分离、建立缓存等手段优化性能。 熟练掌握lucene，能基于lucene开发大型的搜索引擎，并能用lucene来改善和优化数据库的like查询。 项目部分缓存的使用，如果现在需要实现一个简单的缓存，供搜索框中的ajax异步请求调用，使用什么结构？ 内存中的缓存不能一直存在，用什么算法定期将搜索权重较低的entry去掉？ TCP如何保证安全性 红黑树的问题，B+数 JDK1.8中对HashMap的增强，如果一个桶上的节点数量过多，链表+数组的结构就会转换为红黑树。 项目中使用的单机服务器，如果将它部署成分布式服务器？ MySQL的常见优化方式、定为慢查询 手写一个线程安全的单例模式 进阿里必会知识： 算法和数据结构数组、链表、二叉树、队列、栈的各种操作（性能，场景） 二分查找和各种变种的二分查找 各类排序算法以及复杂度分析（快排、归并、堆） 各类算法题（手写） 理解并可以分析时间和空间复杂度。 动态规划（笔试回回有。。）、贪心。 红黑树、AVL树、Hash树、Tire树、B树、B+树。 图算法（比较少，也就两个最短路径算法理解吧） 计算机网络OSI7层模型（TCP4层）每层的协议 get/post 以及幂等性 http 协议头相关 网络攻击（CSRF、XSS） TCP/IP三次握手、四次挥手 TCP与UDP比较 DDos攻击 (B)IO/NIO/AIO三者原理，各个语言是怎么实现的 Netty Linux内核select poll epoll 数据库（最多的还是mysql，Nosql有redis）索引（包括分类及优化方式，失效条件，底层结构） sql语法（join，union，子查询，having，group by） 引擎对比（InnoDB，MyISAM） 数据库的锁（行锁，表锁，页级锁，意向锁，读锁，写锁，悲观锁，乐观锁，以及加锁的select sql方式） 隔离级别，依次解决的问题（脏读、不可重复读、幻读） 事务的ACID B树、B+树 优化（explain，慢查询，show profile） 数据库的范式 分库分表，主从复制，读写分离。 Nosql相关（redis和memcached区别之类的，如果你熟悉redis，redis还有一堆要问的） 操作系统：进程通信IPC（几种方式），与线程区别 OS的几种策略（页面置换，进程调度等，每个里面有几种算法） 互斥与死锁相关的 linux常用命令（问的时候都会给具体某一个场景） Linux内核相关（select、poll、epoll） 编程语言（这里只说Java）：把我之后的面经过一遍，Java感觉覆盖的就差不多了，不过下面还是分个类。 Java基础（面向对象、四个特性、重载重写、static和final等等很多东西） 集合（HashMap、ConcurrentHashMap、各种List，最好结合源码看） 并发和多线程（线程池、SYNC和Lock锁机制、线程通信、volatile、ThreadLocal、CyclicBarrier、Atom包、CountDownLatch、AQS、CAS原理等等） JVM（内存模型、GC垃圾回收，包括分代，GC算法，收集器、类加载和双亲委派、JVM调优，内存泄漏和内存溢出） IO/NIO相关 反射和代理、异常、Java8相关、序列化 设计模式（常用的，jdk中有的） Web相关（servlet、cookie/session、Spring) 阿里面试题目范畴：内存模型 类加载机制 GC JVM调优 线程池原理 动态代理 悲观锁乐观锁 高并发问题 事务隔离级别 索引原理 限流 分库分表 分布式事务提交 微服务 dubbo原理 阿里面试总结阿里比较喜欢的人才特点：对技术有热情，强硬的技术基础实力；主动，善于团队协作，善于总结思考。 技术基础以及的问题多看看书准备，不懂的直接说不懂没关系的；在项目细节上多把关一下，根据项目有针对性的谈自己的技术亮点，能表达清楚，可以引导面试官来问你比较擅长的技术问题。 阿里的面试特别喜欢面试技术原理，特别是 多线程 NIO 异步消息框架 分布式相关的缓存算法等 JVM的加载过程和原理 回收算法 以及具体使用过的框架，会问部分参数检验你是否熟用 第一面能通过，后续被录用的可能性就比较高了，第一轮非常重要，建议系统性的学习面试题目！ 原创不易，转载请注明出处。加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>阿里面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>阿里</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端优化的几种方式--缓存化，异步化，服务化]]></title>
    <url>%2F2019%2F03%2F28%2Fback-optimal%2F</url>
    <content type="text"><![CDATA[整理了一下后端优化的几种方式，不太全，先写一部分，然后不断的更新。 1.硬件升级硬件问题对性能的影响不容忽视。举一个例子：一个DB集群经常有慢SQL报警，业务排查下来发现SQL都很简单，该做的索引优化也都做了。后来DBA同学帮忙定位到问题是硬件过旧导致，将机械硬盘升级成固态硬盘之后报警立马消失了，效果立竿见影！ 2.缓存化缓存可以称的上是性能优化的利器，使用缓存时需要考虑缓存命中率、缓存更新、数据一致性、缓存穿透及雪崩、Value过大等问题，可以通过mutiGet将多次请求合并一次、异步访问等方式来提升缓存读取的性能。 3.产品逻辑优化业务逻辑优化经常会容易被忽略，但效果却往往比数据库调优、JVM调优之类的来的更明显。举一个例子，12306春运抢火车票的场景，由于访问的人多，用户点击“查票”之后系统会非常卡，进度条非常慢，作为用户，我们会习惯性的再去点“查票”，可能会连续点个好几次。假设平均一个用户点5次，则后端系统负载就增加了5倍！而其中80%的请求是重复请求。这个时候我们可以通过产品逻辑的方式来优化，比如，在用户点击查询之后将“按钮置灰”，或者通过JS控制xx秒只能只能提交一次请求等，有效的拦截了80%的无效流量。 4.服务化做服务化最基础的是按业务做服务拆分，避免跨业务间的互相影响，数据和服务同时拆分。同一个业务内部我们还按计算密集型/IO密集型的服务拆分、C端/B端服务拆分、核心/非核心服务拆分、高频服务单独部署等原则做拆分。 5.异步化异步化可以利用线程池、消息队列等方式实现。使用线程池的时候一定要注意核心参数的设置，可以通过监控工具去观测实际创建、活跃、空闲的线程数，结合CPU、内存的使用率情况来做线程池调优。另一种是通过NIO实现异步化，一切网络IO皆可异步：RPC框架、Servlet 3.0提供的异步技术、Apache HttpAsyncClient、缓存异步接口等等。 6.搜索引擎复杂查询以及一些聚合计算不适合在数据库中做，可以利用搜索引擎来实现，另外搜索引擎还可以帮我们很好的解决跨库、跨数据源检索的场景。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>优化</category>
      </categories>
      <tags>
        <tag>后端</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你制作一份吸引人的简历]]></title>
    <url>%2F2019%2F03%2F28%2Fresume%2F</url>
    <content type="text"><![CDATA[简历是面试中一个重要的名片，通过一份简历能够让面试官留下你和别人不一样的印象。那么如何制作一份优秀的简历，将自己的特色展示出来，根据自己的经验和最近几天从大神那收集来的资料。汇总整理了简历制作的一些注意点供大家参考。 如果你本身具备很强的实力，不要因为对求职简历细节不重视，反而痛失良机。求职加薪，除了平时的积累以外，更要懂得积累求职相关的技巧和经验。想进入心仪的公司，这样才能事半功倍。看看大神是怎么总结简历制作的。 求职跳槽的三大误区第一个误区，求职跳槽就是找工作之前3个月的事。如果想进入自己心仪的公司，首先肯定需要方法和长时间准备的。而且，越好的公司，准备的时间越长，准备跳槽的时间周期，最好以年为单位，而不是3月的临时备战。 第二个误区，以纯生态的方式去面试，简历不做优化，谈吐技巧不做改进，以原味出现。很多人会说，这不是简历作弊吗？请记住，这并不是让你去瞎改莫需有的经历和职位，这是优化你自己。人靠衣装，马靠鞍，简历就是每一个人的名片和身份，一定要更早的意识到并“优化”自己，而不是有意去“破坏”。现在，基本每一份简历，在你投递以后，都有第三方的公司，系统结合人工的方式，鉴别简历真假，只不过你不知道而已。所以，很多时候你的简历已经进入了大部分公司的黑名单，你还不知道是为什么。你能想到的，其实公司在面试的第一个环节已经通过第三方公司做过筛选了。 第三个误区，我的履历是小公司，想进大公司需要镀金。很多小公司的朋友，一直比较懊恼的事就是没有大公司的经历和背书。一直认为这是自己不能进入大公司的原因。其实，小公司也有自己的优势，只不过你没有在你的简历上重点突出而已。其实，这些优势也是大公司目前找人最看重的，你却没写。稍后，我会给到大家具体的例子，看到小公司去BAT的典型案例。 我见过很多完全能力在BAT等一线互联网公司胜任朋友，去面试过一两次就放弃了，时间一长，这事就完全放弃了。其实，并不是实力不够，主要是方法不当。如果你真打算进去，其实方法真的太多。比如，我以前在淘宝的同事，就是同时投阿里好几个子公司的简历，去面试了阿里多家子公司，最后选择了去淘宝。当然，除了他的硬实力以外，他也有一套没有告知你的方法。如果把这些误区都正视了，自然就会开始重视和思考怎样才能去自己心仪的公司的方法和步骤。 以下谈谈之前面试公司的准备步骤和心得，希望对你们有所用。第一步：简历自我评价改进简历就是每一个人的招牌，招牌必须得帅。之前谈到的几位读者朋友，我第一眼看到他们简历的时候，基本都是千篇一律。并不是说简历的个人经历，而是简历的排版以及不突出重点，看到的都是千篇一律的简历。这在茫茫人海中，怎样能率先凸显出你的价值呢？首先，需要快速改进自己的招牌。每一个简历基本都有自我评价这一项，很多朋友并不是很关心，苏不然就是在这个环节丢失了面试的机会。我作为面试官，不太可能仔细查看完整你的简历，除非你的简历很特别。首先，看的就是你的自我评价。如果在自我评价环节，看到了有价值的信息，才会继续往下看完整简历。 我截取一小段我的简历的自我评价作为例子：在互联网电子商务行业和旅游行业，有着10年以上的产品技术资深的经验：先后在美国新蛋newegg、淘宝、百度、携程等大型互联网公司工作，从最早美国新蛋一名程序员，到淘宝的技术架构工作，再到百度的研发经理，再到现在的携程定制旅游CTO，管理产品技术等团队，一路的成长轨迹已经成为自己最大的财富。后面是你在工作期间取得的重要的业绩，比如：在公司期间的最佳团队奖(说明你的管理实力)，这里要结合你面试的公司的职位来突出业绩点。 首先是让对方对你有兴趣，其次才是你取得的成绩，尽可能把自己的优势突出在最前方。当然，我知道你肯定会说，我没有你这样好的履历。那就拿出你现阶段，最拿得出手的优势点，放在最前面。 举个栗子，你没有大公司的经历，那就突出你自己本身，比如你的成长经历，或者你的自学成才经历，你的项目历练经历等。这个阶段没有大公的履历，那就要突出自我未来成长优势。还是举个栗子，俞军，前百度产品副总裁、首席产品架构师，互联网产品经理的祖师爷，我截取一段他当时面试百度的简历中的自我评价。26岁，上海籍，同济大学化学系五年制，览群书，多游历。长期想踏入搜索引擎业，无奈欲投无门，心下甚急，故有此文。如有公司想做最好的中文搜索，诚意乞一参与机会。 自我评价：本人热爱搜索成痴，只要是做搜索，不计较地域（无论天南海北，刀山火海），不计较职位（无论高低贵贱一线二线，与搜索相关即可），不计较薪水（可维持个人当地衣食住行即是底线），不计较工作强度（反正已习惯了每日14小时工作制）。 如果是你在找搜索相关的人，你看到这样一份简历会怎么样，是不是会眼前一亮呢？这个阶段，你看到有任何大公司履历吗，没有，有的就是自己想励志从事搜索行业的抱负和雄心壮志，这就叫突出自我未来成长的优势。 这里还给大家提醒一点，简历内容不要再出现什么精通、熟悉等字样，需要把精通、熟悉转换成更具体的内容，并且把简历用ppt等工具重新制作，最好转换成pdf格式。简历切记别千篇一律！ 第二步：简历项目经验改进有了第一步自我评价的敲门砖，才会有项目经验的介绍。很多朋友喜欢把自己所有的项目经历都写一遍，请切记，项目经历仍然是突出自己的优势。首先，要做的就是精简你的项目经历。我相信，你一定会有你拿得出手的项目，你认为还不错的项目，重点突出这些项目的详细描述，不重要的经历就一笔带过即可。 还是举一个栗子，我以自己的简历中截取一段项目描述，大家作参考。负责携程旅游事业部签证，国际火车票，定制旅游三大业务线的产品和技术团队管理工作。具体工作分为五个方面：团队建设和管理，业务的产品规划和把控，负责整体技术架构的规划和把控，项目的推动和持续跟踪，敏捷教练： 1.团队建设和管理：组建整个团队，团队成员包含：产品经理，开发主管，开发工程师，项目经理，测试主管，测试人员。2.业务的产品规划和把控：负责各业务线产品的需求调研以及规划，制定各业务板块每一个季度详细产品规划方案，并且跟踪实施和改进；3.负责整体技术架构规划和把控：负责各业务板块的整体技术系统架构设计方案，推动各业务板块往大型分布式系统演进；4.项目的推动和持续跟踪：把控开发任务的需求分析和开发计划制定；带领以及指导整个团队：产品、技术、测试，用敏捷的方式快速推进产品和技术计划；5.敏捷教练：第一个在携程内部，采用敏捷的方式驱动产品技术，帮助成员跟上敏捷的节奏。 这样，是否把具体你在项目中的工作职责，拆解成具体可评估的内容，并且这些内容还能突出你的优势。当然，你需要根据你当前的情况来具体分析。比如，这个阶段你是程序员，产品经理等，自我评价和项目经验等，都是一脉相承，通过项目来辅助来突出自己的优势。 第三步：针对公司职位定制简历在互联网大数据的背景时代下，谈到的都是千人千面，为什么简历就不能是千人千面呢？目前，很多朋友投递简历到公司的时候，还有一个习惯，采用是机海战术，认为投递简历是多多益善。其实，投递简历不需要太多，重点是找到投递的渠道，以及根据公司的情况来定制自己的简历。举一个栗子，你准备面试架构师 or 项目经理，又 or你打算技术转产品，是不是你投递的简历里，是否应该突出未来你想从事的具体工作，与你相关的优势点在哪呢？这里还给大家提醒一点，这里最好结合你投递的公司来优化你的简历。每一个公司的文化和氛围都不一样，就拿我去过的这几家公司来讲，文化氛围差别都特别大。所所以，对应的面试官，查看简历以及现场面试的时候，除了你的硬实力考察以外，软实力的考察还不太一样。这些，你可以考虑在简历中，根据公司的情况来重点凸显。 以上就是汇总的一些建议，希望你们能早日加入你心仪的公司。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>简历，面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多级缓存构建高并发高可用系统]]></title>
    <url>%2F2019%2F03%2F27%2Fcache%2F</url>
    <content type="text"><![CDATA[缓存是构建高并发，高可用系统的有效系统。 传统缓存：将缓存化的数据存放到内存的副本上，可以提高并发，提高读取速度。 1.基于内存操作 2.空间换时间 3.降低瓶颈操作 4.减少对外依赖 本地缓存与分布式缓存的对比 分层级的缓存解决方案]]></content>
      <categories>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>缓存设计</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划]]></title>
    <url>%2F2019%2F03%2F27%2Fdynamic-programming%2F</url>
    <content type="text"><![CDATA[今天看到算法中的动态规划，挺有意思的。所以在网上找了整理了一些关于动态规划的资料。可以参考下。 前言面试题中有关动态规划（Dynamic Programming）算法的题目很多。相对于我来说，算法里面遇到的问题里面感觉最难的也就是动态规划（Dynamic Programming）算法了。于是花了好长时间，查找了相关的文献和资料准备彻底的理解动态规划（Dynamic Programming）算法。一是帮助自己总结知识点，二是也能够帮助他人更好的理解这个算法。后面的参考文献只是我看到的文献的一部分。 动态规划算法的核心理解一个算法就要理解一个算法的核心，动态规划算法的核心是下面的一张图片和一个小故事。 1234567891011A * "1+1+1+1+1+1+1+1 =？" *A : "上面等式的值是多少"B : *计算* "8!"A *在上面等式的左边写上 "1+" *A : "此时等式的值为多少"B : *quickly* "9!"A : "你怎么这么快就知道答案了"A : "只要在8的基础上加1就行了"A : "所以你不用重新计算因为你记住了第一个等式的值为8!动态规划算法也可以说是 '记住求过的解来节省时间'" 由上面的图片和小故事可以知道动态规划算法的核心就是记住已经解决过的子问题的解。 动态规划算法的两种形式上面已经知道动态规划算法的核心是记住已经求过的解，记住求解的方式有两种：①自顶向下的备忘录法 ②自底向上。为了说明动态规划的这两种方法，举一个最简单的例子：求斐波拉契数列Fibonacci 。先看一下这个问题： 123Fibonacci (n) = 1; n = 0Fibonacci (n) = 1; n = 1Fibonacci (n) = Fibonacci(n-1) + Fibonacci(n-2) 以前学c语言的时候写过这个算法使用递归十分的简单。先使用递归版本来实现这个算法： 12345678910public int fib(int n)&#123; if(n&lt;=0) return 0; if(n==1) return 1; return fib( n-1)+fib(n-2);&#125;//输入6//输出：8 先来分析一下递归算法的执行流程，假如输入6，那么执行的递归树如下： 上面的递归树中的每一个子节点都会执行一次，很多重复的节点被执行，fib(2)被重复执行了5次。由于调用每一个函数的时候都要保留上下文，所以空间上开销也不小。这么多的子节点被重复执行，如果在执行的时候把执行过的子节点保存起来，后面要用到的时候直接查表调用的话可以节约大量的时间。下面就看看动态规划的两种方法怎样来解决斐波拉契数列Fibonacci 数列问题。 ①自顶向下的备忘录法12345678910111213141516171819202122public static int Fibonacci(int n)&#123; if(n&lt;=0) return n; int []Memo=new int[n+1]; for(int i=0;i&lt;=n;i++) Memo[i]=-1; return fib(n, Memo); &#125; public static int fib(int n,int []Memo) &#123; if(Memo[n]!=-1) return Memo[n]; //如果已经求出了fib（n）的值直接返回，否则将求出的值保存在Memo备忘录中。 if(n&lt;=2) Memo[n]=1; else Memo[n]=fib( n-1,Memo)+fib(n-2,Memo); return Memo[n]; &#125; 备忘录法也是比较好理解的，创建了一个n+1大小的数组来保存求出的斐波拉契数列中的每一个值，在递归的时候如果发现前面fib（n）的值计算出来了就不再计算，如果未计算出来，则计算出来后保存在Memo数组中，下次在调用fib（n）的时候就不会重新递归了。比如上面的递归树中在计算fib（6）的时候先计算fib（5），调用fib（5）算出了fib（4）后，fib（6）再调用fib（4）就不会在递归fib（4）的子树了，因为fib（4）的值已经保存在Memo[4]中。 ②自底向上的动态规划备忘录法还是利用了递归，上面算法不管怎样，计算fib（6）的时候最后还是要计算出fib（1），fib（2），fib（3）……,那么何不先计算出fib（1），fib（2），fib（3）……,呢？这也就是动态规划的核心，先计算子问题，再由子问题计算父问题。12345678910111213public static int fib(int n)&#123; if(n&lt;=0) return n; int []Memo=new int[n+1]; Memo[0]=0; Memo[1]=1; for(int i=2;i&lt;=n;i++) &#123; Memo[i]=Memo[i-1]+Memo[i-2]; &#125; return Memo[n];&#125; 自底向上方法也是利用数组保存了先计算的值，为后面的调用服务。观察参与循环的只有 i，i-1 , i-2三项，因此该方法的空间可以进一步的压缩如下。 12345678910111213141516public static int fib(int n) &#123; if(n&lt;=1) return n; int Memo_i_2=0; int Memo_i_1=1; int Memo_i=1; for(int i=2;i&lt;=n;i++) &#123; Memo_i=Memo_i_2+Memo_i_1; Memo_i_2=Memo_i_1; Memo_i_1=Memo_i; &#125; return Memo_i; &#125; 一般来说由于备忘录方式的动态规划方法使用了递归，递归的时候会产生额外的开销，使用自底向上的动态规划方法要比备忘录方法好。你以为看懂了上面的例子就懂得了动态规划吗？那就too young too simple了。动态规划远远不止如此简单，下面先给出一个例子看看能否独立完成。然后再对动态规划的其他特性进行分析。 动态规划小试牛刀例题：钢条切割 上面的例题来自于算法导论关于题目的讲解就直接截图算法导论书上了这里就不展开讲。现在使用一下前面讲到三种方法来来实现一下。①递归版本1234567891011public static int cut(int []p,int n) &#123; if(n==0) return 0; int q=Integer.MIN_VALUE; for(int i=1;i&lt;=n;i++) &#123; q=Math.max(q, p[i-1]+cut(p, n-i)); &#125; return q; &#125; 递归很好理解，如果不懂可以看上面的讲解，递归的思路其实和回溯法是一样的，遍历所有解空间但这里和上面斐波拉契数列的不同之处在于，在每一层上都进行了一次最优解的选择，q=Math.max(q, p[i-1]+cut(p, n-i));这个段语句就是最优解选择，这里上一层的最优解与下一层的最优解相关。 ②备忘录版本12345678910111213141516171819202122public static int cutMemo(int []p) &#123; int []r=new int[p.length+1]; for(int i=0;i&lt;=p.length;i++) r[i]=-1; return cut(p, p.length, r); &#125; public static int cut(int []p,int n,int []r) &#123; int q=-1; if(r[n]&gt;=0) return r[n]; if(n==0) q=0; else &#123; for(int i=1;i&lt;=n;i++) q=Math.max(q, cut(p, n-i,r)+p[i-1]); &#125; r[n]=q; return q; &#125; 有了上面求斐波拉契数列的基础，理解备忘录方法也就不难了。备忘录方法无非是在递归的时候记录下已经调用过的子函数的值。这道钢条切割问题的经典之处在于自底向上的动态规划问题的处理，理解了这个也就理解了动态规划的精髓。 ③自底向上的动态规划12345678910111213public static int buttom_up_cut(int []p) &#123; int []r=new int[p.length+1]; for(int i=1;i&lt;=p.length;i++) &#123; int q=-1; //① for(int j=1;j&lt;=i;j++) q=Math.max(q, p[j-1]+r[i-j]); r[i]=q; &#125; return r[p.length]; &#125; 自底向上的动态规划问题中最重要的是理解注释①处的循环，这里外面的循环是求r[1],r[2]……，里面的循环是求出r[1],r[2]……的最优解，也就是说r[i]中保存的是钢条长度为i时划分的最优解，这里面涉及到了最优子结构问题，也就是一个问题取最优解的时候，它的子问题也一定要取得最优解。下面是长度为4的钢条划分的结构图。我就偷懒截了个图。 动态规划原理虽然已经用动态规划方法解决了上面两个问题，但是大家可能还跟我一样并不知道什么时候要用到动态规划。总结一下上面的斐波拉契数列和钢条切割问题，发现两个问题都涉及到了重叠子问题，和最优子结构。 ①最优子结构用动态规划求解最优化问题的第一步就是刻画最优解的结构，如果一个问题的解结构包含其子问题的最优解，就称此问题具有最优子结构性质。因此，某个问题是否适合应用动态规划算法，它是否具有最优子结构性质是一个很好的线索。使用动态规划算法时，用子问题的最优解来构造原问题的最优解。因此必须考查最优解中用到的所有子问题。 ②重叠子问题在斐波拉契数列和钢条切割结构图中，可以看到大量的重叠子问题，比如说在求fib（6）的时候，fib（2）被调用了5次，在求cut（4）的时候cut（0）被调用了4次。如果使用递归算法的时候会反复的求解相同的子问题，不停的调用函数，而不是生成新的子问题。如果递归算法反复求解相同的子问题，就称为具有重叠子问题（overlapping subproblems）性质。在动态规划算法中使用数组来保存子问题的解，这样子问题多次求解的时候可以直接查表不用调用函数递归。 动态规划的经典模型线性模型线性模型的是动态规划中最常用的模型，上文讲到的钢条切割问题就是经典的线性模型，这里的线性指的是状态的排布是呈线性的。【例题1】是一个经典的面试题，我们将它作为线性模型的敲门砖。 【例题1】在一个夜黑风高的晚上，有n（n &lt;= 50）个小朋友在桥的这边，现在他们需要过桥，但是由于桥很窄，每次只允许不大于两人通过，他们只有一个手电筒，所以每次过桥的两个人需要把手电筒带回来，i号小朋友过桥的时间为T[i]，两个人过桥的总时间为二者中时间长者。问所有小朋友过桥的总时间最短是多少。 每次过桥的时候最多两个人，如果桥这边还有人，那么还得回来一个人（送手电筒），也就是说N个人过桥的次数为2*N-3（倒推，当桥这边只剩两个人时只需要一次，三个人的情况为来回一次后加上两个人的情况…）。有一个人需要来回跑，将手电筒送回来（也许不是同一个人，realy？！）这个回来的时间是没办法省去的，并且回来的次数也是确定的，为N-2，如果是我，我会选择让跑的最快的人来干这件事情，但是我错了…如果总是跑得最快的人跑回来的话，那么他在每次别人过桥的时候一定得跟过去，于是就变成就是很简单的问题了，花费的总时间： T = minPTime * (N-2) + (totalSum-minPTime) 来看一组数据 四个人过桥花费的时间分别为 1 2 5 10，按照上面的公式答案是19，但是实际答案应该是17。 具体步骤是这样的：第一步：1和2过去，花费时间2，然后1回来（花费时间1）；第二歩：3和4过去，花费时间10，然后2回来（花费时间2）；第三部：1和2过去，花费时间2，总耗时17。 所以之前的贪心想法是不对的。我们先将所有人按花费时间递增进行排序，假设前i个人过河花费的最少时间为opt[i]，那么考虑前i-1个人过河的情况，即河这边还有1个人，河那边有i-1个人，并且这时候手电筒肯定在对岸，所以opt[i] = opt[i-1] + a[1] + a[i] (让花费时间最少的人把手电筒送过来，然后和第i个人一起过河)如果河这边还有两个人，一个是第i号，另外一个无所谓，河那边有i-2个人，并且手电筒肯定在对岸，所以opt[i] = opt[i-2] + a[1] + a[i] + 2a[2] (让花费时间最少的人把电筒送过来，然后第i个人和另外一个人一起过河，由于花费时间最少的人在这边，所以下一次送手电筒过来的一定是花费次少的，送过来后花费最少的和花费次少的一起过河，解决问题)所以 opt[i] = min{opt[i-1] + a[1] + a[i] , opt[i-2] + a[1] + a[i] + 2a[2] } 区间模型区间模型的状态表示一般为d[i][j]，表示区间[i, j]上的最优解，然后通过状态转移计算出[i+1, j]或者[i, j+1]上的最优解，逐步扩大区间的范围，最终求得[1, len]的最优解。 【例题2】给定一个长度为n（n &lt;= 1000）的字符串A，求插入最少多少个字符使得它变成一个回文串。典型的区间模型，回文串拥有很明显的子结构特征，即当字符串X是一个回文串时，在X两边各添加一个字符’a’后，aXa仍然是一个回文串，我们用d[i][j]来表示A[i…j]这个子串变成回文串所需要添加的最少的字符数，那么对于A[i] == A[j]的情况，很明显有 d[i][j] = d[i+1][j-1] （这里需要明确一点，当i+1 &gt; j-1时也是有意义的，它代表的是空串，空串也是一个回文串，所以这种情况下d[i+1][j-1] = 0）；当A[i] != A[j]时，我们将它变成更小的子问题求解，我们有两种决策：1、在A[j]后面添加一个字符A[i]；2、在A[i]前面添加一个字符A[j]； 根据两种决策列出状态转移方程为：d[i][j] = min{ d[i+1][j], d[i][j-1] } + 1; (每次状态转移，区间长度增加1)空间复杂度O(n^2)，时间复杂度O(n^2)， 下文会提到将空间复杂度降为O(n)的优化算法。 背包模型背包问题是动态规划中一个最典型的问题之一。由于网上有非常详尽的背包讲解，这里只将常用部分抽出来。 【例题3】有N种物品（每种物品1件）和一个容量为V的背包。放入第 i 种物品耗费的空间是Ci，得到的价值是Wi。求解将哪些物品装入背包可使价值总和最大。f[i][v]表示前i种物品恰好放入一个容量为v的背包可以获得的最大价值。决策为第i个物品在前i-1个物品放置完毕后，是选择放还是不放，状态转移方程为： f[i][v] = max{ f[i-1][v], f[i-1][v – Ci] +Wi } 时间复杂度O(VN)，空间复杂度O(VN) （空间复杂度可利用滚动数组进行优化达到O(V) ）。 动态规划题集整理1、最长单调子序列Constructing Roads In JG Kingdom★★☆☆☆Stock Exchange ★★☆☆☆ 2、最大M子段和Max Sum ★☆☆☆☆最长公共子串 ★★☆☆☆ 3、线性模型Skiing ★☆☆☆☆ 总结弄懂动态规划问题的基本原理和动态规划问题的几个常见的模型，对于解决大部分的问题已经足够了。 参考文献1.算法导论 本文转自：https://blog.csdn.net/u013309870/article/details/75193592 支持原创。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redisHandle]]></title>
    <url>%2F2019%2F03%2F26%2FredisHandle%2F</url>
    <content type="text"><![CDATA[接下来提供redis的java操作详细介绍，请期待……]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis的java操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redisIntroduce]]></title>
    <url>%2F2019%2F03%2F26%2FredisIntroduce%2F</url>
    <content type="text"><![CDATA[接下来提供redis数据结构操作的详细介绍，请期待……]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis数据结构操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filebeat-02使用教程]]></title>
    <url>%2F2019%2F03%2F26%2Ffilebeat02%2F</url>
    <content type="text"><![CDATA[安装Filebeatyum install filebeat 安装目录一览: 让我们分别介绍目录的功能：kibana: 接入kibana时，其提供可视化配置功能module module.d: 配置参数:用于快速启动功能fields.yml: Filebeat提供针对不同组件，采集的参数名称 类型等filebeat: 可执行文件filebeat.reference.yml: Filebeat支持的参数手册，所有支持配置参数都在这filebeat.yml: 启动Filebeat需要配置文件。后面我们会重点解析 配置Filebeat配置文件：filebeat.yml 1.定义日志文件路径对于最基本的Filebeat配置，你可以使用单个路径。例如：12345filebeat.inputs:- type: log enabled: true paths: - /var/log/*.log 以上例子中，获取在/var/log/*.log路径下的所有文件作为输入，这就意味着Filebeat将获取/var/log目录下所有以.log结尾的文件。 也可以使用以下配置：/var/log//.log 抓取/var/log的子文件夹下所有的以.log结尾的文件。目前不可能递归地抓取这个目录下的所有子目录下的所有.log文件。也就是说假设配置的输入路径是/var/log//.log，如下图：那么只会抓取到2.log和3.log，而不会抓到1.log和4.log。因为/var/log/aaa/ccc/1.log和/var/log/4.log不会被抓到。 2.如果你发送输出目录到kafka或者Elasticsearch（并且不用Logstash），那么设置IP地址和端口以便能够找kafka或者到Elasticsearch：12output.elasticsearch: hosts: ["127.0.0.1:9200"] 12345678 output.kafka:#avaiable kafkaenable: true#initial broker for reading cluster metadata hosts: ["127.0.0.1:9092", "127.0.0.2:9092", "127.0.0.3:9092"] #message topic topic: testFilebeatTopic 3.如果你打算用Kibana仪表盘，可以这样配置Kibana端点：12setup.kibana: host: "localhost:5601" 4.如果你的Elasticsearch,kafka和Kibana配置了安全策略，那么在你启动Filebeat之前需要在配置文件中指定访问凭据。例如：12345678910111213output.elasticsearch: hosts: ["elasticsearchHost:9200"] username: "filebeat_elasticsearch" password: "&#123;pwd&#125;" //或者output.kafka: hosts: ["kafkaHost:9092"] username: "filebeat_kafka" password: "&#123;pwd&#125;" setup.kibana: host: "kibanaHost:5601" username: "kibana_user" password: "&#123;pwd&#125;" 配置Filebeat以使用Logstash如果你想使用Logstash对Filebeat收集的数据执行额外的处理，那么你需要将Filebeat配置为使用Logstash。12output.logstash: hosts: ["127.0.0.1:5044"] 以上配置实现日志导入到指定模块elasticsearch/kafka的基本配置了。Filebeat又有哪些特殊参数？如何实现特色的需求哪？ Filebeat的输出起输出数据的格式是json，类似这样：1234567891011&#123;"@timestamp": "2018-12-18T08:33:01.604Z", #采集时间 UTC"@metadata": &#123;....&#125;, #描述beat的信息"message": "日志内容", ### 数据主体"source": "/var/log/run.log", #数据来源"prospector": &#123; "type": "log"&#125;,"input": &#123;"type": "log" &#125;, #数据类型"beat": &#123;.... &#125;,"host": &#123;.... &#125;, #系统信息 ip 系统版本 名称等"offset": 244 #偏移&#125; 输出数据格式除包含数据主体 message 外，还包括部分附加信息。对于不需要信息，如何进行过滤和转换？这涉及Filebeat不算强大的数据过滤功能。 Filebeat数据过滤过滤内容 exclude_lines: [‘^INFO’] #exclude_lines关键字排除包含内容INFO include_lines: [‘^ERR’, ‘^WARN’] exclude_files: [‘.gz/pre&gt;] #排查压缩文件 multiline.pattern: ^[ #内容拼接，用户异常堆栈输出多行 拼接成一条过滤内容和内容拼接，需要日志的格式是json,否则不生效 过滤 json 中输出字段Filebeat提供类似管道功能的处理器(processors)，来指定生成字段,如下形式。event -&gt; filter1 -&gt; event1 -&gt; filter2 -&gt;event2 … 每次数据采集是一个事件，每个filter是一个处理器。让我们自己定义一个处理器，如下：12345processors: -drop_fields: when: has_fields:['source'] fields:["input_type"] 功能：过滤器功能删除字段(drop_fields)，条件是当存在source字段时，删除input_type字段。 在Elasticsearch中加载索引模板默认情况下，如果启用了Elasticsearch输出，Filebeat会自动加载推荐的模板文件fields.yml,可以将filebeat.yml配置文件中的默认值更改为: setup.template.name: “your_template_name” setup.template.fields: “path/to/fields.yml”覆盖现有模板: setup.template.overwrite: true禁用自动模板加载:(如果禁用自动模板加载，则需要手动加载模板) setup.template.enabled: false要手动加载模板，请运行setup命令。需要连接到Elasticsearch。如果启用了Logstash输出，则需要临时禁用Logstash输出并使用-E选项启用Elasticsearch。此处的示例假定已启用Logstash输出。如果已启用Elasticsearch输出，则可以省略-E标志。1filebeat setup --template -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["localhost:9200"]' 启动启动kafka ./kafka-server-start.sh –zookeeper localhost:2181使用Elasticsearch启动 /usr/local/programs/elasticsearch/elasticsearch-6.3.2/bin/elasticsearch启动Kibana /usr/local/programs/kibana/kibana-6.3.2-linux-x86_64/bin/kibana设置dashboard ./filebeat setup –dashboards启动Filebeat ./filebeat -e -c filebeat.yml -d “publish”浏览器访问 127.0.0.1:5601 关于以上配置这部分。更全的可以参考Filebeat官网配置filebeat提供的过滤功能挺有意思的，更全的可以参考Filebeat官网过滤使用语法 原创不易，转载请注明出处。加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>Elastic Stack</category>
        <category>监控</category>
      </categories>
      <tags>
        <tag>beats系列</tag>
        <tag>filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimal-design]]></title>
    <url>%2F2019%2F03%2F26%2Foptimal-design%2F</url>
    <content type="text"><![CDATA[接下来提供优化策略的整理，请期待……]]></content>
      <categories>
        <category>优化</category>
      </categories>
      <tags>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构下静态数据通用缓存机制]]></title>
    <url>%2F2019%2F03%2F26%2FredisCommonDesign%2F</url>
    <content type="text"><![CDATA[在分布式系统中，特别是最近很火的微服务架构下，有没有或者能不能总结出一个业务静态数据的通用缓存处理机制或方案，我根据实际的研发经验和收集的资料整理了这边文章，尝试理清其中存在的关键问题以及探寻通用的解决之道。 什么是静态数据这里静态数据是指不经常发生变化或者变化频率比较低的数据，比如车型库、用户基本信息、车辆基本信息等，车型库这种可能每个月会更新一次，用户和车辆基本信息的变化来源于用户注册、修改，这个操作的频率相对也是比较低的。另外这类数据的另一个特点是要求准确率和实时性都比较高，不能出现丢失、错误，以及过长时间的陈旧读。具体是不是应该归类为静态数据要看具体的业务，以及对变化频率高低的划分标准。在这里的业务定义中，上边这几类数据都归为静态数据。 为什么需要缓存在面向用户或车联网的业务场景中，车型信息、用户基本信息和车辆基本信息有着广泛而高频的业务需求，很多数据都需要对其进行关联处理。在这里缓存的目的就是为了提高数据查询效率。静态数据通常都保存在关系型数据库中，这类数据库的IO效率普遍不高，应对高并发的查询往往捉襟见肘。使用缓存可以极大的提升读操作的吞吐量，特别是KV类的缓存，没有复杂的关系操作，时间复杂度一般都在O(1)。注意这里说的缓存指内存缓存。 当然除了使用缓存，还可以通过其它手段来提高IO吞吐量，比如读写分离，分库分表，但是这类面向关系型数据库的方案更倾向于同时提高读写效率，对于单纯提升读吞吐量的需求，这类方案不够彻底，不能在有限的资源情况下发挥更好的作用。 通用缓存机制下面将直接给出一个我认为的通用处理机制，然后会对其进行分析。 对于某个具体的业务，其涉及到六个核心程序： ==&gt;业务服务：提供对某种业务数据的操作接口，比如车辆服务，提供对车辆基本信息的增删改查服务。 ==&gt;关系数据库：使用若干表持久化业务数据，比如SQLServer、MySQL、Oracle等。 ==&gt;持久化队列：可独立部署的队列程序，支持数据持久化，比如RabbitMQ、RocketMQ、Kafka等。 ==&gt;缓存处理程序：从队列接收数据，然后写入缓存。 ==&gt;数据一致处理程序：负责检查缓存数据库和关系型数据库中数据是否一致，如果不一致则使用关系数据库进行更新。 ==&gt;缓存数据库（Redis）：支持持久化的缓存数据库，这里直接选了Redis，这个基本是业界标准了。 以及两个外部定义： 数据生产者：业务静态数据的来源，可以理解为前端APP、Web系统的某个功能或者模块。 数据消费者：需要使用这些业务静态数据的服务或者系统，比如报警系统需要获取车辆对应的用户信息以便发送报警。 下面以问答的形式来说明为什么是这样一种机制。为什么需要业务服务？既然是微服务架构，当然离不开服务了，因为这里探讨的是业务静态数据，所以是业务服务。不过为了更好的理解，这里还是简单说下服务出现的原因。当今业务往往需要在多个终端进行使用，比如PC、手机、平板等，既有网页的形式，又有APP的形式，另外某个数据可能在多种不同的业务被需要，如果将数据操作分布在多个程序中很可能产生数据不一致的情况，另外代码不可避免的冗余，读写性能更很难控制，变更也基本上是不敢变的。通过一个业务服务可以将对业务数据的操作有序的管理起来，并通过接口的形式对外提供操作能力，代码不用冗余了，性能也好优化了，数据不一致也得到了一定的控制，编写上层应用的人也舒服了。 为什么不是进程内缓存？很多开发语言都提供了进程内缓存的支持，即使没有提供直接操作缓存的包或库，也可以通过静态变量的方式来实现。对数据的查询请求直接在进程内存完成，效率可以说是杠杠滴了。但是进程内缓存存在两个问题： 缓存数据的大小：进程可以缓存数据的大小受限于系统可用内存，同时如果机器上部署了多个服务，某个服务使用了太多的内存，则可能会影响其它服务的正常访问，因此不适合大量数据的缓存。 缓存雪崩：缓存同时大量过期或者进程重启的情况下，可能产生大量的缓存穿透，过多的请求打到关系数据库上，可能导致关系数据库的崩溃，引发更大的不可用问题。 为什么是Redis？Redis这类数据库可以解决进程内缓存的两个问题： 独立部署，不影响其它业务，还可以做集群，内存扩容比较方便。 支持数据持久化，即使Redis重启了，缓存的数据自身就可以很快恢复。 另外Redis提供了很好的读写性能，以及方便的水平扩容能力，还支持多种常用数据结构，使用起来比较方便，可以说是通用缓存首选。 为什么需要队列？队列在这里的目的是为了解耦，坦白的说这个方案中可以没有队列，业务服务在关系数据库操作完成后，直接更新到缓存也是可以的。之所以加上这个队列是由于当前的业务开发有很明显的系统拆分的需求，特别是在微服务架构下，为了降低服务之间的耦合，使用队列是个常用选择，在某些开发模型中也是很推崇的，比如Actor模型。举个例子，比如新注册一个用户，需要赠送其300积分，同时还要给其发个注册成功的邮件，如果将注册用户、赠送积分、发成功邮件都写到一起执行，会产生两个问题：一是注册操作耗时增加，二是其中某个处理引发整体不可用的几率增大，三是程序的扩展性不好；通多引入队列，将注册信息分别发到积分队列和通知队列，然后由积分模块和通知模块分别处理，用户、积分、通知三个模块的耦合降低了，相互影响变小了，以后再增加注册后的其它处理也就是增加个队列的事，整体的扩展性得到了增强。队列作为一种常用的解耦方案，在缓存这里虽然产生的影响不大，但是除了缓存难免同时还会有其它业务处理，所以为了统一处理机制，这里保留了下来。（既然用了，就把它发扬光大。） 为什么队列需要持久化？持久化是为了解决网络抖动或者崩溃导致数据丢失的问题，在数据从业务服务到队列，队列自身处理，再从队列到缓存处理程序，中间都可能丢失数据。为了解决丢失数据的问题，需要发送时确认、队列自身持久化、接收时确认；但是需要注意确认机制可能会导致重复数据的产生，因为在未收到确认时就需要重新发送或接收，而数据实际上可能被正常处理，只是确认丢失了；确认机制还会降低队列的吞吐量，但是根据我们的定义业务静态数据的变更频率应该不高，如果同时还需要较高的并发分片是个不错的选择。这里持久化队列推荐选择RabbitMQ，虽然吞吐量支持的不是很大，但是各方面综合不错，并发够用就好。 为什么需要数据一致检查程序？在业务服务操作完关系数据库后，数据发送到队列之前（或者不用队列就是直接写入缓存之前），业务服务崩溃了，这时候数据就不能更新到缓存了。还有一种情况是Redis发生了故障转移，master中的更新没有同步到slaver。通过引入这么一个检查程序，定时的检查关系数据库数据和缓存数据的差别，如果缓存数据比较陈旧，则更新之。这样提供了一种极端情况下的挽救措施。 这个检查程序的运行频率需要综合考虑数据库压力和能够承受的数据陈旧时间，不能把数据库查死了，也不能陈旧太久导致大量数据不一致。可以通过设置上次检查时间点的方式，每次只检查从上次检查时间点（或者最近几次，防止Redis故障转移数据未同步的问题）到本次检查时间点发生变更的数据，这样每次检查只对增量变更，效率更高。同时需要理解在分布式系统中，微服务架构下，数据不一致是经常出现的，必须在一致性和可用性之间做出权衡，尽力去降低影响，比如使用准实时或最终一致性。 只要数据一致检查程序是不是就够了？假设没有缓存处理程序，通过定时同步关系数据库和缓存数据库是不是就够了呢？这还是取决于业务，如果是车型库这种数据，增加一个新的车型，本来之前就没有，时间上并不是很敏感，这个是可以的。但是对于新增了用户或者车辆，数据消费者还是希望能够马上使用最新的数据进行处理，越快越好，这时使用同步或者准同步更新就能更加贴近需求。 为什么不用缓存过期机制？使用缓存过期机制可以不需要缓存处理程序和数据一致检查程序，业务服务首先从Redis查询数据，如果数据存在就直接返回，如果不存在则从关系数据库查询，然后写入Redis，然后再返回，这也是一种常用的缓存处理机制，网上可以查询到很多，很多人用的也很好。但是缓存的过期时间是个问题：缓存多长时间过期，设置的短可以降低数据的陈旧，但是会增加缓存穿透的概率，即使采用随机的缓存过期时间，在Redis重启或者故障转移的情况下还是会可能导致缓存雪崩，雪崩的情况下采用数据预热机制，也可能会导致服务更长时间的不可用；设置的长可以提升缓存的使用率，但是增加了数据陈旧，在上边对静态数据的定义中对其准确率和实时性都有较高的要求，业务上能不能接受需要考虑。而且如果操作数据和查询存在波动的峰谷，是不是要引入动态TTL的机制，以达到缓存使用和直接访问数据库的一种平衡，这就需要权衡业务需求和技术方案。 总结通过上边的这些问题问答，再来看看上面提出的微服务架构下静态数据通用缓存处理机制。 ==&gt;通过业务服务来包装对数据的操作，不管是操作关系数据库还是缓存数据库，数据消费者其实不需要关心，它只关心业务服务能不能提供高并发实时数据的查询能力。 ==&gt;利用分布式系统中经常使用队列进行解耦的方式，业务服务不干写入缓存的事，增加一个队列订阅数据变更，然后从队列取数据写入缓存数据库。 ==&gt;对于绝大部分正常的情况，通过队列更新缓存数据和业务服务中更新缓存数据，其实时性是差不多的，同时实现了业务操作和写缓存的解耦。 ==&gt;在极端崩溃导致数据不一致的情况下，通过数据一致检查程序进行补救，尽快更新缓存数据。 ==&gt;现在业务服务可以通过访问Redis缓存来提供对静态数据的高并发准实时查询能力，缓存中不存在的数据就是不存在，没有缓存穿透。 对于微服务架构而言，这个机制借助队列这种通用的解耦方式，独立了缓存更新处理，通过准实时更新和定时检查，保证了缓存的实时性和极端情况下较短时间内达到最终一致，通过缓存的持久化机制消除了缓存穿透和雪崩，在缓存的数据较大或读取并发较高时支持水平扩容，可以认为对业务静态数据提供了一种广泛适用的缓存处理机制。这个方案在某些情况下可能是没有必要的，比如你要缓存一个全国限行的城市列表，使用一个进程内缓存就够了。最后剩下的就是工作量的问题了，这个会给开发和维护带来复杂性，队列有没有用的顺手的，人手是不是够，业务需求是什么样的，需要考虑清楚。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beats]]></title>
    <url>%2F2019%2F03%2F25%2Fbeats%2F</url>
    <content type="text"><![CDATA[在介绍filebeat之前我们先来看下Beats，Beats平台是 Elastic.co从packetbeat发展出来的数据收集器系统。beat收集器可以直接写入 Elasticsearch，也可以传输给 Logstash。其中抽象出来的 libbeat，提供了统一的数据发送方法，输入配置解析，日志记录框架等功能。也就是说，所有的 beat 工具，在配置上，除了 input 以外，在output、filter、shipper、logging、run-options 上的配置规则都是完全一致的。 以下是beats系列的特点： Beats 系列：全品类采集器，搞定所有数据类型。 轻量型:从源头采集。简单明了。Beats 是数据采集的得力工具。将 Beats 和您的容器一起置于服务器上，或者将 Beats 作为函数加以部署，然后便可在 Elastisearch 中集中处理数据。如果需要更加强大的处理性能，Beats 还能将数据输送到 Logstash 进行转换和解析。 即插即用:借助模块加速数据可视化体验Filebeat 和 Metricbeat 内部集成了一系列模块，用以简化常见日志格式（例如 NGINX、Apache 或诸如 Redis 或 Docker 等系统指标）的收集、解析和可视化过程。只需运行一行命令，即可开始探索。 从环境中获得洞见跟踪数据沿袭:Beats 从您的专属环境中收集日志和指标，然后通过来自主机、诸如 Docker 和 Kubernetes 等容器平台以及云服务提供商的必要元数据对这些内容进行记录，然后再传输到 Elastic Stack 中。从监测容器到从无需服务器的架构传输数据，我们确保您拥有所需的上下文。 可扩展:缺少某种采集器？别着急。您可以自行构建并分享。每款开源 Beat 都以 libbeat（转发数据时所用的通用库）为基石。需要监控某个专用协议？自行构建。我们将为您提供所需的构建基块。同时，我们的 Beats社区正在不断壮大。 托管式 ELASTICSEARCH:Beats 同样能向 Elastic Cloud 输送数据，正在运行托管式 Elasticsearch，这些轻量型采集器同样能有效地将数据发送至 Elastic Cloud。 而这里的filebeat就是beats 的一员，目前beat可以发送数据给Elasticsearch，Logstash，Kafka，Redis，File，Console等多个目的地址。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>Elastic Stack</category>
        <category>监控</category>
      </categories>
      <tags>
        <tag>beats系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filebeat-01开始使用]]></title>
    <url>%2F2019%2F03%2F24%2Ffilebeat01%2F</url>
    <content type="text"><![CDATA[Filebeatfilebeat就是beats 的一员，目前beat可以发送数据给Elasticsearch，Logstash，File，Console四个目的地址。filebeat 是基于原先 logstash-forwarder 的源码改造出来的。换句话说：filebeat 就是新版的 logstash-forwarder，也会是 ELK Stack 在 shipper 端的第一选择。以下主要介绍下filebeat。 汇总、“tail -f” 和搜索：启动 Filebeat 后，打开 Logs UI，直接在 Kibana 中观看对您的文件进行 tail 操作的过程。通过搜索栏按照服务、应用程序、主机、数据中心或者其他条件进行筛选，以跟踪您的全部汇总日志中的异常行为。 性能稳健，不错过任何检测信号：无论在任何环境中，随时都潜伏着应用程序中断的风险。Filebeat 能够读取并转发日志行，如果出现中断，还会在一切恢复正常后，从中断前停止的位置继续开始。 Filebeat 让简单的事情简单化：Filebeat 内置有多种模块（auditd、Apache、NGINX、System、MySQL 等等），可针对常见格式的日志大大简化收集、解析和可视化过程，只需一条命令即可。之所以能实现这一点，是因为它将自动默认路径（因操作系统而异）与 Elasticsearch 采集节点管道的定义和 Kibana 仪表板组合在一起。不仅如此，数个 Filebeat 模块还包括预配置的 Machine Learning 任务。 容器就绪和云端就绪：正在对所有内容进行容器化，或者正在云端环境中运行？通过 Elastic Stack，可以轻松地监测容器和云服务。在 Kubernetes、Docker 或云端部署中部署 Filebeat，即可获得所有的日志流：信息十分完整，包括日志流的 pod、容器、节点、VM、主机以及自动关联时用到的其他元数据。此外，Beats Autodiscover 功能可检测到新容器，并使用恰当的 Filebeat 模块对这些容器进行自适应监测。 不会导致您的管道过载：当将数据发送到 Logstash 或 Elasticsearch 时，Filebeat 使用背压敏感协议，以应对更多的数据量。如果 Logstash 正在忙于处理数据，则会告诉 Filebeat 减慢读取速度。一旦拥堵得到解决，Filebeat 就会恢复到原来的步伐并继续传输数据。 输送至 Elasticsearch 或 Logstash，在 Kibana 中实现可视化：Filebeat 是 Elastic Stack 的一部分，因此能够与 Logstash、Elasticsearch 和 Kibana 无缝协作。无论您要使用 Logstash 转换或充实日志和文件，还是在 Elasticsearch 中随意处理一些数据分析，亦或在 Kibana 中构建和分享仪表板，Filebeat 都能轻松地将您的数据发送至最关键的地方。 Filebeat概览：filebeat是转发和集中日志数据的轻量级输出。可以作为代理的服务器上安装，filebeat监视指定的日志文件或者位置，收集日志事件，将其转发给Elasticsearch或者Logstash进行索引。但是filebeat是怎么工作的呢？当启动filebeat的时候，它会启动一个或多个输入监视你指定位置的日志文件。对于filebeat定位的每个日志文件，它都会启动一个harvester（收割机）。每一个filebeat都会读日志文件中当前新写入的内容，并将日志数据发送给libbeat，libbeat聚合事件并将聚合数据发送给filebeat指定的输出。 filebeat是基于libbeat框架。以下是架构设计。 那filebeat是怎么工作的呢？明白filebeat是如何工作的对于决定使用filebeat的哪些特定配置至关重要。filebeat包含那个重要部分：输入input和收割者harvesters。这两部分协同工作去读取指定文件并输出到指定位置。 什么是收割者harvesters？收割者是一行行的读取单个文件的内容，然后将读取的内容发送给输出。每一个收割者harvester启动自一个单独的文件。收割者harvester负责关闭和开启一个文件，这意味着只要收割者harvester运行，文件描述符都是打开的。如果一个文件被删除或者改名，收割者harvester仍旧保持对这个文件的读取。这有个副作用就是直到收割者harvester关闭前，磁盘的空间都为之保留着。默认情况下，filebeat都会保持文件打开状态直到达到关闭不活动状态。 什么是输出input？输出input的职责是管理收割者harvester从哪里读数据。如果输入类型是log，则输入会查找驱动上定义的与全局路径匹配的所有文件，并为每个文件启动一个收割者harvester，每一个输入都运行在自己的执行实例中。12345filebeat.inputs:- type: log paths: - /var/log/*.log - /var/path2/*.log filebeat支持多种输入类型，每一种输入类型都可以被多次定义。日志输入可以检查每个文件是否需要启动收割者harvester。 fileBeat如何保持文件的状态？filebeat保留每个文件的状态，并经常将状态刷新到注册表文件中的磁盘。状态用于记住harvester读取的最后一个偏移量，并确保将所有日志按行发送。如果无法访问输出（如ElasticSearch或Logstash），则FileBeat会跟踪最后发送的行，并在输出再次可用时继续读取文件。当filebeat运行时，每个输入的状态信息也保存在内存中。当filebeat重新启动时，注册表文件中的数据用于重建状态，filebeat将在最后一个已知位置继续运行harvester。对于每个输入，filebeat都保持其找到的每个文件的状态。由于文件可以重命名或移动，因此文件名和路径不足以标识文件。对于每个文件，filebeat存储唯一的标识符，以检测是否是之前harvester过的文件。如果涉及每天创建大量的新文件，就可能会发现注册表文件变得太大。看到注册表文件太大了吗？filebeat提供配置合理的参数方法解决这个问题。 fileBeat如何确保至少一次送达的？filebeat保证事件将至少传递一次到配置的输出，并且不会丢失数据。FileBeat能够实现这种行为，因为它将每个事件的传递状态存储在注册表文件中。在已定义的输出被阻止且尚未确认所有事件的情况下，FileBeat将继续尝试发送事件，直到输出确认已收到事件为止。如果filebeat在发送事件的过程中关闭，它不会在关闭之前等待输出确认所有事件。当filebeat重新启动时，任何发送到输出但在filebeat关闭前未确认的事件都会再次发送。这样可以确保每个事件至少发送一次，但最终可能会向输出发送重复的事件。通过设置shutdown_timeout选项，可以将filebeat配置为在关闭之前等待特定的时间。 FileBeat的至少一次送达有一个限制，包括日志轮换和删除旧文件。如果日志文件被写入磁盘并旋转得比文件节拍处理的速度快，或者如果文件在输出时被删除不可用，数据可能会丢失。 原创不易，转载请注明出处。加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>Elastic Stack</category>
        <category>监控</category>
      </categories>
      <tags>
        <tag>beats系列</tag>
        <tag>filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic-Stack]]></title>
    <url>%2F2019%2F03%2F24%2FElastic-Stack%2F</url>
    <content type="text"><![CDATA[最近接触了Elastic技术栈的产品filebeat，觉得用着挺不错的。所以汇总下Elastic技术栈的所有产品，深入了解一下。Elastic Stack 其实是一个集合，除了包含大家熟知的Elasticsearch，还有Kibana，Logstash 和 Beats 这几种开源产品。 基于version版本6.6：Elasticsearch：是一个实时，分布式存储，搜索和分析引擎。它可以用做多种目的，但是他擅长的一个方面是索引半结构化数据流，如日志和解码网络数据包。Kibana：是一个开源的分析和可视化的平台，旨在和Elasticsearch合作。你可以使用Kibana来搜索和查看和与Elasticsearch索引中存储的数据进行交互。你可以轻松的执行高级数据的分析，并在各种图标，表格和地图中可视化数据。Logstash：是一个功能强大的工具，他可以广泛的和各种部署集成在一起，并提供大量可选插件来帮助你解析，丰富，转换和缓冲来自各个数据源的数据。Beats：是一个开放源码的数据托运人，你可以在服务器上作为代理安装，并向Elasticsearch发送操作数据。Beats可以通过Logstash直接向Elasticsearch发送数据，你可在此进一步的处理和增强数据。 每一个beat都可以单独安装，彼此实现功能，没有依赖。 Auditbeat：Audit data审计数据 Filebeat：Log files日志文件 Functionbeat：Cloud data云数据 Heartbeat：Availability monitoring可用性监控 Journalbeat：Systemd journals系统日志 Metricbeat：Metrics度量指标 Packetbeat：Network traffic网络流量 Winlogbeat：Windows event logsWindows事件日志 Elastic Stack官网提供了一个简单的系统监控解决方案，该解决方案使用MetricBeat收集服务器度量数据并将数据发送到ElasticSearch，然后使用Kibana来查询和可视化数据。除此之外还可以添加Logstash进行额外的解析。 这个监控方案的部署可参照以上链接，页面效果官网也有提供，具体性能可做压测等看看效果，在此不做太多介绍。]]></content>
      <categories>
        <category>Elastic Stack</category>
        <category>监控</category>
      </categories>
      <tags>
        <tag>简介</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支付业务设计]]></title>
    <url>%2F2019%2F03%2F24%2FfinaPayFlow%2F</url>
    <content type="text"><![CDATA[本文主要以2B2B平台为例对尝试着对支付绑定、支付解绑、在线支付、退款等业务流程及设计方式进行说明、梳理。设计策略：在做支付业务设计的时候，不光要分析业务流程还要考虑设计策略，关于支付业务的设计策略主要体现在数据安全性和扩展性。 1、数据安全性采用银联签名机制、加密机制确保支付过程中支付的安全性。 报文签名与验签机制的共同点：首先都需要对报文中出现签名域（signature）之外的所有数据元采用key=value的形式按照名称排序，然后以&amp;作为连接符拼接成待签名串。其次，对待签名串使用SHA-1算法做摘要。 报文签名机制与验签机制不同点：==&gt;报文的签名处理机制如下：使用银联颁发给商户的商户RSA私钥证书对摘要做签名。最后，对签名做Base64编码，将编码后的签名串放在签名（signature）表单域里和其他表单域一起通过HTTP Post的方式传输给银联在线支付平台。==&gt;报文的验签处理机制如下：使用商户入网时银联提供的银联在线支付通讯RSA公钥证书对摘要做签名。最后，对签名做Base64编码，与返回报文表单中的签名（signature）域签名串做比较，如一致则验签成功，否则验签失败。 加密机制：对于持卡人密码银联在线支付平台使用RSA公钥证书对ANSI X9.8带主帐号格式的PIN加密并做Base64编码后传输，以保障密码的安全性。依据商户可选配置，对于CVN2、有效期、卡号使用RSA公钥证书分别做加密并Base64处理。对于敏感信息银行卡验证信息及身份信息部分内容，采用Base64编码后传输，以做数据屏蔽。对于文件内容，使用DEFLATE压缩算法压缩后，Base64编码的方式传输，压缩编码后的内容参与签名摘要运算。 2、程序扩展性 在支付业务分析设计后，其相关数据库表包括支付记录表 支付记录表支付银行表 支付绑定表 支付异常表、支付订单表；以后再有类似的在线银联支付业务可以直接进行复用，不用新建表。此外以下业务流程设计也可以进行扩展使用。 支付业务流程设计首先介绍整个业务的整体业务流程、资金流业务流程。其次分别介绍支付绑定、解绑、支付、退款等业务流程的设计。 1、支付业务整体流程图 2、支付业务资金流顺序图参考以上流程图资金流。 3、银行卡绑定业务顺序图绑定业务：首先要建立绑定关系 建立绑定关系，指商户/收单机构的用户号（即报文中的绑定标识号）与用户的银联卡信息进行关联，关联信息留存在银联系统中。建立绑定关系可分为前台模式和后台模式，前台模式时，用户在银联页面输入相关银行卡信息；后台模式由商户采集银行卡信息发送至银联进行绑定。下面以前台模式为例进行说明，业务流程图如下所示： 流程说明： 如果采购方没有绑定过银行卡，会提示“请先绑定银行开再支付”；如果绑定过银行卡，会跳转至支付页面； 在绑定银行卡页面，如果第一次绑定银行卡，绑定信息中会进行“交易平台支付密码”新增操作，否则无此操作，一个采购方只能有一个“交易平台支付密码”，可在会员中心修改此支付密码； 在银联绑定卡页面，可选择储蓄卡和信用卡，点击“绑定”，跳转至银联绑定页面，输入卡号等绑定信息，绑定成功后会跳转到平台页面，绑定成功的卡可进行解绑。 绑定卡以后，可进行支付，在支付页面，选择一张绑定银行卡，输入手机验证码和“交易平台支付密码”，支付点击“支付”，支付成功会跳转至支付成功页面，订单列表中可看到支付成功后订单信息。 在已付款订单列表页，可对当天清算前订单进行退款，点击“退款”，会返回退款结果；如果订单支付成功，配送商（销售）可进行接单；采购方在配送商（销售）接单后可进行“确认收货”操作； 确认收货后，在第二天配送商（销售）可收到相应款项，如果订单开具发票，则款项打至对公账号，如果不开具发票，则款项打至对私账号。 4、银行卡解绑业务流程图 银行卡解绑业务实际上是与银行卡绑定业务相对的业务，其业务流程非常相似。 5、银行卡在线支付业务流程图 一般交易平台支付有四个触发场景：购物车支付、直接购买支付、快速购买支付、订单列表支付。 触发支付操作后，根据是否开具发票来判断调用哪种类型的支付接口，如果开具发票且是增值税发票，调用B2B支付接口（前台交易），如果未开具发票或开具普通发票，调用绑定支付接口，支付之前必须先绑定银行卡，如果是借记卡，调用借记卡绑定接口，之后调用绑定代收接口，如果是贷记卡，则调用贷记卡绑定接口，之后调用绑定消费接口。 从技术实现方式上交易大致可划分为前台类交易、后台类交易 前台类交易是指交易请求方（如商户、收单机构）与银联在线支付系统之间的交易信息通过用户浏览器进行传递的交易，是一种异步的、需要持卡人参与完成的交易类型。对于涉及金额的前台类交易（即交易请求中有金额字段）银联在线支付系统系统均会给请求方后台通知(后台通知报文要素同前台应答的要素)，请求方也必须实现接收后台通知。对于交易状态未知的交易请求方必须发起交易状态查询交易。 后台类交易是指交易请求方（如商户、收单机构）将交易信息直接通过请求方服务器发送至银联在线支付系统服务器的交易方式。后台交易均为同步短连接方式，不需要持卡人参与完成的交易类型。对于涉及金额的后台类交易，若通讯超时，则交易请求方必须发起交易状态查询交易。 6、银行卡退款业务流程图银行在线退款业务根据业务需求设计如下业务规则：==&gt;整单整退（子订单不能单独退款，只能大订单进行退款）==&gt;当天晚上11点前的订单，在11点后不可以进行线上退款。 已发货订单和交易完成订单不能线上退款，只能线下退款 7、整体接口设计 8、支付开发设计8.1前台交易开发步骤==&gt;以表单的方式组装要发送给银联在线支付的数据对象（包括订单号、商户号、交易类型、交易金额、交易时间等各域）。每个域填写方法可参考文档《中国银联在线支付平台-商户接入接口规范》。==&gt;将组装好的数据排序好并用&amp;连接后签名，生成signature字段，可使用插件包提供的方法“sign(未签名报文, 报文字符集);”。可通过调用插件包提供的签名方法来完成签名。==&gt;把所有要发送给银联在线支付的域包括signature和signMethod，组成表单以POST方式送给银联在线支付前台交易的地址。==&gt;交易完成后，银联在线支付系统将把交易结果分别返回通知到商户通的前台应答地址和后台应答地址上，商户接收到交易通知后可分别调用“coverResultString2Map(应答报文);”方法进行应答报文解析，和“MpiUtil.validate(应答报文, 报文字符集)”方法进行签名验证。 8.2后台类交易接口开发步骤商户按照不同的交易组装报文，交易报文请参考《中国银联在线支付平台-商户接入接口规范》,把组装好的数据排序好并用&amp;连接后签名，生成signature字段，可使用插件包提供的方法“sign(未签名报文, 报文字符集);”。把所有数据，包括signature和signMethod用key=value并用&amp;符号链接的方式组装成字符串，通过插件包提供的方法进行发送。接收到返回报文后，通过插件包提供的方法进行验证签名，调用“coverResultString2Map(应答报文);”方法进行应答报文解析，和“MpiUtil.validate(应答报文, 报文字符集)”方法进行签名验证。 8.3合并付款交易接口功能说明:合并付款是指当支付订单中包含多个商户的商品信息的时候，能一次进行合并付款，避免一单一付的情况。 8.4退款交易合并退款交易是指付款人在付款后因为某些原因想退回已付款项而触发的操作，银联交易完成后，需要向银联商务发送合并退款报文。 8.5当日确认收货及需要划付当日2B2B平台收到的确认收货或已经超过确认收货时间节点的待划付订单。银商根据收到的报文数据，按子订单中的商户编号，对此商户做资金划付。]]></content>
      <categories>
        <category>金融业务</category>
      </categories>
      <tags>
        <tag>支付类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[金融系统三模型，交易，户账，总账]]></title>
    <url>%2F2019%2F03%2F23%2FfinaModel%2F</url>
    <content type="text"><![CDATA[交易模型交易是具有业务属性的模型，描述业务自身的信息，例如消费、付款、投资、贷款等。在交易中，除了包含支付要素（例如收付方账号、金额等）外，还要与业务要素相关联，例如商品信息、合同信息、预算信息、产品信息等等。 户账模型户账是指以账户为核心，包含账户和账户的账两部分。户账可以看做是金融系统的核心，对于交易模型来说，更多的是状态驱动，而户账的账则是财务凭证驱动，两者是完全不同的思考维度。 按照现代的复式记账法规则，账户具备借贷属性，又可以分为资产、负债、所有者权益等业务大类。做账时根据交易的收付方账户、交易类型生成包含借贷的凭证信息，例如张三银行账户转账给李四银行账户100元，其做账为： 借：张三银行账户100元 贷：李四银行账户100元 复式记账法需要选好记账主体，例如A公司的在工行的一般存款账户，对于A公司来说是资产，但对于银行来说就成了负债。 账户做账根据交易类型，未必都是一借一贷的，包含单笔借、单笔贷、一借一贷、一借多贷、一贷多借。例如对于一些自动扣费的业务，就可以采用一贷多借的记账方式： 借：张三缴费账户320元 借：李四缴费账户110元 借：王五缴费账户160元 贷：XXX公司一般存款账户存款820元 总账模型总账更具有财务属性，户账围绕账户进行做账，总账围绕科目进行做账，二者通过账户的科目属性进行关联；此外有一些业务是可以没有账户的，例如固定资产等信息就可以直接通过科目做账。由总账引出的就是经典的财务三大报表：现金流量表、资产负债表、利润表。 总结交易、户账、总账，其实每个模型都有很多的细节关注点，例如交易如何幂等、如何提升交易处理正确性和大数据量交易并发处理；户账如何解决热点账户、如何做账、如何对账、如何检查；总账如何多维度的分析、报告等等，每一项都值得深究。 再回到最开始，交易、户账、总账层层深入，每深入一层就表示了金融水平增进一层。例如很多企业早期可能只有交易模型，但发展到一定阶段就不得不引入账户模型、总账模型，因为只有这样业务才会更严谨，系统才会更完善。 我会在今后逐渐的完善交易，户账，总账的模型结构及细节。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>金融业务</category>
      </categories>
      <tags>
        <tag>金融系统模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[金融机构编码结构完整介绍]]></title>
    <url>%2F2019%2F03%2F23%2FfinaInstiCodeStruct%2F</url>
    <content type="text"><![CDATA[金融机构编码介绍金融机构编码是为了统一金融机构编码的方式与方法，加强信息系统之间的互联、互通，提高信息共享的效率，规范中国人民银行及金融机构在新建、开发、升级、改造信息系统、数据仓库中使用的金融机构编码，确保金融机构编码信息的真实、准确和完整而存在的，其编码由人民银行制定。可参考《金融机构编码规范》 针对的对象中华人民共和国的货币当局、监管当局及其境内外派出机构；境内银行、证券、保险类金融机构的法人机构及其境内外具有经营许可的分支机构；交易结算类金融机构及其境内分支机构；境内设立的金融控股公司；国外金融机构在我国境内设立的具有经营许可的非法人分支机构，中国人民银行认定的其他有关金融机构。“境内”指中华人民共和国 （不含港、澳、台地区）境内的地区。 编码结构金融机构编码是特征组合码，长度为十四位，分别为大写拉丁字母或阿拉伯数字。编码分为六段，从左至右分别为：一位金融机构一级分类码；一位金融机构二级分类码；四位金融机构三级分类码；两位地区代码；五位顺序码；一位校验码，如下图所示： 金融机构一级分类码长度为一位，采用大写拉丁字母或阿拉伯数字编码，表示金融机构的一级分类。 A-货币当局 B-监管当局 C-银行业存款类金融机构 D-银行业非存款类金融机构 E-证券业金融机构 F-保险业金融机构 G-交易及结算类金融机构 H-金融控股公司 Z-其他 J～Y(I、O除外)，1～9(0除外)-预留 金融机构二级分类码长度为一位，采用阿拉伯数字编码在同一级分类内按顺序编码，表示金融机构的二级分类。 A-货币当局 1-中国人民银行 2-国家外汇管理局 B-监管当局 1-中国银行业监督管理委员会 2-中国证券监督管理委员会 3-中国保险监督管理委员会 C-银行业存款类金融机构 1-银行 2-城市信用合作社(含联社) 3-农村信用合作社(含联社) 4-农村资金互助社 5-财务公司 D-银行业非存款类金融机构 1-信托公司 2-金融资产管理公司 3-金融租赁公司 4-汽车金融公司 5-贷款公司 6-货币经纪公司 E-证券业金融机构 1-证券公司 2-证券投资基金管理公司 3-期货公司 4-投资咨询公司 F-保险业金融机构 1-财产保险公司 2-人身保险公司 3-再保险公司 4-保险资产管理公司 5-保险经纪公司 6-保险代理公司 7-保险公估公司 8-企业年金 G-交易及结算类金融机构 1-交易所 2-登记结算类机构 H-金融控股公司 1-中央金融控股公司 2-其他金融控股公司 Z-其他 1-小额贷款公司 2- 三方支付公司 金融机构三级分码长度为四位，采用阿拉伯数字0001～9999在同一二级分类内按顺序编码，表示金融机构的三级分类，三级分类指境内单家法人金融机构或境外金融机构直接在境内设立的不具备法人资格的机构。 地区代码长度为两位，采用拉丁字母和阿拉伯数字编码，表示金融机构所在地区的代码。按金融机构所在地不同，分别按照如下两种方式赋码：当为境内金融机构时，采用《GB／T 2260-2007中华人民共和国行政区划代码》，取其数字码前两位为金融机构所属省、自治区、直辖市代码。当为境外金融机构时，采用《GB／T 2659-2000世界各国和地区名称代码(eqv ISO 3166-1：1997)》，取其两字符拉丁字母代码为金融机构属地国家或地区的代码。(台湾、香港和澳门归于此类) 顺序码长度为五位，采用阿拉伯数字编码，表示金融机构的顺序号。同一金融机构(三级)分类、同一地区代码下，多个不同营业机构的顺序编号从00001-99999顺序连续编码。 校验码长度为一位，采用阿拉伯数字编码，使用The Luhn Mod-10 Method算法生成。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>金融业务</category>
      </categories>
      <tags>
        <tag>金融编码标准</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[央行现代化支付系统CNAPS行号介绍]]></title>
    <url>%2F2019%2F03%2F23%2FbankCNAPSintroduce%2F</url>
    <content type="text"><![CDATA[现代化支付系统（CNAPS）行号由12位数字组成，是通过人民银行大小额支付系统进行跨行交易所必须的要素，每个参与机构都会被分配一个唯一的编号。现代化支付系统行号贯穿于CNAPS系统各业务子系统处理的全过程，既是支付业务分发路径，又是支付业务清算的依据。同时作为银行机构的唯一标志，在人民币账户管理系统、电子商业汇票系统、商业银行行内系统等系统中共享使用。 CNAPS行号由3位行别代码、4位地区代码、4位分支机构序号和1位校验码，共12位定长数字构成。 行别代码：包含3位定长数字，由1位类别代码和2位顺序编码组成。 ==&gt;类别代码：1位数字，0—9，标识银行类别。 0—中央银行； 1—国有独资商业银行； 2—政策性银行； 3—其他商业银行； 4—非银行金融机构； 5、6、7—外资银行； 9—特许参与者； 8—待分配。 ==&gt;顺序编码：2位数字，01—99，赋予以下银行的行别代码 0、中央银行 中国人民银行会计营业部门 0 01 中国人民银行国家金库 0 11 1、国有独资商业银行 中国工商银行 1 02 中国农业银行 1 03 中国银行1 04 中国建设银行1 05 2、政策性银行 国家开发银行2 01 中国进出口银行2 02 中国农业发展银行2 03 3、其他商业银行 交通银行3 01 中信实业银行3 02 中国光大银行3 03 华夏银行3 04 中国民生银行3 05 广东发展银行3 06 深圳发展银行3 07 招商银行3 08 兴业银行3 09 上海浦东发展银行3 10 城市商业银行3 13 农村商业银行3 14 4、非银行金融机构 城市信用社4 01 农村信用社4 02 5、6、7、外资银行 香港上海汇丰银行5 01 东亚银行5 02 南洋商业银行5 03 恒生银行5 04 中银香港5 05 集友银行5 06 9、特许参与者 中央国债登记结算有限责任公司9 01 中国人民银行公开市场操作室9 02 中国银行间外汇交易中心9 03 城市商业银行资金清算中心9 04 地区代码：由4位定长数字组成。地区代码为银行机构所在城市的电子联行清算中心代码，编制标准采用GB13497-1992《全国清算中心代码》。分支机构序号：采用4位定长数字排序组成。各银行分支机构序号由其分行或所在城市总行与其上级管辖行协商确认后，对城市内所属分支机构顺序编号。校验码：一位数字。算法采用双模算法，录入前11位行号后系统自动生成校验码。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>金融业务</category>
      </categories>
      <tags>
        <tag>金融编码标准</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[银行存款业务介绍]]></title>
    <url>%2F2019%2F03%2F23%2FbankDeposit%2F</url>
    <content type="text"><![CDATA[对于银行存款可以大致分为活期存款、协定存款、定期存款、通知存款、智能存款几大类。 活期存款是一种不限存期，凭银行卡或存折及预留密码可在银行营业时间内通过柜面或通过银行自助设备随时存取现金的服务。人民币活期存款1元起存，外币活期存款起存金额为不低于人民币20元的等值外汇。 对于企业而言，活期存款账户又分为多种类型：基本存款帐户==&gt;是企事业单位的主要存款帐户，是办理日常转帐结算、工资、奖金和现金收付的帐户。 只能开立一个基本帐户。一般存款帐户==&gt;是企事业单位开立基本存款帐户后，根据资金管理需要，选择在其他银行开立的存款帐户，主要办理转帐结算和现金缴存，不能办理现金支取。专用存款帐户==&gt;是企事业因基本建设、更新改造或办理信托、代理业务、政策性房地产开发、信用卡等特定用途需要开立的帐户。企事业单位的销货款不能进入该帐户。该帐户一般不能支取现金。临时存款帐户==&gt;是企事业单位为临时经济活动或通过应解汇款及汇票解入的款项需要所开立的帐户，主要办理转帐结算和按规定办理现金收付。临时存款帐户原则上不使用支票结算。 协定存款适用于对公客户（个人不适用），是指对公客户与银行签订协定存款合同，双方商定对公客户保留一定金额的存款以应付日常结算，此部分按普通活期利率计付利息，超过定额金额的那部分存款按协定存款利率计付利息。协定存款功能等同活期存款，但收益要高出活期存款近2倍，一般银行只会跟有大额存款的客户做协定存款。协定存款合同期限最长为一年（含一年），到期任何一方如未提出终止或修改，则自动延期。 定期存款是银行与存款人双方在存款时事先约定期限、利率，到期后支取本息的存款。存期为三个月、六个月、一年、二年、三年、五年。可办理部分提前支取一次，存款到期，凭存单支取本息，也可按原存期自动转存多次。定期储蓄存款到期支取按存单开户日存款利率计付利息，提前支取按支取日活期储蓄存款利率计息，逾期支取，逾期部分按支取日活期存款利率计息。 定期储蓄存款方式有：整存整取、零存整取、存本取息、整存零取。整存整取==&gt;是一种由客户选择存款期限，整笔存入，到期提取本息的一种定期储蓄。零存整取==&gt;是指客户如需逐步积累每月结余，可以选择“零存整取”存款方式。零存整取，是一种事先约定金额，逐月按约定金额存入，到期支取本息的定期储蓄。存本取息==&gt;指如果客户有款项在一定时期内不需动用，只需定期支取利息以作生活零用，客户可选择“存本取息”方式作为自己的定期储蓄存款形式。“存本取息”业务是一种一次存入本金，分次支取利息，到期支取本金的定期储蓄。整存零取==&gt;指如果客户有整笔较大款项收入且需要在一定时期内分期陆续支取使用时，可以选择“整存零取”方式作为自己的储蓄存款方式。 “整存零取”业务是一种事先约定存期，整数金额一次存入，分期平均支取本金，到期支取利息的定期储蓄。 通知存款是一种不约定存期、一次性存入、可多次支取，支取时需提前通知银行、约定支取日期和金额方能支取的存款。人民币通知存款最低起存金额5万元、单位最低起存金额50万元，个人最低支取金额5万元、单位最低支取金额10万元。外币最低起存金额为1000美元等值外币。个人通知存款不论实际存期多长，按存款人提前通知的期限长短划分为一天通知存款和七天通知存款两个品种（一天通知存款必须提前一天通知约定支取存款，七天通知存款则必须提前七天通知约定支取存款）。 利息计算注意事项：通知存款按支取日挂牌公告的相应利率和实际存期计息，利随本清。个人通知存款遇以下情况，按支取日挂牌公告的活期存款利率计息：==&gt;实际存期不足通知期限的，按活期存款利率计息==&gt;未提前通知而支取的，支取部分按活期存款利率计息==&gt;已办理通知手续而提前支取或逾期支取的，支取部分按活期存款利率计息==&gt;支取金额不足或超过约定金额的，不足或超过部分按支取日活期存款利率计息==&gt;支取金额不足最低支取金额的，按活期存款利率计息 通知存款如已办理通知手续而不支取或在通知期限内取消通知的，通知期限内不计息。通知存款约定取款日遇节假日不予顺延。 智能存款目前没有统一的定义，各家银行的智能存款的存款、取款、算息规则也不相同，例如有的银行规定存款时不约定期限，取款时根据定期规则靠档计息；有的银行取款时根据通知规则进行靠档计息；还有的银行混合使用定期和通知两种靠档规则。 加油！Coding For Dream！！I never feared death or dying, I only fear never trying. –Fast &amp; Furious]]></content>
      <categories>
        <category>金融业务</category>
      </categories>
      <tags>
        <tag>存款业务分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[addTwoNumbers]]></title>
    <url>%2F2019%2F03%2F23%2FaddTwoNumbers%2F</url>
    <content type="text"><![CDATA[算法第二题： You are given two non-empty linked lists representing two non-negative integers.给两个表示非负整数的非空链表The digits are stored in reverse order and each of their nodes contain a single digit.数字在链表中以反向顺序保存，而且每个节点包含一个数字。Add the two numbers and return it as a linked list.将两个数字相加并以链表返回。You may assume the two numbers do not contain any leading zero, except the number 0 itself.你可以假定每个数字不包含任何前导零，除非0本身。 Example:Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. my solution我的解法: 1234567891011121314151617181920212223242526272829303132/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode returnListNode = new ListNode(0); ListNode m1 = l1; ListNode m2 = l2; ListNode currentNode = returnListNode; int nextVal =0; while(m1 !=null || m2 !=null)&#123; int x = (m1!=null)?m1.val:0; int y = (m2!=null)?m2.val:0; int sum = x+y+nextVal; nextVal = sum/10; currentNode.next = new ListNode(sum%10); currentNode = currentNode.next; if(m1 !=null) m1=m1.next; if(m2 !=null) m2=m2.next; &#125; if(nextVal&gt;0)&#123; currentNode.next = new ListNode(nextVal); &#125; return returnListNode.next; &#125;&#125; Runtime: 19 ms, faster than 99.14% of Java online submissions for Add Two Numbers.Memory Usage: 47.7 MB, less than 47.01% of Java online submissions for Add Two Numbers. Most Vote Solution投票最多的解法: 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; int carry = 0; ListNode p, dummy = new ListNode(0); p = dummy; while (l1 != null || l2 != null || carry != 0) &#123; if (l1 != null) &#123; carry += l1.val; l1 = l1.next; &#125; if (l2 != null) &#123; carry += l2.val; l2 = l2.next; &#125; p.next = new ListNode(carry%10); carry /= 10; p = p.next; &#125; return dummy.next;&#125;&#125; Runtime: 20 ms, faster than 80.39% of Java online submissions for Add Two Numbers.Memory Usage: 48.4 MB, less than 6.94% of Java online submissions for Add Two Numbers.]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>ListNode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode算法题-01-twosum]]></title>
    <url>%2F2019%2F03%2F21%2Ftwosum%2F</url>
    <content type="text"><![CDATA[算法第一题： Given an array of integers, return indices of the two numbers such that they add up to a specific target.给一个integer类型的数组，找出其中包含的两个数字，这两个数字相加可得到指定数字。You may assume that each input would have exactly one solution, and you may not use the same element twice.你可以假设每个输入只有一个解决方案，而且不能使用两次相同元素。 Example:Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. my solution我的解法:12345678910111213141516171819202122232425class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; int[] returnResult = new int[2]; int size = nums.length; for(int i=0; i&lt;size; i++)&#123; int firstNum = nums[i]; boolean flag =false; for(int j= i+1; j&lt;size; j++)&#123; int secondNum = nums[j]; int sum = firstNum + secondNum; if(sum == target)&#123; returnResult[0]=i; returnResult[1]=j; flag =true; break; &#125; &#125; if(flag)&#123; break; &#125; &#125; return returnResult; &#125;&#125; Runtime: 19 ms, faster than 39.98% of Java online submissions for Two Sum. 运行时间超过39.98%的提交。Memory Usage: 38.5 MB, less than 46.13% of Java online submissions for Two Sum. 所占内存超过46.13%的提交。 Most Vote Solution投票最多的解法:Time complexity = O(n^2);（时间复杂度大，但运行及内存使用优秀的，应该是数据量较少的原因） 12345678910111213public int[] twoSum(int[] nums, int target) &#123; HashMap&lt;Integer, Integer&gt; tracker = new HashMap&lt;Integer, Integer&gt;(); int len = nums.length; for(int i = 0; i &lt; len; i++)&#123; if(tracker.containsKey(nums[i]))&#123; int left = tracker.get(nums[i]); return new int[]&#123;left+1, i+1&#125;; &#125;else&#123; tracker.put(target - nums[i], i); &#125; &#125; return new int[2]; &#125; Runtime: 3 ms, faster than 99.30% of Java online submissions for Two Sum.Memory Usage: 38.8 MB, less than 34.98% of Java online submissions for Two Sum. Most Posts Solution大多数的解决方案： Time complexity:step1 : copy an array, and sort it using quick sort, O(nlogn)step2 : using start and end points to find a, b which satifys a+b==target, O(n)step3 : find the index of a, b from origin array, O(n) 123456789101112131415161718192021222324252627282930313233343536373839404142434445public int[] twoSum_n2(int[] nums, int target) &#123; if(nums == null) return null; int[] nums2 = Arrays.copyOf(nums, nums.length); Arrays.sort(nums2); int a = 0, b = 0; int start = 0, end = nums2.length-1; //find two nums while(start&lt;end)&#123; int sum = nums2[start] + nums2[end]; if(sum &lt; target) start++; else if(sum &gt; target) end--; else&#123; a = nums2[start]; b = nums2[end]; break; &#125; &#125; //find the index of two numbers int[] res = new int[2]; for(int i = 0; i &lt; nums.length; i++)&#123; if(nums[i] == a)&#123; res[0] = i; break; &#125; &#125; if(a != b)&#123; for(int i = 0; i &lt; nums.length; i++)&#123; if(nums[i] == b)&#123; res[1] = i; break; &#125; &#125; &#125; else&#123; for(int i = 0; i &lt; nums.length; i++)&#123; if(nums[i] == b &amp;&amp; i != res[0])&#123; res[1] = i; break; &#125; &#125; &#125; return res; &#125; Runtime: 3 ms, faster than 99.30% of Java online submissions for Two Sum.Memory Usage: 39 MB, less than 28.65% of Java online submissions for Two Sum. 看了第三种应该在大数据量运算的时候更加优秀，Arrays.copyOf好在哪？]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>Arrays</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetCodeSchedule]]></title>
    <url>%2F2019%2F03%2F21%2FleetCodeSchedule%2F</url>
    <content type="text"><![CDATA[关于算法题，以前觉得我一个做后端开发的，做好应用，管好业务就好了。算法都是算法工程师该去做的，我会使用就好了。 过完年看了一些文章，其中有关于最近几年算法工程师及AI工程师对比Java工程师的市场需求很大，现在已经不是那个做个应用就能够卖好多钱的时代了。市场对于更智能使用更便捷，用户体验更好的应用具有好感，一旦有了好感就有了依赖性粘性，就会促使应用更好。当然此时需要更好的中后台支撑，那就是更多的具备优秀能力的人才去做设计优化。一个好的平台从一开始的设计上就已经考虑了包括计算机基础的优点及缺陷，规避了好多问题，将优点更加放大，这也体现在了算法上了。不同的算法对于一个事情的处理在性能上是不一样的，一个小的事情上节省零点几毫秒，在大批量高并发下就节省了不少时间，也节省了资源。让应用更加优秀。 这是我做算法题的初衷，让我更加理解计算机基础及java数据结构基础，让我的代码变的更加高效。 leetCode是个不错的平台，我用的英文版，还可以练习英语。 算法工程师和我也是才开始做，一定做的很坑。我会把每次做的都贴出来，大致分析一下，大家可以交流起来。 加油！Coding For Dream！！ I never feared death or dying, I only fear never trying. From Fast &amp; Furious]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>计划</tag>
      </tags>
  </entry>
</search>
